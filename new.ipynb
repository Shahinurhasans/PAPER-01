{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b803ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total slides: 111\n",
      "Processed: 0 | Failed: 0 | Remaining: 111\n",
      "\n",
      "ðŸ”§ Running global calibration...\n",
      "âœ” Tissue threshold: 0.100\n",
      "âœ” Blur threshold  : 0.000021\n",
      "\n",
      "[1/111] Processing YG_0CBM148C1MFN_wsi.svs\n",
      "ðŸ‘¤ Slide        : YG_0CBM148C1MFN_wsi.svs\n",
      "ðŸ§© Tiles used   : 149\n",
      "ðŸ§¬ Nuclei count: 261\n",
      "ðŸ§  DINO dims   : 45\n",
      "\n",
      "[2/111] Processing YG_0PGQQ6USQ9JB_wsi.svs\n",
      "ðŸ‘¤ Slide        : YG_0PGQQ6USQ9JB_wsi.svs\n",
      "ðŸ§© Tiles used   : 141\n",
      "ðŸ§¬ Nuclei count: 281\n",
      "ðŸ§  DINO dims   : 43\n",
      "\n",
      "[3/111] Processing YG_2I5MDHB0AXEA_wsi.svs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 315\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“„ QC report saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQC_REPORT_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 271\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_tiles \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid tiles after QC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m dino_feats \u001b[38;5;241m=\u001b[39m \u001b[43mdino\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m dino_k \u001b[38;5;241m=\u001b[39m pca_reduce(dino_feats)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ‘¤ Slide        : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslide_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 191\u001b[0m, in \u001b[0;36mDinoExtractor.extract\u001b[1;34m(self, tiles)\u001b[0m\n\u001b[0;32m    189\u001b[0m     img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 191\u001b[0m         feats\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(feats)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\timm\\models\\vision_transformer.py:1273\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, attn_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m-> 1273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\timm\\models\\vision_transformer.py:1228\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1228\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\timm\\models\\vision_transformer.py:206\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, attn_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    205\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), attn_mask\u001b[38;5;241m=\u001b[39mattn_mask)))\n\u001b[1;32m--> 206\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\timm\\layers\\mlp.py:52\u001b[0m, in \u001b[0;36mMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q1-LEVEL TILE-BASED HISTOLOGY FEATURE DISCOVERY PIPELINE\n",
    "# Fully data-driven | No arbitrary thresholds | Resume-safe\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import timm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.filters import threshold_otsu, laplace\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (ONLY NON-LEARNABLE ITEMS)\n",
    "# ============================================================\n",
    "\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUT_DIR = r\"D:\\combined_q1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "FEATURE_CSV = os.path.join(OUT_DIR, \"histology_features_all.csv\")\n",
    "QC_REPORT_CSV = os.path.join(OUT_DIR, \"qc_report.csv\")\n",
    "CHECKPOINT_FILE = os.path.join(OUT_DIR, \"checkpointtt.json\")\n",
    "CALIBRATION_FILE = os.path.join(OUT_DIR, \"calibrationnn.json\")\n",
    "\n",
    "TILE_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINTING\n",
    "# ============================================================\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"processed\": [], \"failed\": []}\n",
    "\n",
    "def save_checkpoint(processed, failed):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"processed\": list(processed),\n",
    "            \"failed\": list(failed),\n",
    "        }, f, indent=2)\n",
    "\n",
    "# ============================================================\n",
    "# TILE UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def read_tile(slide, x, y, level):\n",
    "    return np.array(\n",
    "        slide.read_region((x, y), level, (TILE_SIZE, TILE_SIZE)).convert(\"RGB\")\n",
    "    )\n",
    "\n",
    "def tissue_mask(tile):\n",
    "    gray = rgb2gray(tile)\n",
    "    t = threshold_otsu(gray)\n",
    "    mask = gray < t\n",
    "    mask = remove_small_objects(mask, 500)\n",
    "    return binary_dilation(mask, disk(3))\n",
    "\n",
    "def blur_score(tile):\n",
    "    return laplace(rgb2gray(tile)).var()\n",
    "\n",
    "def background_ratio(tile):\n",
    "    return np.mean(tile) / 255.0\n",
    "\n",
    "# ============================================================\n",
    "# TILE SAMPLING (SYSTEMATIC, TILE-BASED)\n",
    "# ============================================================\n",
    "\n",
    "def sample_tiles(slide_path, max_tiles=300):\n",
    "    slide = openslide.OpenSlide(slide_path)\n",
    "    level = slide.get_best_level_for_downsample(1)\n",
    "    w, h = slide.level_dimensions[level]\n",
    "\n",
    "    tiles = []\n",
    "    step = TILE_SIZE\n",
    "\n",
    "    for y in range(0, h - TILE_SIZE, step):\n",
    "        for x in range(0, w - TILE_SIZE, step):\n",
    "            if len(tiles) >= max_tiles:\n",
    "                break\n",
    "            tile = read_tile(slide, x, y, level)\n",
    "            if background_ratio(tile) < 0.95:\n",
    "                tiles.append(tile)\n",
    "        if len(tiles) >= max_tiles:\n",
    "            break\n",
    "\n",
    "    slide.close()\n",
    "    return tiles\n",
    "\n",
    "# ============================================================\n",
    "# DATA-DRIVEN THRESHOLD OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "def optimize_tissue_threshold(tiles):\n",
    "    thresholds = np.linspace(0.1, 0.9, 17)\n",
    "    scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        coverage = []\n",
    "        for tile in tiles:\n",
    "            m = tissue_mask(tile)\n",
    "            if m.mean() >= t:\n",
    "                coverage.append(m.mean())\n",
    "        scores.append(np.var(coverage) if len(coverage) > 20 else 0)\n",
    "\n",
    "    best_t = thresholds[np.argmax(scores)]\n",
    "\n",
    "    plt.plot(thresholds, scores, marker=\"o\")\n",
    "    plt.xlabel(\"Tissue Fraction Threshold\")\n",
    "    plt.ylabel(\"Stability (Variance)\")\n",
    "    plt.title(\"Optimal Tissue Threshold\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"tissue_threshold_optimization.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return float(best_t)\n",
    "\n",
    "def estimate_blur_threshold(tiles):\n",
    "    scores = np.array([blur_score(t) for t in tiles])\n",
    "    threshold = np.percentile(scores, 10)\n",
    "\n",
    "    plt.hist(scores, bins=50)\n",
    "    plt.axvline(threshold, color=\"red\")\n",
    "    plt.title(\"Blur Score Distribution\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"blur_threshold.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return float(threshold)\n",
    "\n",
    "# ============================================================\n",
    "# MORPHOLOGY FEATURES\n",
    "# ============================================================\n",
    "\n",
    "def morphology_features(tile, mask):\n",
    "    labeled = label(mask)\n",
    "    regions = regionprops(labeled)\n",
    "    if len(regions) == 0:\n",
    "        return None\n",
    "\n",
    "    areas = [r.area for r in regions]\n",
    "    ecc = [r.eccentricity for r in regions]\n",
    "\n",
    "    return {\n",
    "        \"nuclei_count\": len(regions),\n",
    "        \"area_mean\": np.mean(areas),\n",
    "        \"area_std\": np.std(areas),\n",
    "        \"ecc_mean\": np.mean(ecc),\n",
    "        \"ecc_std\": np.std(ecc),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# TEXTURE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "def texture_features(tile):\n",
    "    gray = rgb2gray(tile)\n",
    "    hist, _ = np.histogram(gray, bins=32, density=True)\n",
    "    entropy = -np.sum(hist * np.log(hist + 1e-8))\n",
    "    return {\n",
    "        \"gray_mean\": np.mean(gray),\n",
    "        \"gray_std\": np.std(gray),\n",
    "        \"gray_entropy\": entropy,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# DINO FEATURE EXTRACTOR\n",
    "# ============================================================\n",
    "\n",
    "class DinoExtractor:\n",
    "    def __init__(self):\n",
    "        self.model = timm.create_model(\n",
    "            \"vit_small_patch16_224.dino\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "    def extract(self, tiles):\n",
    "        feats = []\n",
    "        for t in tiles:\n",
    "            img = torch.tensor(t / 255.0).permute(2,0,1).unsqueeze(0).float().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                feats.append(self.model(img).cpu().numpy().squeeze())\n",
    "        return np.array(feats)\n",
    "\n",
    "def pca_reduce(features, tag):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(features)\n",
    "\n",
    "    pca = PCA()\n",
    "    Xp = pca.fit_transform(X)\n",
    "    cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    k = np.argmax(cum >= 0.95) + 1\n",
    "\n",
    "    plt.plot(cum)\n",
    "    plt.axhline(0.95, color=\"red\")\n",
    "    plt.title(f\"PCA Variance â€“ {tag}\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"pca_{tag}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return Xp[:, :k], k\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE (RESUME SAFE)\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    slides = sorted([f for f in os.listdir(SVS_DIR) if f.lower().endswith(\".svs\")])\n",
    "    checkpoint = load_checkpoint()\n",
    "\n",
    "    processed = set(checkpoint[\"processed\"])\n",
    "    failed = set(checkpoint[\"failed\"])\n",
    "\n",
    "    results = []\n",
    "    qc_rows = []\n",
    "\n",
    "    if os.path.exists(FEATURE_CSV):\n",
    "        results = pd.read_csv(FEATURE_CSV).to_dict(\"records\")\n",
    "    if os.path.exists(QC_REPORT_CSV):\n",
    "        qc_rows = pd.read_csv(QC_REPORT_CSV).to_dict(\"records\")\n",
    "\n",
    "    print(f\"\\nTotal slides: {len(slides)}\")\n",
    "    print(f\"Processed: {len(processed)} | Failed: {len(failed)} | Remaining: {len(slides)-len(processed)-len(failed)}\")\n",
    "\n",
    "    # ---- GLOBAL CALIBRATION (ONCE) ----\n",
    "    if not os.path.exists(CALIBRATION_FILE):\n",
    "        print(\"\\nðŸ”§ Running global calibration...\")\n",
    "        sample_tiles_all = []\n",
    "        for s in slides[:5]:\n",
    "            sample_tiles_all.extend(sample_tiles(os.path.join(SVS_DIR, s)))\n",
    "\n",
    "        tissue_t = optimize_tissue_threshold(sample_tiles_all)\n",
    "        blur_t = estimate_blur_threshold(sample_tiles_all)\n",
    "\n",
    "        with open(CALIBRATION_FILE, \"w\") as f:\n",
    "            json.dump({\"tissue_threshold\": tissue_t, \"blur_threshold\": blur_t}, f, indent=2)\n",
    "    else:\n",
    "        with open(CALIBRATION_FILE) as f:\n",
    "            calib = json.load(f)\n",
    "            tissue_t = calib[\"tissue_threshold\"]\n",
    "            blur_t = calib[\"blur_threshold\"]\n",
    "\n",
    "    print(f\"âœ” Tissue threshold: {tissue_t:.3f}\")\n",
    "    print(f\"âœ” Blur threshold  : {blur_t:.3f}\")\n",
    "\n",
    "    dino = DinoExtractor()\n",
    "\n",
    "    for idx, slide_name in enumerate(slides, 1):\n",
    "        if slide_name in processed or slide_name in failed:\n",
    "            print(f\"[SKIP] {slide_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{idx}/{len(slides)}] Processing {slide_name}\")\n",
    "        path = os.path.join(SVS_DIR, slide_name)\n",
    "\n",
    "        try:\n",
    "            tiles = sample_tiles(path)\n",
    "            morph_list, tex_list = [], []\n",
    "\n",
    "            for tile in tiles:\n",
    "                mask = tissue_mask(tile)\n",
    "                if mask.mean() < tissue_t:\n",
    "                    continue\n",
    "                if blur_score(tile) < blur_t:\n",
    "                    continue\n",
    "\n",
    "                mf = morphology_features(tile, mask)\n",
    "                tf = texture_features(tile)\n",
    "                if mf:\n",
    "                    morph_list.append(list(mf.values()))\n",
    "                    tex_list.append(list(tf.values()))\n",
    "\n",
    "            if len(morph_list) < 5:\n",
    "                raise RuntimeError(\"Insufficient valid tiles\")\n",
    "\n",
    "            dino_feats = dino.extract(tiles)\n",
    "            dino_red, dino_k = pca_reduce(dino_feats, slide_name)\n",
    "\n",
    "            row = {\n",
    "                \"slide_id\": slide_name,\n",
    "                \"morph_features\": len(morph_list[0]),\n",
    "                \"texture_features\": len(tex_list[0]),\n",
    "                \"dino_features\": dino_k,\n",
    "                \"total_features\": len(morph_list[0]) + len(tex_list[0]) + dino_k\n",
    "            }\n",
    "\n",
    "            results.append(row)\n",
    "            processed.add(slide_name)\n",
    "            qc_rows.append({\"slide_id\": slide_name, \"status\": \"success\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed: {e}\")\n",
    "            failed.add(slide_name)\n",
    "            qc_rows.append({\"slide_id\": slide_name, \"status\": \"failed\", \"reason\": str(e)})\n",
    "\n",
    "        # SAVE EVERY SLIDE\n",
    "        pd.DataFrame(results).to_csv(FEATURE_CSV, index=False)\n",
    "        pd.DataFrame(qc_rows).to_csv(QC_REPORT_CSV, index=False)\n",
    "        save_checkpoint(processed, failed)\n",
    "\n",
    "    print(\"\\nâœ… PIPELINE COMPLETED\")\n",
    "    print(f\"Features saved: {FEATURE_CSV}\")\n",
    "    print(f\"QC report saved: {QC_REPORT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962f44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/111] YG_2VWCV5YWB078_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 26599\n",
      "\n",
      "[5/111] YG_30TUKBI1ZXBK_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 32721\n",
      "\n",
      "[6/111] YG_31S9L6RD6RCA_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 15647\n",
      "\n",
      "[7/111] YG_34W2PP4X6FL6_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 5346\n",
      "\n",
      "[8/111] YG_37RLQEBG98MP_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 24461\n",
      "\n",
      "[9/111] YG_38SD26C9HLLT_wsi.svs\n",
      "ðŸ§© Tiles   : 34\n",
      "ðŸ§¬ Nuclei : 2088\n",
      "\n",
      "[10/111] YG_3LUYSEZA89OT_wsi.svs\n",
      "ðŸ§© Tiles   : 148\n",
      "ðŸ§¬ Nuclei : 6215\n",
      "\n",
      "[11/111] YG_3OAF908JG3XG_wsi.svs\n",
      "ðŸ§© Tiles   : 254\n",
      "ðŸ§¬ Nuclei : 5636\n",
      "\n",
      "[12/111] YG_3ULZIC6OE5NB_wsi.svs\n",
      "ðŸ§© Tiles   : 120\n",
      "ðŸ§¬ Nuclei : 6887\n",
      "\n",
      "[13/111] YG_3YJ63A56N6VQ_wsi.svs\n",
      "ðŸ§© Tiles   : 161\n",
      "ðŸ§¬ Nuclei : 3768\n",
      "\n",
      "[14/111] YG_4M3SWS9DT0W0_wsi.svs\n",
      "ðŸ§© Tiles   : 207\n",
      "ðŸ§¬ Nuclei : 5337\n",
      "\n",
      "[15/111] YG_4RD15Z2MNGTF_wsi.svs\n",
      "ðŸ§© Tiles   : 70\n",
      "ðŸ§¬ Nuclei : 3325\n",
      "\n",
      "[16/111] YG_5LPM5R5PDW2S_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11721\n",
      "\n",
      "[17/111] YG_5WXJER534E8W_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 23359\n",
      "\n",
      "[18/111] YG_62XGKPMBQUTH_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 20126\n",
      "\n",
      "[19/111] YG_6ANW17ML2ZXY_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 5491\n",
      "\n",
      "[20/111] YG_6NJKER3GCTC9_wsi.svs\n",
      "ðŸ§© Tiles   : 14\n",
      "ðŸ§¬ Nuclei : 279\n",
      "\n",
      "[21/111] YG_6U4LW891P2JA_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 22864\n",
      "\n",
      "[22/111] YG_748ADE23A96L_wsi.svs\n",
      "ðŸ§© Tiles   : 164\n",
      "ðŸ§¬ Nuclei : 2197\n",
      "\n",
      "[23/111] YG_7C0IKK9GHJ7Z_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 33670\n",
      "\n",
      "[24/111] YG_7E5NDHCCM5NM_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 33021\n",
      "\n",
      "[25/111] YG_7Q9B55HPYEP8_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 21544\n",
      "\n",
      "[26/111] YG_827U9W3DNZW2_wsi.svs\n",
      "ðŸ§© Tiles   : 177\n",
      "ðŸ§¬ Nuclei : 7884\n",
      "\n",
      "[27/111] YG_8K0CM0O5X8OW_wsi.svs\n",
      "ðŸ§© Tiles   : 88\n",
      "ðŸ§¬ Nuclei : 2150\n",
      "\n",
      "[28/111] YG_8TBC6OEVGY2D_wsi.svs\n",
      "ðŸ§© Tiles   : 112\n",
      "ðŸ§¬ Nuclei : 4925\n",
      "\n",
      "[29/111] YG_985XO5NHL516_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 113314\n",
      "\n",
      "[30/111] YG_9N53OC2E1U4S_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 41733\n",
      "\n",
      "[31/111] YG_9P6T37XA3XDG_wsi.svs\n",
      "ðŸ§© Tiles   : 229\n",
      "ðŸ§¬ Nuclei : 15947\n",
      "\n",
      "[32/111] YG_9VLV5DLCK9YI_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 42946\n",
      "\n",
      "[33/111] YG_A9LZ4KMS3CMI_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 20432\n",
      "\n",
      "[34/111] YG_AGXLFWLSM70S_wsi.svs\n",
      "ðŸ§© Tiles   : 268\n",
      "ðŸ§¬ Nuclei : 14360\n",
      "\n",
      "[35/111] YG_B0JG2H7KNW0V_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 27996\n",
      "\n",
      "[36/111] YG_BQQF9664YAU5_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11361\n",
      "\n",
      "[37/111] YG_CIVF62SD7HC3_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 31156\n",
      "\n",
      "[38/111] YG_D79WDV6LM56N_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 67860\n",
      "\n",
      "[39/111] YG_DAG9VMDZP5V1_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 18255\n",
      "\n",
      "[40/111] YG_DFB3TW2VVCYK_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 28926\n",
      "\n",
      "[41/111] YG_E0SJA5ZEGI6T_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 21636\n",
      "\n",
      "[42/111] YG_EBD55KZ1V8NM_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 22813\n",
      "\n",
      "[43/111] YG_ES6X68HBRT2B_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 26137\n",
      "\n",
      "[44/111] YG_EZWYLLB6BKIS_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 23767\n",
      "\n",
      "[45/111] YG_F2VHX1DGAYIC_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 23581\n",
      "\n",
      "[46/111] YG_FBCLQ9J1UEZY_wsi.svs\n",
      "ðŸ§© Tiles   : 57\n",
      "ðŸ§¬ Nuclei : 1002\n",
      "\n",
      "[47/111] YG_FH82F8IE9K3H_wsi.svs\n",
      "ðŸ§© Tiles   : 51\n",
      "ðŸ§¬ Nuclei : 1300\n",
      "\n",
      "[48/111] YG_FIUSCJCISOT2_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 39655\n",
      "\n",
      "[49/111] YG_FLTKHZ9CCQVP_wsi.svs\n",
      "ðŸ§© Tiles   : 39\n",
      "ðŸ§¬ Nuclei : 985\n",
      "\n",
      "[50/111] YG_FZWUWT3HOB1V_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 7870\n",
      "\n",
      "[51/111] YG_GK99VZLND6JR_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 8464\n",
      "\n",
      "[52/111] YG_HAEAKFOSOSWC_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 23915\n",
      "\n",
      "[53/111] YG_HHJSDG645U1U_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 27783\n",
      "\n",
      "[54/111] YG_HUD40BU36H9E_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 24376\n",
      "\n",
      "[55/111] YG_IONBN2T5QSBF_wsi.svs\n",
      "ðŸ§© Tiles   : 106\n",
      "ðŸ§¬ Nuclei : 1058\n",
      "\n",
      "[56/111] YG_JCGGWVPZ3S0Z_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 23611\n",
      "\n",
      "[57/111] YG_JEBDFNTNS3Z1_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 34921\n",
      "\n",
      "[58/111] YG_JJ6C3ZNB2CLF_wsi.svs\n",
      "ðŸ§© Tiles   : 122\n",
      "ðŸ§¬ Nuclei : 3315\n",
      "\n",
      "[59/111] YG_K04YUWH1VDV0_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 46052\n",
      "\n",
      "[60/111] YG_K2W6NCT5QEUZ_wsi.svs\n",
      "ðŸ§© Tiles   : 267\n",
      "ðŸ§¬ Nuclei : 3956\n",
      "\n",
      "[61/111] YG_KA6BIZWS7TU5_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 17005\n",
      "\n",
      "[62/111] YG_KB1LCW4PCOIS_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 7246\n",
      "\n",
      "[63/111] YG_KUDUDTQCIQMT_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 27412\n",
      "\n",
      "[64/111] YG_L0WV7DL4DNW0_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 41687\n",
      "\n",
      "[65/111] YG_LDY21C4TSC7L_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 14113\n",
      "\n",
      "[66/111] YG_LG5SV5PWDEBF_wsi.svs\n",
      "ðŸ§© Tiles   : 182\n",
      "ðŸ§¬ Nuclei : 7115\n",
      "\n",
      "[67/111] YG_LPO2XULIXN9W_wsi.svs\n",
      "ðŸ§© Tiles   : 46\n",
      "ðŸ§¬ Nuclei : 1370\n",
      "\n",
      "[68/111] YG_LSMF1KDV6GFL_wsi.svs\n",
      "ðŸ§© Tiles   : 183\n",
      "ðŸ§¬ Nuclei : 6198\n",
      "\n",
      "[69/111] YG_MGO4964VSKLW_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 13036\n",
      "\n",
      "[70/111] YG_MGRE5V4Q0I2R_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 26821\n",
      "\n",
      "[71/111] YG_MQP6BODVI9B3_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 10413\n",
      "\n",
      "[72/111] YG_MSPIZ41HGCU6_wsi.svs\n",
      "ðŸ§© Tiles   : 177\n",
      "ðŸ§¬ Nuclei : 6808\n",
      "\n",
      "[73/111] YG_N1WAQSN5IRL2_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 20124\n",
      "\n",
      "[74/111] YG_N44P5EWB2TGI_wsi.svs\n",
      "ðŸ§© Tiles   : 71\n",
      "ðŸ§¬ Nuclei : 2529\n",
      "\n",
      "[75/111] YG_NHXKSO7LZ3OZ_wsi.svs\n",
      "ðŸ§© Tiles   : 175\n",
      "ðŸ§¬ Nuclei : 4869\n",
      "\n",
      "[76/111] YG_OKZMVD17QKZS_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 94758\n",
      "\n",
      "[77/111] YG_OONB74IHNGUI_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11181\n",
      "\n",
      "[78/111] YG_OUD88SBAPAOC_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11139\n",
      "\n",
      "[79/111] YG_P8W7SBCME4VH_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 41348\n",
      "\n",
      "[80/111] YG_PJ6VL4EIAVKN_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 17416\n",
      "\n",
      "[81/111] YG_PN1GGCPTKE1T_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 19049\n",
      "\n",
      "[82/111] YG_PT6LAWSXHST3_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 12928\n",
      "\n",
      "[83/111] YG_Q70O5ZGBYWBC_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11127\n",
      "\n",
      "[84/111] YG_QDPH4L1OB5Q3_wsi.svs\n",
      "ðŸ§© Tiles   : 278\n",
      "ðŸ§¬ Nuclei : 10865\n",
      "\n",
      "[85/111] YG_RA7N8XKCHWJW_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 33900\n",
      "\n",
      "[86/111] YG_RIKABNRC1GGU_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 20477\n",
      "\n",
      "[87/111] YG_RLQOQB3PRCA6_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 17846\n",
      "\n",
      "[88/111] YG_S77I03P4O3LA_wsi.svs\n",
      "ðŸ§© Tiles   : 147\n",
      "ðŸ§¬ Nuclei : 4309\n",
      "\n",
      "[89/111] YG_S98GUOYZYFJD_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 34594\n",
      "\n",
      "[90/111] YG_SAQJZ877R0XQ_wsi.svs\n",
      "ðŸ§© Tiles   : 36\n",
      "ðŸ§¬ Nuclei : 1359\n",
      "\n",
      "[91/111] YG_SVOESMUJE9C2_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 15852\n",
      "\n",
      "[92/111] YG_TB3261RS6ME1_wsi.svs\n",
      "ðŸ§© Tiles   : 202\n",
      "ðŸ§¬ Nuclei : 4709\n",
      "\n",
      "[93/111] YG_TS0QSN58FLFU_wsi.svs\n",
      "ðŸ§© Tiles   : 101\n",
      "ðŸ§¬ Nuclei : 4629\n",
      "\n",
      "[94/111] YG_TTVMAOQ58J1J_wsi.svs\n",
      "ðŸ§© Tiles   : 180\n",
      "ðŸ§¬ Nuclei : 11647\n",
      "\n",
      "[95/111] YG_U8D90FYR7XQ6_wsi.svs\n",
      "ðŸ§© Tiles   : 89\n",
      "ðŸ§¬ Nuclei : 1030\n",
      "\n",
      "[96/111] YG_UMS3ARN00O7Q_wsi.svs\n",
      "ðŸ§© Tiles   : 241\n",
      "ðŸ§¬ Nuclei : 8933\n",
      "\n",
      "[97/111] YG_USOH1WE4K10O_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 15321\n",
      "\n",
      "[98/111] YG_UZOLBU0UJ0ZP_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 34594\n",
      "\n",
      "[99/111] YG_V6WILIQCL1EM_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 21243\n",
      "\n",
      "[100/111] YG_VLRAC165LOOI_wsi.svs\n",
      "ðŸ§© Tiles   : 254\n",
      "ðŸ§¬ Nuclei : 24927\n",
      "\n",
      "[101/111] YG_VNUGR99FHDRB_wsi.svs\n",
      "ðŸ§© Tiles   : 289\n",
      "ðŸ§¬ Nuclei : 8863\n",
      "\n",
      "[102/111] YG_W7KRO4CZBDJ3_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 27366\n",
      "\n",
      "[103/111] YG_WNJIJZXQW7BT_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 13336\n",
      "\n",
      "[104/111] YG_WY3KFQ1IYNOA_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 56306\n",
      "\n",
      "[105/111] YG_X0H31MQV52A5_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 41862\n",
      "\n",
      "[106/111] YG_XFE7DX3IDU55_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 11614\n",
      "\n",
      "[107/111] YG_XKGYFGZHUMTO_wsi.svs\n",
      "ðŸ§© Tiles   : 191\n",
      "ðŸ§¬ Nuclei : 6470\n",
      "\n",
      "[108/111] YG_XMCGCFTP0CC0_wsi.svs\n",
      "ðŸ§© Tiles   : 300\n",
      "ðŸ§¬ Nuclei : 13422\n",
      "\n",
      "[109/111] YG_XY6SP25JK2UJ_wsi.svs\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.27 GiB for an array with shape (29522, 14873) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 264\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… RAW FEATURE EXTRACTION COMPLETE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 222\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(slides)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m slide \u001b[38;5;241m=\u001b[39m openslide\u001b[38;5;241m.\u001b[39mOpenSlide(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SVS_DIR,f))\n\u001b[1;32m--> 222\u001b[0m mask, lvl \u001b[38;5;241m=\u001b[39m \u001b[43mget_tissue_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslide\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m nuclei, tissue_area \u001b[38;5;241m=\u001b[39m count_nuclei_whole_slide(slide, mask, lvl)\n\u001b[0;32m    224\u001b[0m tiles \u001b[38;5;241m=\u001b[39m extract_tiles(slide, mask, lvl)\n",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36mget_tissue_mask\u001b[1;34m(slide)\u001b[0m\n\u001b[0;32m     52\u001b[0m lvl \u001b[38;5;241m=\u001b[39m slide\u001b[38;5;241m.\u001b[39mget_best_level_for_downsample(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     53\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(slide\u001b[38;5;241m.\u001b[39mread_region((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), lvl, slide\u001b[38;5;241m.\u001b[39mlevel_dimensions[lvl]))[:,:,:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m---> 54\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2gray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m t \u001b[38;5;241m=\u001b[39m threshold_otsu(gray)\n\u001b[0;32m     56\u001b[0m mask \u001b[38;5;241m=\u001b[39m gray \u001b[38;5;241m<\u001b[39m t\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\_shared\\utils.py:445\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\color\\colorconv.py:984\u001b[0m, in \u001b[0;36mrgb2gray\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    982\u001b[0m rgb \u001b[38;5;241m=\u001b[39m _prepare_colorarray(rgb)\n\u001b[0;32m    983\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.2125\u001b[39m, \u001b[38;5;241m0.7154\u001b[39m, \u001b[38;5;241m0.0721\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mrgb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrgb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoeffs\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.27 GiB for an array with shape (29522, 14873) and data type float64"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q1 RAW HISTOLOGY FEATURE EXTRACTION (NO PCA / NO REDUCTION)\n",
    "# Whole-slide nuclear segmentation | Fixed dims | Restart-safe\n",
    "# ============================================================\n",
    "\n",
    "import os, json, warnings, traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUT_DIR = r\"D:\\Q1_RAW_FEATURES\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TILE_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "FEATURE_CSV = f\"{OUT_DIR}/histology_raw_features.csv\"\n",
    "CHECKPOINT = f\"{OUT_DIR}/checkpoint.json\"\n",
    "\n",
    "# =========================\n",
    "# CHECKPOINT\n",
    "# =========================\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT):\n",
    "        return json.load(open(CHECKPOINT))\n",
    "    return {\"done\": []}\n",
    "\n",
    "def save_checkpoint(done):\n",
    "    json.dump({\"done\": done}, open(CHECKPOINT, \"w\"), indent=2)\n",
    "\n",
    "# =========================\n",
    "# WHOLE-SLIDE TISSUE MASK\n",
    "# =========================\n",
    "def get_tissue_mask(slide):\n",
    "    lvl = slide.get_best_level_for_downsample(16)\n",
    "    img = np.array(slide.read_region((0,0), lvl, slide.level_dimensions[lvl]))[:,:,:3]\n",
    "    gray = rgb2gray(img)\n",
    "    t = threshold_otsu(gray)\n",
    "    mask = gray < t\n",
    "    mask = remove_small_objects(mask, 5000)\n",
    "    mask = binary_dilation(mask, disk(5))\n",
    "    return mask, lvl\n",
    "\n",
    "# =========================\n",
    "# WHOLE-SLIDE NUCLEAR SEGMENTATION\n",
    "# =========================\n",
    "def count_nuclei_whole_slide(slide, mask, lvl):\n",
    "    down = slide.level_downsamples[lvl]\n",
    "    H, W = mask.shape\n",
    "    nuclei = 0\n",
    "\n",
    "    for y in range(0, H, TILE_SIZE):\n",
    "        for x in range(0, W, TILE_SIZE):\n",
    "            if mask[y:y+TILE_SIZE, x:x+TILE_SIZE].mean() < 0.1:\n",
    "                continue\n",
    "            x0, y0 = int(x*down), int(y*down)\n",
    "            tile = np.array(\n",
    "                slide.read_region((x0,y0), 0, (TILE_SIZE,TILE_SIZE))\n",
    "            )[:,:,:3]\n",
    "            gray = rgb2gray(tile)\n",
    "            try:\n",
    "                t = threshold_otsu(gray)\n",
    "            except:\n",
    "                continue\n",
    "            nuc = gray < t\n",
    "            nuc = remove_small_objects(nuc, 30)\n",
    "            nuclei += len(regionprops(label(nuc)))\n",
    "\n",
    "    return nuclei, mask.sum()\n",
    "\n",
    "# =========================\n",
    "# TILE EXTRACTION\n",
    "# =========================\n",
    "def extract_tiles(slide, mask, lvl, max_tiles=300):\n",
    "    tiles = []\n",
    "    down = slide.level_downsamples[lvl]\n",
    "    H, W = mask.shape\n",
    "\n",
    "    for y in range(0, H, TILE_SIZE):\n",
    "        for x in range(0, W, TILE_SIZE):\n",
    "            if len(tiles) >= max_tiles:\n",
    "                break\n",
    "            if mask[y:y+TILE_SIZE, x:x+TILE_SIZE].mean() < 0.5:\n",
    "                continue\n",
    "            x0, y0 = int(x*down), int(y*down)\n",
    "            tile = np.array(\n",
    "                slide.read_region((x0,y0), 0, (TILE_SIZE,TILE_SIZE))\n",
    "            )[:,:,:3]\n",
    "            tiles.append(tile)\n",
    "    return tiles\n",
    "\n",
    "# =========================\n",
    "# MORPHOLOGY FEATURES\n",
    "# =========================\n",
    "def morphology_features(tile):\n",
    "    gray = rgb2gray(tile)\n",
    "    t = threshold_otsu(gray)\n",
    "    mask = gray < t\n",
    "    mask = remove_small_objects(mask, 50)\n",
    "    props = regionprops(label(mask))\n",
    "    if len(props) == 0:\n",
    "        return None\n",
    "    areas = [p.area for p in props]\n",
    "    ecc = [p.eccentricity for p in props]\n",
    "    return {\n",
    "        \"nuclei_tile\": len(props),\n",
    "        \"area_mean\": np.mean(areas),\n",
    "        \"area_std\": np.std(areas),\n",
    "        \"ecc_mean\": np.mean(ecc),\n",
    "        \"ecc_std\": np.std(ecc),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# TEXTURE FEATURES\n",
    "# =========================\n",
    "def texture_features(tile):\n",
    "    gray = (rgb2gray(tile) * 255).astype(np.uint8)\n",
    "\n",
    "    glcm = graycomatrix(\n",
    "        gray,\n",
    "        distances=[1, 2, 4],\n",
    "        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "        symmetric=True,\n",
    "        normed=True\n",
    "    )\n",
    "\n",
    "    feats = {\n",
    "        \"glcm_contrast\": graycoprops(glcm, \"contrast\").mean(),\n",
    "        \"glcm_homogeneity\": graycoprops(glcm, \"homogeneity\").mean(),\n",
    "        \"glcm_energy\": graycoprops(glcm, \"energy\").mean(),\n",
    "        \"glcm_correlation\": graycoprops(glcm, \"correlation\").mean(),\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "# =========================\n",
    "# DEEP MODELS\n",
    "# =========================\n",
    "class Dino:\n",
    "    def __init__(self):\n",
    "        self.model = timm.create_model(\n",
    "            \"vit_small_patch16_224.dino\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "    def __call__(self, tiles):\n",
    "        feats = []\n",
    "        for t in tiles:\n",
    "            x = torch.tensor(t/255.).permute(2,0,1).unsqueeze(0).float().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                feats.append(self.model(x).cpu().numpy().squeeze())\n",
    "        return np.array(feats)\n",
    "\n",
    "class ResNet50:\n",
    "    def __init__(self):\n",
    "        self.model = timm.create_model(\n",
    "            \"resnet50\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        ).to(DEVICE).eval()\n",
    "\n",
    "    def __call__(self, tiles):\n",
    "        feats = []\n",
    "        for t in tiles:\n",
    "            x = torch.tensor(t/255.).permute(2,0,1).unsqueeze(0).float().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                feats.append(self.model(x).cpu().numpy().squeeze())\n",
    "        return np.array(feats)\n",
    "\n",
    "# =========================\n",
    "# AGGREGATION\n",
    "# =========================\n",
    "def aggregate(feats, prefix):\n",
    "    row = {}\n",
    "    for stat, fn in {\n",
    "        \"mean\": np.mean,\n",
    "        \"std\": np.std,\n",
    "        \"min\": np.min,\n",
    "        \"max\": np.max,\n",
    "        \"median\": np.median\n",
    "    }.items():\n",
    "        for i,v in enumerate(fn(feats,axis=0)):\n",
    "            row[f\"{prefix}_{stat}_{i}\"] = float(v)\n",
    "    return row\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    slides = [f for f in os.listdir(SVS_DIR) if f.endswith(\".svs\")]\n",
    "    ckpt = load_checkpoint()\n",
    "    done = set(ckpt[\"done\"])\n",
    "\n",
    "    dino = Dino()\n",
    "    resnet = ResNet50()\n",
    "    rows = []\n",
    "\n",
    "    for i,f in enumerate(slides):\n",
    "        if f in done:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(slides)}] {f}\")\n",
    "        slide = openslide.OpenSlide(os.path.join(SVS_DIR,f))\n",
    "\n",
    "        mask, lvl = get_tissue_mask(slide)\n",
    "        nuclei, tissue_area = count_nuclei_whole_slide(slide, mask, lvl)\n",
    "        tiles = extract_tiles(slide, mask, lvl)\n",
    "\n",
    "        print(f\"ðŸ§© Tiles   : {len(tiles)}\")\n",
    "        print(f\"ðŸ§¬ Nuclei : {nuclei}\")\n",
    "\n",
    "        # Morph + texture\n",
    "        morph, text = [], []\n",
    "        for t in tiles:\n",
    "            mf = morphology_features(t)\n",
    "            if mf: morph.append(list(mf.values()))\n",
    "            text.append(list(texture_features(t).values()))\n",
    "\n",
    "        # Deep\n",
    "        dino_feats = dino(tiles)\n",
    "        res_feats  = resnet(tiles)\n",
    "\n",
    "        row = {\n",
    "            \"slide\": f,\n",
    "            \"tiles\": len(tiles),\n",
    "            \"nuclei\": nuclei,\n",
    "            \"tissue_pixels\": tissue_area,\n",
    "            \"nuclei_density\": nuclei / tissue_area\n",
    "        }\n",
    "\n",
    "        if len(morph):\n",
    "            row.update(aggregate(np.array(morph), \"morph\"))\n",
    "        row.update(aggregate(np.array(text), \"texture\"))\n",
    "        row.update(aggregate(dino_feats, \"dino\"))\n",
    "        row.update(aggregate(res_feats, \"resnet\"))\n",
    "\n",
    "        rows.append(row)\n",
    "        done.add(f)\n",
    "        save_checkpoint(list(done))\n",
    "        pd.DataFrame(rows).to_csv(FEATURE_CSV, index=False)\n",
    "\n",
    "        slide.close()\n",
    "\n",
    "    print(\"\\nâœ… RAW FEATURE EXTRACTION COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c4b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1-READY: LUNIT ATOM + INTERPRETABLE FEATURES (PRODUCTION)\n",
      "================================================================================\n",
      "Device: cpu\n",
      "Seed: 42\n",
      "Output: histology_q1_production_final\n",
      "\n",
      "Found 111 SVS files\n",
      "\n",
      "================================================================================\n",
      "STEP 1: OPTIMIZATION (CALIBRATION)\n",
      "================================================================================\n",
      "Calibration slides: 10\n",
      "Processing slides: 111 (ALL - no data loss)\n",
      "\n",
      "METHOD 1: Elbow (Tile Count)\n",
      "âœ… Optimal tiles: 150\n",
      "METHOD 2: Youden's J (Blur)\n",
      "âœ… Blur threshold: 0.2368\n",
      "âš ï¸ Using robust multi-method tissue threshold instead of ROC\n",
      "METHOD 3: Tissue Threshold (Multi-Method)\n",
      "  ðŸ“Š Tissue % distribution:\n",
      "     Samples: 600\n",
      "     Mean: 0.144, Std: 0.124\n",
      "     Min: 0.000, Max: 0.330\n",
      "     P10: 0.000, P50: 0.137\n",
      "  Method A (P25): 0.010\n",
      "  Method B failed: Axis must be specified when shapes of a and weights differ.\n",
      "  Method C (Gap): 0.203\n",
      "  Method D (Mixture): 0.231\n",
      "\n",
      "  ðŸ“Š Multi-Method Results:\n",
      "     A (P25): 0.010\n",
      "     B (Otsu): 0.010\n",
      "     C (Gap): 0.203\n",
      "     D (Mixture): 0.231\n",
      "  ðŸŽ¯ Consensus (median): 0.250\n",
      "âœ… Tissue threshold: 0.25 (robust multi-method)\n",
      "METHOD 4: Bootstrap\n",
      "âœ… Bootstrap: 0.1454Â±0.0112\n",
      "METHOD 5: Entropy (Stain)\n",
      "âœ… Stain: means=[0.778 0.602 0.882]\n",
      "âœ… Optimization results saved\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "Loading ATOM (ResNet-50)...\n",
      "âœ… ATOM loaded (2048D)\n",
      "\n",
      "\n",
      "[1/111] YG_P8W7SBCME4VH_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[2/111] YG_3OAF908JG3XG_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[3/111] YG_30TUKBI1ZXBK_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[4/111] YG_RA7N8XKCHWJW_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[5/111] YG_LDY21C4TSC7L_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[6/111] YG_MGO4964VSKLW_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[7/111] YG_9P6T37XA3XDG_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[8/111] YG_FBCLQ9J1UEZY_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[9/111] YG_USOH1WE4K10O_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[10/111] YG_3ULZIC6OE5NB_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "  ðŸ’¾ Checkpoint saved (10 slides processed)\n",
      "\n",
      "[11/111] YG_PJ6VL4EIAVKN_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[12/111] YG_PN1GGCPTKE1T_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[13/111] YG_0CBM148C1MFN_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[14/111] YG_PT6LAWSXHST3_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  ðŸ” Removed 1 outlier tiles\n",
      "  âœ… Final: 149 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[15/111] YG_6ANW17ML2ZXY_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[16/111] YG_MQP6BODVI9B3_wsi.svs\n",
      "  Extracting ATOM features from 150 tiles...\n",
      "  âœ“ Extracted 150 tile features\n",
      "  âœ… Final: 150 tiles\n",
      "  âœ… Success: 150 tiles\n",
      "\n",
      "[17/111] YG_JEBDFNTNS3Z1_wsi.svs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "remove: path should be string, bytes or os.PathLike, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mmessagestream.pyx:91\u001b[0m, in \u001b[0;36mscipy._lib.messagestream.MessageStream.close\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: remove: path should be string, bytes or os.PathLike, not NoneType"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'scipy._lib.messagestream.MessageStream.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"messagestream.pyx\", line 91, in scipy._lib.messagestream.MessageStream.close\n",
      "TypeError: remove: path should be string, bytes or os.PathLike, not NoneType\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 838\u001b[0m\n\u001b[0;32m    835\u001b[0m     log_msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/optimization.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 838\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 762\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;66;03m# Extract interpretable features\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [interp\u001b[38;5;241m.\u001b[39mextract(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tiles]\n\u001b[0;32m    763\u001b[0m idf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ifs)\n\u001b[0;32m    765\u001b[0m iagg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslide\u001b[39m\u001b[38;5;124m'\u001b[39m: fn}\n",
      "Cell \u001b[1;32mIn[8], line 762\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;66;03m# Extract interpretable features\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [\u001b[43minterp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tiles]\n\u001b[0;32m    763\u001b[0m idf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ifs)\n\u001b[0;32m    765\u001b[0m iagg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslide\u001b[39m\u001b[38;5;124m'\u001b[39m: fn}\n",
      "Cell \u001b[1;32mIn[8], line 532\u001b[0m, in \u001b[0;36mInterpExtractor.extract\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract all interpretable features from tile\"\"\"\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnuclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march(t), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture(t)}\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# Return zeros on error\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_m\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_s\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdens\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcirc\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;241m|\u001b[39m \\\n\u001b[0;32m    536\u001b[0m            {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124march_org\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124march_uni\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m} \u001b[38;5;241m|\u001b[39m \\\n\u001b[0;32m    537\u001b[0m            {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtex_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrast\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhomogeneity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Cell \u001b[1;32mIn[8], line 489\u001b[0m, in \u001b[0;36mInterpExtractor.nuclear\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m    487\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x\u001b[38;5;241m.\u001b[39marea \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[0;32m    488\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39marea\u001b[38;5;241m/\u001b[39m(x\u001b[38;5;241m.\u001b[39mperimeter\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[1;32m--> 489\u001b[0m s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x\u001b[38;5;241m.\u001b[39msolidity \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_cnt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(r),\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_area_m\u001b[39m\u001b[38;5;124m'\u001b[39m: a\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_sol\u001b[39m\u001b[38;5;124m'\u001b[39m: s\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    498\u001b[0m }\n",
      "Cell \u001b[1;32mIn[8], line 489\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    487\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x\u001b[38;5;241m.\u001b[39marea \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[0;32m    488\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39marea\u001b[38;5;241m/\u001b[39m(x\u001b[38;5;241m.\u001b[39mperimeter\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[1;32m--> 489\u001b[0m s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolidity\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m r])\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_cnt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(r),\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_area_m\u001b[39m\u001b[38;5;124m'\u001b[39m: a\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuc_sol\u001b[39m\u001b[38;5;124m'\u001b[39m: s\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    498\u001b[0m }\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\measure\\_regionprops.py:675\u001b[0m, in \u001b[0;36mRegionProperties.solidity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msolidity\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marea \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marea_convex\u001b[49m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\measure\\_regionprops.py:243\u001b[0m, in \u001b[0;36m_cached.<locals>.wrapper\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(obj)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[1;32m--> 243\u001b[0m     cache[prop] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[prop]\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\measure\\_regionprops.py:461\u001b[0m, in \u001b[0;36mRegionProperties.area_convex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;129m@_cached\u001b[39m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21marea_convex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_convex\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pixel_area\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\measure\\_regionprops.py:243\u001b[0m, in \u001b[0;36m_cached.<locals>.wrapper\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(obj)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[1;32m--> 243\u001b[0m     cache[prop] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[prop]\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\measure\\_regionprops.py:468\u001b[0m, in \u001b[0;36mRegionProperties.image_convex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;129m@_cached\u001b[39m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimage_convex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphology\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvex_hull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convex_hull_image\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvex_hull_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\morphology\\convex_hull.py:148\u001b[0m, in \u001b[0;36mconvex_hull_image\u001b[1;34m(image, offset_coordinates, tolerance, include_borders)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Find the convex hull\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     hull \u001b[38;5;241m=\u001b[39m \u001b[43mConvexHull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m QhullError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    150\u001b[0m     warn(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get convex hull image. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning empty image, see error message below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m     )\n",
      "File \u001b[1;32m_qhull.pyx:2488\u001b[0m, in \u001b[0;36mscipy.spatial._qhull.ConvexHull.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_qhull.pyx:267\u001b[0m, in \u001b[0;36mscipy.spatial._qhull._Qhull.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmessagestream.pyx:29\u001b[0m, in \u001b[0;36mscipy._lib.messagestream.MessageStream.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\tempfile.py:357\u001b[0m, in \u001b[0;36mmkstemp\u001b[1;34m(suffix, prefix, dir, text)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m     flags \u001b[38;5;241m=\u001b[39m _bin_openflags\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    254\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkstemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPLETE Q1-READY HISTOLOGY PIPELINE - PRODUCTION VERSION\n",
    "# LUNIT ATOM + INTERPRETABLE FEATURES + ROBUST OPTIMIZATION\n",
    "# ALL SLIDES PROCESSED (NO DATA LOSS)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import timm\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# REPRODUCIBILITY\n",
    "# ===============================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# ===============================\n",
    "# CONFIGURATION\n",
    "# ===============================\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUTPUT_DIR = \"histology_q1_production_final\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(f\"{OUTPUT_DIR}/figures\").mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: LUNIT ATOM + INTERPRETABLE FEATURES (PRODUCTION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {RANDOM_SEED}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    \"\"\"Thread-safe logging\"\"\"\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ===============================\n",
    "# OPTIMIZER - FIXED & ROBUST\n",
    "# ===============================\n",
    "class Optimizer:\n",
    "    \"\"\"Production-grade optimizer with robust error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t):\n",
    "        \"\"\"Check if tile is background\"\"\"\n",
    "        return np.mean(t) > 220\n",
    "    \n",
    "    def _blur(self, t):\n",
    "        \"\"\"Compute blur score with gradient boost for low contrast\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    \n",
    "    def _mask(self, t):\n",
    "        \"\"\"Tissue segmentation mask\"\"\"\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        m = binary_dilation(m, disk(3))\n",
    "        return m\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        \"\"\"Elbow method for optimal tile count\"\"\"\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                \n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(cnts)<3:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using default: 100\")\n",
    "            return 100\n",
    "        \n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        d2 = np.gradient(np.gradient(vars))\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(d2))]), 200))\n",
    "        \n",
    "        self.results['elbow'] = {'optimal': opt, 'samples': len(cnts)}\n",
    "        log_msg(f\"âœ… Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        \"\"\"Youden's J statistic for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        \n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 100:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using default: 0.1\")\n",
    "            return 0.1\n",
    "        \n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        \n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            # Fallback to percentile\n",
    "            opt = float(np.percentile(ba, 5))\n",
    "            log_msg(f\"âœ… Blur threshold (percentile): {opt:.4f}\")\n",
    "            return opt\n",
    "        \n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = []\n",
    "        \n",
    "        for th in ths:\n",
    "            tp = (ba[emp]<th).sum()\n",
    "            fn = (ba[tis]>=th).sum()\n",
    "            fp = (ba[tis]<th).sum()\n",
    "            tn = (ba[emp]>=th).sum()\n",
    "            \n",
    "            sensitivity = tp/(tp+fn+1e-8)\n",
    "            specificity = tn/(tn+fp+1e-8)\n",
    "            js.append(sensitivity + specificity - 1)\n",
    "        \n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt, 'j': float(max(js))}\n",
    "        log_msg(f\"âœ… Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        \"\"\"\n",
    "        ROBUST tissue threshold using multiple methods\n",
    "        Q1-ready: No arbitrary defaults, data-driven fallbacks\n",
    "        \"\"\"\n",
    "        log_msg(\"METHOD 3: Tissue Threshold (Multi-Method)\")\n",
    "        \n",
    "        tisss = []\n",
    "        \n",
    "        # Collect tissue percentages\n",
    "        for p in self.slides[:5]:  # Use more slides\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600:  # More samples\n",
    "                            break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tp = self._mask(t).sum()/t.size\n",
    "                            tisss.append(tp)\n",
    "                    if len(tisss)>=600:\n",
    "                        break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tisss) < 100:\n",
    "            log_msg(\"  âŒ CRITICAL: Insufficient data for tissue threshold\")\n",
    "            return None\n",
    "        \n",
    "        ta = np.array(tisss)\n",
    "        \n",
    "        log_msg(f\"  ðŸ“Š Tissue % distribution:\")\n",
    "        log_msg(f\"     Samples: {len(ta)}\")\n",
    "        log_msg(f\"     Mean: {ta.mean():.3f}, Std: {ta.std():.3f}\")\n",
    "        log_msg(f\"     Min: {ta.min():.3f}, Max: {ta.max():.3f}\")\n",
    "        log_msg(f\"     P10: {np.percentile(ta, 10):.3f}, P50: {np.percentile(ta, 50):.3f}\")\n",
    "        \n",
    "        # METHOD A: Percentile-based (conservative)\n",
    "        # Use 25th percentile - excludes mostly-background tiles\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        log_msg(f\"  Method A (P25): {method_a:.3f}\")\n",
    "        \n",
    "        # METHOD B: Otsu on tissue distribution\n",
    "        try:\n",
    "            # Bin the tissue percentages\n",
    "            hist, bin_edges = np.histogram(ta, bins=50)\n",
    "            # Find threshold that separates background-heavy from tissue-rich\n",
    "            cumsum = np.cumsum(hist)\n",
    "            total = cumsum[-1]\n",
    "            \n",
    "            max_var = 0\n",
    "            best_th = 0.3\n",
    "            \n",
    "            for i in range(1, len(hist)-1):\n",
    "                w0 = cumsum[i] / total\n",
    "                w1 = 1 - w0\n",
    "                \n",
    "                if w0 == 0 or w1 == 0:\n",
    "                    continue\n",
    "                \n",
    "                m0 = np.average(bin_edges[:i+1], weights=hist[:i+1]) if hist[:i+1].sum() > 0 else 0\n",
    "                m1 = np.average(bin_edges[i+1:], weights=hist[i+1:]) if hist[i+1:].sum() > 0 else 0\n",
    "                \n",
    "                var = w0 * w1 * (m0 - m1)**2\n",
    "                \n",
    "                if var > max_var:\n",
    "                    max_var = var\n",
    "                    best_th = bin_edges[i]\n",
    "            \n",
    "            method_b = float(best_th)\n",
    "            log_msg(f\"  Method B (Otsu): {method_b:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method B failed: {e}\")\n",
    "            method_b = method_a\n",
    "        \n",
    "        # METHOD C: Gap statistic\n",
    "        # Find largest gap in sorted tissue percentages\n",
    "        try:\n",
    "            sorted_ta = np.sort(ta)\n",
    "            gaps = np.diff(sorted_ta)\n",
    "            \n",
    "            # Find gap in range [0.2, 0.6]\n",
    "            valid_gaps = []\n",
    "            for i, gap in enumerate(gaps):\n",
    "                if 0.2 <= sorted_ta[i] <= 0.6:\n",
    "                    valid_gaps.append((gap, sorted_ta[i]))\n",
    "            \n",
    "            if valid_gaps:\n",
    "                max_gap = max(valid_gaps, key=lambda x: x[0])\n",
    "                method_c = float(max_gap[1])\n",
    "                log_msg(f\"  Method C (Gap): {method_c:.3f}\")\n",
    "            else:\n",
    "                method_c = method_a\n",
    "                log_msg(f\"  Method C (Gap): No gap found, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method C failed: {e}\")\n",
    "            method_c = method_a\n",
    "        \n",
    "        # METHOD D: Mixture model (simple 2-component)\n",
    "        try:\n",
    "            # Assume bimodal: background-heavy vs tissue-rich\n",
    "            # Find local minimum between modes\n",
    "            hist, bins = np.histogram(ta, bins=30)\n",
    "            smoothed = np.convolve(hist, np.ones(3)/3, mode='same')\n",
    "            \n",
    "            # Find local minima\n",
    "            minima = []\n",
    "            for i in range(1, len(smoothed)-1):\n",
    "                if smoothed[i] < smoothed[i-1] and smoothed[i] < smoothed[i+1]:\n",
    "                    if 0.2 <= bins[i] <= 0.6:\n",
    "                        minima.append((smoothed[i], bins[i]))\n",
    "            \n",
    "            if minima:\n",
    "                # Use deepest minimum\n",
    "                method_d = float(min(minima, key=lambda x: x[0])[1])\n",
    "                log_msg(f\"  Method D (Mixture): {method_d:.3f}\")\n",
    "            else:\n",
    "                method_d = method_a\n",
    "                log_msg(f\"  Method D (Mixture): No minimum, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method D failed: {e}\")\n",
    "            method_d = method_a\n",
    "        \n",
    "        # CONSENSUS: Use median of methods (robust to outliers)\n",
    "        methods = [method_a, method_b, method_c, method_d]\n",
    "        consensus = float(np.median(methods))\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        consensus = max(0.25, min(consensus, 0.65))\n",
    "        \n",
    "        log_msg(f\"\\n  ðŸ“Š Multi-Method Results:\")\n",
    "        log_msg(f\"     A (P25): {method_a:.3f}\")\n",
    "        log_msg(f\"     B (Otsu): {method_b:.3f}\")\n",
    "        log_msg(f\"     C (Gap): {method_c:.3f}\")\n",
    "        log_msg(f\"     D (Mixture): {method_d:.3f}\")\n",
    "        log_msg(f\"  ðŸŽ¯ Consensus (median): {consensus:.3f}\")\n",
    "        \n",
    "        self.results['tissue_threshold'] = {\n",
    "            'optimal': consensus,\n",
    "            'method_a_p25': method_a,\n",
    "            'method_b_otsu': method_b,\n",
    "            'method_c_gap': method_c,\n",
    "            'method_d_mixture': method_d,\n",
    "            'samples': len(ta),\n",
    "            'distribution': {\n",
    "                'mean': float(ta.mean()),\n",
    "                'std': float(ta.std()),\n",
    "                'p10': float(np.percentile(ta, 10)),\n",
    "                'p25': float(np.percentile(ta, 25)),\n",
    "                'p50': float(np.percentile(ta, 50)),\n",
    "                'p75': float(np.percentile(ta, 75))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        log_msg(f\"âœ… Tissue threshold: {consensus:.2f} (robust multi-method)\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz):\n",
    "        \"\"\"DEPRECATED: Use tissue_threshold_robust() instead\"\"\"\n",
    "        log_msg(\"âš ï¸ Using robust multi-method tissue threshold instead of ROC\")\n",
    "        return self.tissue_threshold_robust(sz)\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        \"\"\"Bootstrap confidence interval for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        \n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 50:\n",
    "            log_msg(\"  âš ï¸ Insufficient data for bootstrap\")\n",
    "            return 0.1, 0.0\n",
    "        \n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        \n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"âœ… Bootstrap: {mu:.4f}Â±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        \"\"\"Compute stain normalization targets\"\"\"\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tiles) < 20:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using defaults\")\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        \n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"âœ… Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save optimization results\"\"\"\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'seed': RANDOM_SEED,\n",
    "                    **self.results\n",
    "                }, f, indent=2)\n",
    "            log_msg(f\"âœ… Optimization results saved\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"âš ï¸ Could not save optimization: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# INTERPRETABLE FEATURES\n",
    "# ===============================\n",
    "class InterpExtractor:\n",
    "    \"\"\"Extract interpretable histology features\"\"\"\n",
    "    \n",
    "    def nuclear(self, t):\n",
    "        \"\"\"Nuclear morphology features\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        try:\n",
    "            b = g < threshold_otsu(g)*0.8\n",
    "        except:\n",
    "            b = g < 100\n",
    "        \n",
    "        l = label(b)\n",
    "        r = regionprops(l)\n",
    "        \n",
    "        if not r:\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']}\n",
    "        \n",
    "        a = np.array([x.area for x in r])\n",
    "        c = np.array([4*np.pi*x.area/(x.perimeter**2+1e-8) for x in r])\n",
    "        s = np.array([x.solidity for x in r])\n",
    "        \n",
    "        return {\n",
    "            'nuc_cnt': len(r),\n",
    "            'nuc_area_m': a.mean(),\n",
    "            'nuc_area_s': a.std(),\n",
    "            'nuc_dens': len(r)/b.size,\n",
    "            'nuc_circ': c.mean(),\n",
    "            'nuc_sol': s.mean()\n",
    "        }\n",
    "    \n",
    "    def arch(self, t):\n",
    "        \"\"\"Architectural features (organization uniformity)\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        sm = gaussian(g, 5)\n",
    "        \n",
    "        # Compute local variance\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        \n",
    "        return {\n",
    "            'arch_org': np.mean(vs) if vs else 0,\n",
    "            'arch_uni': np.std(vs) if vs else 0\n",
    "        }\n",
    "    \n",
    "    def texture(self, t):\n",
    "        \"\"\"Texture features via GLCM\"\"\"\n",
    "        g = (rgb2gray(t)*255).astype(np.uint8)\n",
    "        \n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            f = {}\n",
    "            for p in ['contrast','homogeneity','energy']:\n",
    "                f[f'tex_{p}'] = float(graycoprops(glcm, p)[0,0])\n",
    "        except:\n",
    "            f = {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def extract(self, t):\n",
    "        \"\"\"Extract all interpretable features from tile\"\"\"\n",
    "        try:\n",
    "            return {**self.nuclear(t), **self.arch(t), **self.texture(t)}\n",
    "        except Exception as e:\n",
    "            # Return zeros on error\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']} | \\\n",
    "                   {'arch_org':0, 'arch_uni':0} | \\\n",
    "                   {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "\n",
    "# ===============================\n",
    "# ATOM EXTRACTOR (RESNET50)\n",
    "# ===============================\n",
    "class ATOMExtractor:\n",
    "    \"\"\"LUNIT ATOM-style feature extractor using ResNet50\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        log_msg(\"Loading ATOM (ResNet-50)...\")\n",
    "        try:\n",
    "            self.model = timm.create_model(\n",
    "                'resnet50',\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "                global_pool='avg'\n",
    "            ).to(DEVICE).eval()\n",
    "            log_msg(\"âœ… ATOM loaded (2048D)\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"âŒ ATOM loading failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        \"\"\"Extract ATOM features from tiles\"\"\"\n",
    "        if not tiles:\n",
    "            return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting ATOM features from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                # Resize if needed\n",
    "                if t.shape[0]!=sz or t.shape[1]!=sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz,sz)))\n",
    "                \n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fs.append(self.model(x).squeeze().cpu().numpy())\n",
    "                \n",
    "                if (i+1)%50==0:\n",
    "                    print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not fs:\n",
    "            log_msg(f\"  âŒ No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        fs = np.array(fs)\n",
    "        log_msg(f\"  âœ“ Extracted {len(fs)} tile features\")\n",
    "        \n",
    "        # FIXED: Robust outlier removal\n",
    "        if len(fs) > 10:  # Only remove outliers if we have enough tiles\n",
    "            try:\n",
    "                # Compute z-scores\n",
    "                mean_feat = fs.mean(0)\n",
    "                std_feat = fs.std(0)\n",
    "                \n",
    "                # Avoid division by zero for constant features\n",
    "                std_feat = np.where(std_feat < 1e-6, 1.0, std_feat)\n",
    "                \n",
    "                z = np.abs((fs - mean_feat) / std_feat)\n",
    "                \n",
    "                # More lenient threshold (5 instead of 3)\n",
    "                # AND require multiple features to be outliers (not just 1)\n",
    "                outlier_mask = (z > 5).sum(axis=1) > (z.shape[1] * 0.1)  # >10% features are outliers\n",
    "                \n",
    "                num_outliers = outlier_mask.sum()\n",
    "                \n",
    "                if num_outliers > 0 and num_outliers < len(fs) * 0.5:  # Don't remove >50%\n",
    "                    fs = fs[~outlier_mask]\n",
    "                    log_msg(f\"  ðŸ” Removed {num_outliers} outlier tiles\")\n",
    "                elif num_outliers >= len(fs) * 0.5:\n",
    "                    log_msg(f\"  âš ï¸ Too many outliers ({num_outliers}), keeping all tiles\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Outlier detection failed: {e}, keeping all tiles\")\n",
    "        else:\n",
    "            log_msg(f\"  âš ï¸ Too few tiles for outlier removal, keeping all\")\n",
    "        \n",
    "        if len(fs) == 0:\n",
    "            log_msg(f\"  âŒ All tiles removed as outliers\")\n",
    "            return None\n",
    "        \n",
    "        log_msg(f\"  âœ… Final: {len(fs)} tiles\")\n",
    "        \n",
    "        # Aggregate features (with safety checks)\n",
    "        try:\n",
    "            return {\n",
    "                'atom_m': fs.mean(0),\n",
    "                'atom_s': fs.std(0),\n",
    "                'atom_mx': fs.max(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_mn': fs.min(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_md': np.median(fs, 0) if len(fs) > 0 else np.zeros(fs.shape[1])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Aggregation error: {e}\")\n",
    "            return None\n",
    "\n",
    "# ===============================\n",
    "# MAIN PIPELINE\n",
    "# ===============================\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    \n",
    "    if len(files) < 10:\n",
    "        log_msg(\"âŒ Need â‰¥10 slides for calibration\")\n",
    "        return\n",
    "    \n",
    "    log_msg(f\"Found {len(files)} SVS files\")\n",
    "    \n",
    "    # CALIBRATION: Use random 10 slides\n",
    "    np.random.shuffle(files)\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:10]]\n",
    "    \n",
    "    # PROCESSING: Use ALL slides (NO DATA LOSS)\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 1: OPTIMIZATION (CALIBRATION)\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"Calibration slides: {len(cal_paths)}\")\n",
    "    log_msg(f\"Processing slides: {len(proc_files)} (ALL - no data loss)\\n\")\n",
    "    \n",
    "    # Run optimization\n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    # Save parameters\n",
    "    params = {\n",
    "        'tile_sz': sz,\n",
    "        'n_tiles': n_tiles,\n",
    "        'blur_th': blur_th,\n",
    "        'tiss_th': tiss_th,\n",
    "        'stain_m': stain_m.tolist(),\n",
    "        'stain_s': stain_s.tolist(),\n",
    "        'seed': RANDOM_SEED,\n",
    "        'calibration_slides': 10,\n",
    "        'processing_slides': len(proc_files)\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize extractors\n",
    "    interp = InterpExtractor()\n",
    "    \n",
    "    try:\n",
    "        atom = ATOMExtractor()\n",
    "    except:\n",
    "        log_msg(\"âš ï¸ ATOM loading failed, continuing with interpretable features only\")\n",
    "        atom = None\n",
    "    \n",
    "    # Storage\n",
    "    interp_res, atom_res, qc = [], [], []\n",
    "    \n",
    "    # Process all slides\n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            \n",
    "            # Extract tiles\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles:\n",
    "                        break\n",
    "                    \n",
    "                    t = np.array(sl.read_region(\n",
    "                        (int(x*ds), int(y*ds)),\n",
    "                        lv,\n",
    "                        (sz,sz)\n",
    "                    ).convert(\"RGB\"))\n",
    "                    \n",
    "                    # QC checks\n",
    "                    if np.mean(t)>220:  # Background\n",
    "                        continue\n",
    "                    \n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    \n",
    "                    if m.sum()/m.size < tiss_th:  # Tissue percentage\n",
    "                        continue\n",
    "                    \n",
    "                    if opt._blur(t) < blur_th:  # Blur\n",
    "                        continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles:\n",
    "                    break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            # Check minimum tiles\n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  âŒ Insufficient tiles: {len(tiles)}\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'reason': 'insufficient_tiles', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Extract interpretable features\n",
    "            ifs = [interp.extract(t) for t in tiles]\n",
    "            idf = pd.DataFrame(ifs)\n",
    "            \n",
    "            iagg = {'slide': fn}\n",
    "            for c in idf.columns:\n",
    "                iagg[f'{c}_m'] = idf[c].mean()\n",
    "                iagg[f'{c}_s'] = idf[c].std()\n",
    "            \n",
    "            interp_res.append(iagg)\n",
    "            \n",
    "            # Extract ATOM features\n",
    "            if atom:\n",
    "                try:\n",
    "                    af = atom.extract(tiles, sz)\n",
    "                    if af:\n",
    "                        aagg = {'slide': fn}\n",
    "                        for k, v in af.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                aagg[f'{k}_{j}'] = float(x)\n",
    "                        atom_res.append(aagg)\n",
    "                    else:\n",
    "                        log_msg(f\"  âš ï¸ ATOM extraction returned None\")\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  âš ï¸ ATOM extraction failed: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            log_msg(f\"  âœ… Success: {len(tiles)} tiles\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Periodic save\n",
    "            if i%10==0:\n",
    "                pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "                if atom_res:\n",
    "                    pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  ðŸ’¾ Checkpoint saved ({i} slides processed)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'reason': str(e), 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"FINAL SAVE\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    \n",
    "    if interp_res:\n",
    "        pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "        log_msg(f\"âœ… Interpretable features: {len(interp_res)} slides\")\n",
    "    \n",
    "    if atom_res:\n",
    "        pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "        log_msg(f\"âœ… ATOM features: {len(atom_res)} slides\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    log_msg(f\"âœ… QC report saved\")\n",
    "    \n",
    "    # Summary\n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    failed = (qc_df['status']=='fail').sum()\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"PIPELINE COMPLETED\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"âœ… Successful: {success}/{len(qc_df)} ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"âŒ Failed: {failed}/{len(qc_df)}\")\n",
    "    log_msg(f\"\\nOutput files:\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/interpretable.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/atom.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/optimization.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPLETE Q1-READY HISTOLOGY PIPELINE - PRODUCTION VERSION\n",
    "# LUNIT ATOM + INTERPRETABLE FEATURES + ROBUST OPTIMIZATION\n",
    "# ALL SLIDES PROCESSED (NO DATA LOSS)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import timm\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# REPRODUCIBILITY\n",
    "# ===============================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# ===============================\n",
    "# CONFIGURATION\n",
    "# ===============================\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUTPUT_DIR = \"histology_q1_production_final\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(f\"{OUTPUT_DIR}/figures\").mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: LUNIT ATOM + INTERPRETABLE FEATURES (PRODUCTION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {RANDOM_SEED}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    \"\"\"Thread-safe logging\"\"\"\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ===============================\n",
    "# OPTIMIZER - FIXED & ROBUST\n",
    "# ===============================\n",
    "class Optimizer:\n",
    "    \"\"\"Production-grade optimizer with robust error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t):\n",
    "        \"\"\"Check if tile is background\"\"\"\n",
    "        return np.mean(t) > 220\n",
    "    \n",
    "    def _blur(self, t):\n",
    "        \"\"\"Compute blur score with gradient boost for low contrast\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    \n",
    "    def _mask(self, t):\n",
    "        \"\"\"Tissue segmentation mask\"\"\"\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        m = binary_dilation(m, disk(3))\n",
    "        return m\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        \"\"\"Elbow method for optimal tile count\"\"\"\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                \n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(cnts)<3:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using default: 100\")\n",
    "            return 100\n",
    "        \n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        d2 = np.gradient(np.gradient(vars))\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(d2))]), 200))\n",
    "        \n",
    "        self.results['elbow'] = {'optimal': opt, 'samples': len(cnts)}\n",
    "        log_msg(f\"âœ… Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        \"\"\"Youden's J statistic for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        \n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 100:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using default: 0.1\")\n",
    "            return 0.1\n",
    "        \n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        \n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            # Fallback to percentile\n",
    "            opt = float(np.percentile(ba, 5))\n",
    "            log_msg(f\"âœ… Blur threshold (percentile): {opt:.4f}\")\n",
    "            return opt\n",
    "        \n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = []\n",
    "        \n",
    "        for th in ths:\n",
    "            tp = (ba[emp]<th).sum()\n",
    "            fn = (ba[tis]>=th).sum()\n",
    "            fp = (ba[tis]<th).sum()\n",
    "            tn = (ba[emp]>=th).sum()\n",
    "            \n",
    "            sensitivity = tp/(tp+fn+1e-8)\n",
    "            specificity = tn/(tn+fp+1e-8)\n",
    "            js.append(sensitivity + specificity - 1)\n",
    "        \n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt, 'j': float(max(js))}\n",
    "        log_msg(f\"âœ… Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        \"\"\"\n",
    "        ROBUST tissue threshold using multiple methods\n",
    "        Q1-ready: No arbitrary defaults, data-driven fallbacks\n",
    "        \"\"\"\n",
    "        log_msg(\"METHOD 3: Tissue Threshold (Multi-Method)\")\n",
    "        \n",
    "        tisss = []\n",
    "        \n",
    "        # Collect tissue percentages\n",
    "        for p in self.slides[:5]:  # Use more slides\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600:  # More samples\n",
    "                            break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tp = self._mask(t).sum()/t.size\n",
    "                            tisss.append(tp)\n",
    "                    if len(tisss)>=600:\n",
    "                        break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tisss) < 100:\n",
    "            log_msg(\"  âŒ CRITICAL: Insufficient data for tissue threshold\")\n",
    "            return None\n",
    "        \n",
    "        ta = np.array(tisss)\n",
    "        \n",
    "        log_msg(f\"  ðŸ“Š Tissue % distribution:\")\n",
    "        log_msg(f\"     Samples: {len(ta)}\")\n",
    "        log_msg(f\"     Mean: {ta.mean():.3f}, Std: {ta.std():.3f}\")\n",
    "        log_msg(f\"     Min: {ta.min():.3f}, Max: {ta.max():.3f}\")\n",
    "        log_msg(f\"     P10: {np.percentile(ta, 10):.3f}, P50: {np.percentile(ta, 50):.3f}\")\n",
    "        \n",
    "        # METHOD A: Percentile-based (conservative)\n",
    "        # Use 25th percentile - excludes mostly-background tiles\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        log_msg(f\"  Method A (P25): {method_a:.3f}\")\n",
    "        \n",
    "        # METHOD B: Otsu on tissue distribution\n",
    "        try:\n",
    "            # Bin the tissue percentages\n",
    "            hist, bin_edges = np.histogram(ta, bins=50)\n",
    "            # Find threshold that separates background-heavy from tissue-rich\n",
    "            cumsum = np.cumsum(hist)\n",
    "            total = cumsum[-1]\n",
    "            \n",
    "            max_var = 0\n",
    "            best_th = 0.3\n",
    "            \n",
    "            for i in range(1, len(hist)-1):\n",
    "                w0 = cumsum[i] / total\n",
    "                w1 = 1 - w0\n",
    "                \n",
    "                if w0 == 0 or w1 == 0:\n",
    "                    continue\n",
    "                \n",
    "                m0 = np.average(bin_edges[:i+1], weights=hist[:i+1]) if hist[:i+1].sum() > 0 else 0\n",
    "                m1 = np.average(bin_edges[i+1:], weights=hist[i+1:]) if hist[i+1:].sum() > 0 else 0\n",
    "                \n",
    "                var = w0 * w1 * (m0 - m1)**2\n",
    "                \n",
    "                if var > max_var:\n",
    "                    max_var = var\n",
    "                    best_th = bin_edges[i]\n",
    "            \n",
    "            method_b = float(best_th)\n",
    "            log_msg(f\"  Method B (Otsu): {method_b:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method B failed: {e}\")\n",
    "            method_b = method_a\n",
    "        \n",
    "        # METHOD C: Gap statistic\n",
    "        # Find largest gap in sorted tissue percentages\n",
    "        try:\n",
    "            sorted_ta = np.sort(ta)\n",
    "            gaps = np.diff(sorted_ta)\n",
    "            \n",
    "            # Find gap in range [0.2, 0.6]\n",
    "            valid_gaps = []\n",
    "            for i, gap in enumerate(gaps):\n",
    "                if 0.2 <= sorted_ta[i] <= 0.6:\n",
    "                    valid_gaps.append((gap, sorted_ta[i]))\n",
    "            \n",
    "            if valid_gaps:\n",
    "                max_gap = max(valid_gaps, key=lambda x: x[0])\n",
    "                method_c = float(max_gap[1])\n",
    "                log_msg(f\"  Method C (Gap): {method_c:.3f}\")\n",
    "            else:\n",
    "                method_c = method_a\n",
    "                log_msg(f\"  Method C (Gap): No gap found, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method C failed: {e}\")\n",
    "            method_c = method_a\n",
    "        \n",
    "        # METHOD D: Mixture model (simple 2-component)\n",
    "        try:\n",
    "            # Assume bimodal: background-heavy vs tissue-rich\n",
    "            # Find local minimum between modes\n",
    "            hist, bins = np.histogram(ta, bins=30)\n",
    "            smoothed = np.convolve(hist, np.ones(3)/3, mode='same')\n",
    "            \n",
    "            # Find local minima\n",
    "            minima = []\n",
    "            for i in range(1, len(smoothed)-1):\n",
    "                if smoothed[i] < smoothed[i-1] and smoothed[i] < smoothed[i+1]:\n",
    "                    if 0.2 <= bins[i] <= 0.6:\n",
    "                        minima.append((smoothed[i], bins[i]))\n",
    "            \n",
    "            if minima:\n",
    "                # Use deepest minimum\n",
    "                method_d = float(min(minima, key=lambda x: x[0])[1])\n",
    "                log_msg(f\"  Method D (Mixture): {method_d:.3f}\")\n",
    "            else:\n",
    "                method_d = method_a\n",
    "                log_msg(f\"  Method D (Mixture): No minimum, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method D failed: {e}\")\n",
    "            method_d = method_a\n",
    "        \n",
    "        # CONSENSUS: Use median of methods (robust to outliers)\n",
    "        methods = [method_a, method_b, method_c, method_d]\n",
    "        consensus = float(np.median(methods))\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        consensus = max(0.25, min(consensus, 0.65))\n",
    "        \n",
    "        log_msg(f\"\\n  ðŸ“Š Multi-Method Results:\")\n",
    "        log_msg(f\"     A (P25): {method_a:.3f}\")\n",
    "        log_msg(f\"     B (Otsu): {method_b:.3f}\")\n",
    "        log_msg(f\"     C (Gap): {method_c:.3f}\")\n",
    "        log_msg(f\"     D (Mixture): {method_d:.3f}\")\n",
    "        log_msg(f\"  ðŸŽ¯ Consensus (median): {consensus:.3f}\")\n",
    "        \n",
    "        self.results['tissue_threshold'] = {\n",
    "            'optimal': consensus,\n",
    "            'method_a_p25': method_a,\n",
    "            'method_b_otsu': method_b,\n",
    "            'method_c_gap': method_c,\n",
    "            'method_d_mixture': method_d,\n",
    "            'samples': len(ta),\n",
    "            'distribution': {\n",
    "                'mean': float(ta.mean()),\n",
    "                'std': float(ta.std()),\n",
    "                'p10': float(np.percentile(ta, 10)),\n",
    "                'p25': float(np.percentile(ta, 25)),\n",
    "                'p50': float(np.percentile(ta, 50)),\n",
    "                'p75': float(np.percentile(ta, 75))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        log_msg(f\"âœ… Tissue threshold: {consensus:.2f} (robust multi-method)\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz):\n",
    "        \"\"\"DEPRECATED: Use tissue_threshold_robust() instead\"\"\"\n",
    "        log_msg(\"âš ï¸ Using robust multi-method tissue threshold instead of ROC\")\n",
    "        return self.tissue_threshold_robust(sz)\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        \"\"\"Bootstrap confidence interval for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        \n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 50:\n",
    "            log_msg(\"  âš ï¸ Insufficient data for bootstrap\")\n",
    "            return 0.1, 0.0\n",
    "        \n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        \n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"âœ… Bootstrap: {mu:.4f}Â±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        \"\"\"Compute stain normalization targets\"\"\"\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tiles) < 20:\n",
    "            log_msg(\"  âš ï¸ Insufficient data, using defaults\")\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        \n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"âœ… Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save optimization results\"\"\"\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'seed': RANDOM_SEED,\n",
    "                    **self.results\n",
    "                }, f, indent=2)\n",
    "            log_msg(f\"âœ… Optimization results saved\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"âš ï¸ Could not save optimization: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# INTERPRETABLE FEATURES\n",
    "# ===============================\n",
    "class InterpExtractor:\n",
    "    \"\"\"Extract interpretable histology features\"\"\"\n",
    "    \n",
    "    def nuclear(self, t):\n",
    "        \"\"\"Nuclear morphology features\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        try:\n",
    "            b = g < threshold_otsu(g)*0.8\n",
    "        except:\n",
    "            b = g < 100\n",
    "        \n",
    "        l = label(b)\n",
    "        r = regionprops(l)\n",
    "        \n",
    "        if not r:\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']}\n",
    "        \n",
    "        a = np.array([x.area for x in r])\n",
    "        c = np.array([4*np.pi*x.area/(x.perimeter**2+1e-8) for x in r])\n",
    "        s = np.array([x.solidity for x in r])\n",
    "        \n",
    "        return {\n",
    "            'nuc_cnt': len(r),\n",
    "            'nuc_area_m': a.mean(),\n",
    "            'nuc_area_s': a.std(),\n",
    "            'nuc_dens': len(r)/b.size,\n",
    "            'nuc_circ': c.mean(),\n",
    "            'nuc_sol': s.mean()\n",
    "        }\n",
    "    \n",
    "    def arch(self, t):\n",
    "        \"\"\"Architectural features (organization uniformity)\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        sm = gaussian(g, 5)\n",
    "        \n",
    "        # Compute local variance\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        \n",
    "        return {\n",
    "            'arch_org': np.mean(vs) if vs else 0,\n",
    "            'arch_uni': np.std(vs) if vs else 0\n",
    "        }\n",
    "    \n",
    "    def texture(self, t):\n",
    "        \"\"\"Texture features via GLCM\"\"\"\n",
    "        g = (rgb2gray(t)*255).astype(np.uint8)\n",
    "        \n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            f = {}\n",
    "            for p in ['contrast','homogeneity','energy']:\n",
    "                f[f'tex_{p}'] = float(graycoprops(glcm, p)[0,0])\n",
    "        except:\n",
    "            f = {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def extract(self, t):\n",
    "        \"\"\"Extract all interpretable features from tile\"\"\"\n",
    "        try:\n",
    "            return {**self.nuclear(t), **self.arch(t), **self.texture(t)}\n",
    "        except Exception as e:\n",
    "            # Return zeros on error\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']} | \\\n",
    "                   {'arch_org':0, 'arch_uni':0} | \\\n",
    "                   {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "\n",
    "# ===============================\n",
    "# ATOM EXTRACTOR (RESNET50)\n",
    "# ===============================\n",
    "class ATOMExtractor:\n",
    "    \"\"\"LUNIT ATOM-style feature extractor using ResNet50\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        log_msg(\"Loading ATOM (ResNet-50)...\")\n",
    "        try:\n",
    "            self.model = timm.create_model(\n",
    "                'resnet50',\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "                global_pool='avg'\n",
    "            ).to(DEVICE).eval()\n",
    "            log_msg(\"âœ… ATOM loaded (2048D)\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"âŒ ATOM loading failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        \"\"\"Extract ATOM features from tiles\"\"\"\n",
    "        if not tiles:\n",
    "            return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting ATOM features from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                # Resize if needed\n",
    "                if t.shape[0]!=sz or t.shape[1]!=sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz,sz)))\n",
    "                \n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fs.append(self.model(x).squeeze().cpu().numpy())\n",
    "                \n",
    "                if (i+1)%50==0:\n",
    "                    print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not fs:\n",
    "            log_msg(f\"  âŒ No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        fs = np.array(fs)\n",
    "        log_msg(f\"  âœ“ Extracted {len(fs)} tile features\")\n",
    "        \n",
    "        # FIXED: Robust outlier removal\n",
    "        if len(fs) > 10:  # Only remove outliers if we have enough tiles\n",
    "            try:\n",
    "                # Compute z-scores\n",
    "                mean_feat = fs.mean(0)\n",
    "                std_feat = fs.std(0)\n",
    "                \n",
    "                # Avoid division by zero for constant features\n",
    "                std_feat = np.where(std_feat < 1e-6, 1.0, std_feat)\n",
    "                \n",
    "                z = np.abs((fs - mean_feat) / std_feat)\n",
    "                \n",
    "                # More lenient threshold (5 instead of 3)\n",
    "                # AND require multiple features to be outliers (not just 1)\n",
    "                outlier_mask = (z > 5).sum(axis=1) > (z.shape[1] * 0.1)  # >10% features are outliers\n",
    "                \n",
    "                num_outliers = outlier_mask.sum()\n",
    "                \n",
    "                if num_outliers > 0 and num_outliers < len(fs) * 0.5:  # Don't remove >50%\n",
    "                    fs = fs[~outlier_mask]\n",
    "                    log_msg(f\"  ðŸ” Removed {num_outliers} outlier tiles\")\n",
    "                elif num_outliers >= len(fs) * 0.5:\n",
    "                    log_msg(f\"  âš ï¸ Too many outliers ({num_outliers}), keeping all tiles\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Outlier detection failed: {e}, keeping all tiles\")\n",
    "        else:\n",
    "            log_msg(f\"  âš ï¸ Too few tiles for outlier removal, keeping all\")\n",
    "        \n",
    "        if len(fs) == 0:\n",
    "            log_msg(f\"  âŒ All tiles removed as outliers\")\n",
    "            return None\n",
    "        \n",
    "        log_msg(f\"  âœ… Final: {len(fs)} tiles\")\n",
    "        \n",
    "        # Aggregate features (with safety checks)\n",
    "        try:\n",
    "            return {\n",
    "                'atom_m': fs.mean(0),\n",
    "                'atom_s': fs.std(0),\n",
    "                'atom_mx': fs.max(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_mn': fs.min(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_md': np.median(fs, 0) if len(fs) > 0 else np.zeros(fs.shape[1])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Aggregation error: {e}\")\n",
    "            return None\n",
    "\n",
    "# ===============================\n",
    "# MAIN PIPELINE\n",
    "# ===============================\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    \n",
    "    if len(files) < 10:\n",
    "        log_msg(\"âŒ Need â‰¥10 slides for calibration\")\n",
    "        return\n",
    "    \n",
    "    log_msg(f\"Found {len(files)} SVS files\")\n",
    "    \n",
    "    # CALIBRATION: Use random 10 slides\n",
    "    np.random.shuffle(files)\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:10]]\n",
    "    \n",
    "    # PROCESSING: Use ALL slides (NO DATA LOSS)\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 1: OPTIMIZATION (CALIBRATION)\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"Calibration slides: {len(cal_paths)}\")\n",
    "    log_msg(f\"Processing slides: {len(proc_files)} (ALL - no data loss)\\n\")\n",
    "    \n",
    "    # Run optimization\n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    # Save parameters\n",
    "    params = {\n",
    "        'tile_sz': sz,\n",
    "        'n_tiles': n_tiles,\n",
    "        'blur_th': blur_th,\n",
    "        'tiss_th': tiss_th,\n",
    "        'stain_m': stain_m.tolist(),\n",
    "        'stain_s': stain_s.tolist(),\n",
    "        'seed': RANDOM_SEED,\n",
    "        'calibration_slides': 10,\n",
    "        'processing_slides': len(proc_files)\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize extractors\n",
    "    interp = InterpExtractor()\n",
    "    \n",
    "    try:\n",
    "        atom = ATOMExtractor()\n",
    "    except:\n",
    "        log_msg(\"âš ï¸ ATOM loading failed, continuing with interpretable features only\")\n",
    "        atom = None\n",
    "    \n",
    "    # Storage\n",
    "    interp_res, atom_res, qc = [], [], []\n",
    "    \n",
    "    # Process all slides\n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            \n",
    "            # Extract tiles\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles:\n",
    "                        break\n",
    "                    \n",
    "                    t = np.array(sl.read_region(\n",
    "                        (int(x*ds), int(y*ds)),\n",
    "                        lv,\n",
    "                        (sz,sz)\n",
    "                    ).convert(\"RGB\"))\n",
    "                    \n",
    "                    # QC checks\n",
    "                    if np.mean(t)>220:  # Background\n",
    "                        continue\n",
    "                    \n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    \n",
    "                    if m.sum()/m.size < tiss_th:  # Tissue percentage\n",
    "                        continue\n",
    "                    \n",
    "                    if opt._blur(t) < blur_th:  # Blur\n",
    "                        continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles:\n",
    "                    break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            # Check minimum tiles\n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  âŒ Insufficient tiles: {len(tiles)}\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'reason': 'insufficient_tiles', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Extract interpretable features\n",
    "            ifs = [interp.extract(t) for t in tiles]\n",
    "            idf = pd.DataFrame(ifs)\n",
    "            \n",
    "            iagg = {'slide': fn}\n",
    "            for c in idf.columns:\n",
    "                iagg[f'{c}_m'] = idf[c].mean()\n",
    "                iagg[f'{c}_s'] = idf[c].std()\n",
    "            \n",
    "            interp_res.append(iagg)\n",
    "            \n",
    "            # Extract ATOM features\n",
    "            if atom:\n",
    "                try:\n",
    "                    af = atom.extract(tiles, sz)\n",
    "                    if af:\n",
    "                        aagg = {'slide': fn}\n",
    "                        for k, v in af.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                aagg[f'{k}_{j}'] = float(x)\n",
    "                        atom_res.append(aagg)\n",
    "                    else:\n",
    "                        log_msg(f\"  âš ï¸ ATOM extraction returned None\")\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  âš ï¸ ATOM extraction failed: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            log_msg(f\"  âœ… Success: {len(tiles)} tiles\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Periodic save\n",
    "            if i%10==0:\n",
    "                pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "                if atom_res:\n",
    "                    pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  ðŸ’¾ Checkpoint saved ({i} slides processed)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'reason': str(e), 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"FINAL SAVE\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    \n",
    "    if interp_res:\n",
    "        pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "        log_msg(f\"âœ… Interpretable features: {len(interp_res)} slides\")\n",
    "    \n",
    "    if atom_res:\n",
    "        pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "        log_msg(f\"âœ… ATOM features: {len(atom_res)} slides\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    log_msg(f\"âœ… QC report saved\")\n",
    "    \n",
    "    # Summary\n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    failed = (qc_df['status']=='fail').sum()\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"PIPELINE COMPLETED\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"âœ… Successful: {success}/{len(qc_df)} ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"âŒ Failed: {failed}/{len(qc_df)}\")\n",
    "    log_msg(f\"\\nOutput files:\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/interpretable.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/atom.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/optimization.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe6d6b",
   "metadata": {},
   "source": [
    "I'll replace the ResNet-50 loading with CTransPath, loading the weights from the weights folder. The rest of the code remains unchanged. \n",
    "\n",
    "Using \"Replace String in File\"\n",
    "\n",
    "Let me be more specific with additional context: \n",
    "\n",
    "Using \"Replace String in File\"\n",
    "\n",
    "Read [](file:///d%3A/paper/new.ipynb#1360-1360), lines 1360 to 1385\n",
    "\n",
    "Read [](file:///d%3A/paper/new.ipynb#1200-1200), lines 1200 to 1235\n",
    "\n",
    "Read [](file:///d%3A/paper/new.ipynb#1130-1130), lines 1130 to 1160\n",
    "\n",
    "Now I'll replace the ResNet-50 section with CTransPath: \n",
    "\n",
    "Using \"Replace String in File\"\n",
    "\n",
    "Let me search for all occurrences to be more precise: \n",
    "\n",
    "Searched for regex `ATOM EXTRACTOR.*RESNET50` (`**/*.ipynb`), 1 result\n",
    "\n",
    "Let me use edit_notebook_file instead to modify the cell directly. First, let me get the exact cell: \n",
    "\n",
    "Using \"Replace String in File\"\n",
    "\n",
    "There appear to be multiple notebook cells. Let me check how many cells have this pattern and modify the last one (cell 4): \n",
    "\n",
    "Read [](file:///d%3A/paper/new.ipynb#1130-1130), lines 1130 to 1180\n",
    "\n",
    "Now I'll use a more specific match including unique following code: \n",
    "\n",
    "Using \"Replace String in File\"\n",
    "\n",
    "Let me check the full notebook structure to understand where these duplicates are: \n",
    "\n",
    "Searched for regex `class ATOMExtractor` (`**/d:\\\\paper\\\\new.ipynb`), no results\n",
    "\n",
    "Searched for regex `ATOM EXTRACTOR`, 1 result\n",
    "\n",
    "Good, there's only one. The issue is likely that the notebook has the same code in cell output. Let me be even more specific with additional unique context around it: \n",
    "\n",
    "Read [](file:///d%3A/paper/new.ipynb#1115-1115), lines 1115 to 1125"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
