{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924ce1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\paper\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPLETE Q1-READY HISTOLOGY PIPELINE - PRODUCTION VERSION\n",
    "# LUNIT ATOM + INTERPRETABLE FEATURES + ROBUST OPTIMIZATION\n",
    "# ALL SLIDES PROCESSED (NO DATA LOSS)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, binary_erosion, disk\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import timm\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# REPRODUCIBILITY\n",
    "# ===============================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f9b698",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'watershed' from 'skimage.morphology' (d:\\paper\\venv\\lib\\site-packages\\skimage\\morphology\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m threshold_otsu, laplace, gaussian\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphology\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (remove_small_objects, binary_dilation, binary_erosion,\n\u001b[0;32m     18\u001b[0m                                 disk, watershed, label)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rgb2hsv, rgb2gray\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m regionprops\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'watershed' from 'skimage.morphology' (d:\\paper\\venv\\lib\\site-packages\\skimage\\morphology\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q1-READY: CTRANSPATH + COMPREHENSIVE NUCLEUS SEGMENTATION\n",
    "# TRUE WATERSHED SEGMENTATION + 150+ MORPHOLOGICAL FEATURES\n",
    "# ALL FEATURES IN SINGLE CSV OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import (remove_small_objects, binary_dilation, binary_erosion,\n",
    "                                disk, watershed, label)\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from scipy.ndimage import distance_transform_edt, maximum_filter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import timm\n",
    "import traceback\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# CONFIG\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "CTRANSPATH_WEIGHTS = r\"D:\\paper\\weights\\ctranspath.pth\"\n",
    "OUTPUT_DIR = \"CTRANSPATH_NUCLEUS_UNIFIED\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: CTRANSPATH + TRUE NUCLEUS SEGMENTATION - UNIFIED OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Features: CTransPath (768DÃ—5) + Nucleus Morphology (~40Ã—4) + Texture (~20Ã—4)\")\n",
    "print(f\"Output: Single CSV with all features combined\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except: pass\n",
    "\n",
    "# ============= OPTIMIZER =============\n",
    "class Optimizer:\n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t): return np.mean(t) > 220\n",
    "    def _blur(self, t):\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    def _mask(self, t):\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        return binary_dilation(m, disk(3))\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                sl.close()\n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "            except: continue\n",
    "        if len(cnts)<3: return 100\n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(np.gradient(np.gradient(vars))))]), 200))\n",
    "        self.results['elbow'] = {'optimal': opt}\n",
    "        log_msg(f\"âœ… Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 100: return 0.1\n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            return float(np.percentile(ba, 5))\n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = [(ba[emp]<th).sum()/(len(ba[emp])+1e-8) + (ba[tis]>=th).sum()/(len(ba[tis])+1e-8) - 1 for th in ths]\n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt}\n",
    "        log_msg(f\"âœ… Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        log_msg(\"METHOD 3: Tissue Threshold\")\n",
    "        tisss = []\n",
    "        for p in self.slides[:5]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(tisss)>=600: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tisss) < 100: return 0.3\n",
    "        ta = np.array(tisss)\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        consensus = max(0.25, min(method_a, 0.65))\n",
    "        self.results['tissue_threshold'] = {'optimal': consensus}\n",
    "        log_msg(f\"âœ… Tissue threshold: {consensus:.2f}\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz): return self.tissue_threshold_robust(sz)\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t): blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 50: return 0.1, 0.0\n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"âœ… Bootstrap: {mu:.4f}Â±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tiles) < 20:\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"âœ… Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({'timestamp': datetime.now().isoformat(), 'seed': RANDOM_SEED, **self.results}, f, indent=2)\n",
    "        except: pass\n",
    "\n",
    "# ============= NUCLEUS SEGMENTATION =============\n",
    "class NucleusSegmenter:\n",
    "    def __init__(self):\n",
    "        self.hed_matrix = np.array([\n",
    "            [0.65, 0.70, 0.29],\n",
    "            [0.07, 0.99, 0.11],\n",
    "            [0.27, 0.57, 0.78]\n",
    "        ])\n",
    "    \n",
    "    def extract_hematoxylin(self, rgb):\n",
    "        rgb_norm = np.clip(rgb, 1, 255).astype(np.float64) / 255.0\n",
    "        od = -np.log10(rgb_norm + 1e-6)\n",
    "        hematoxylin = od[:, :, 2]\n",
    "        h_norm = ((hematoxylin - hematoxylin.min()) / \n",
    "                  (hematoxylin.max() - hematoxylin.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        return h_norm\n",
    "    \n",
    "    def segment_nuclei(self, rgb):\n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        h_smooth = gaussian(h_channel, sigma=1.0, preserve_range=True).astype(np.uint8)\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            h_smooth, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        binary_clean = remove_small_objects(binary.astype(bool), min_size=20)\n",
    "        kernel = disk(1)\n",
    "        binary_clean = binary_dilation(binary_clean, kernel)\n",
    "        binary_clean = binary_erosion(binary_clean, kernel)\n",
    "        distance = distance_transform_edt(binary_clean)\n",
    "        local_max = maximum_filter(distance, footprint=np.ones((5, 5)))\n",
    "        markers = label(distance == local_max)\n",
    "        labels = watershed(-distance, markers, mask=binary_clean)\n",
    "        return labels\n",
    "    \n",
    "    def extract_features(self, labels, rgb):\n",
    "        props = regionprops(labels)\n",
    "        if len(props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        valid_props = [p for p in props if 80 < p.area < 8000]\n",
    "        if len(valid_props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        areas = np.array([p.area for p in valid_props])\n",
    "        perimeters = np.array([p.perimeter for p in valid_props])\n",
    "        circularities = 4 * np.pi * areas / (perimeters ** 2 + 1e-8)\n",
    "        eccentricities = np.array([p.eccentricity for p in valid_props])\n",
    "        solidities = np.array([p.solidity for p in valid_props])\n",
    "        convexities = np.array([p.area / (p.convex_area + 1e-8) for p in valid_props])\n",
    "        major_axes = np.array([p.major_axis_length for p in valid_props])\n",
    "        minor_axes = np.array([p.minor_axis_length for p in valid_props])\n",
    "        axis_ratios = major_axes / (minor_axes + 1e-8)\n",
    "        centroids = np.array([p.centroid for p in valid_props])\n",
    "        \n",
    "        if len(centroids) > 1:\n",
    "            dist_matrix = squareform(pdist(centroids))\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            nn_distances = np.min(dist_matrix, axis=1)\n",
    "        else:\n",
    "            nn_distances = np.array([0])\n",
    "        \n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        intensity_vars = []\n",
    "        for p in valid_props:\n",
    "            mask = labels == p.label\n",
    "            intensities = h_channel[mask]\n",
    "            intensity_vars.append(np.var(intensities) if len(intensities) > 0 else 0)\n",
    "        intensity_vars = np.array(intensity_vars)\n",
    "        \n",
    "        features = {\n",
    "            'nuc_count': len(valid_props),\n",
    "            'nuc_density': len(valid_props) / labels.size,\n",
    "            'nuc_area_mean': areas.mean(),\n",
    "            'nuc_area_std': areas.std(),\n",
    "            'nuc_area_cv': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_area_p25': np.percentile(areas, 25),\n",
    "            'nuc_area_p50': np.percentile(areas, 50),\n",
    "            'nuc_area_p75': np.percentile(areas, 75),\n",
    "            'nuc_perimeter_mean': perimeters.mean(),\n",
    "            'nuc_perimeter_std': perimeters.std(),\n",
    "            'nuc_circularity_mean': circularities.mean(),\n",
    "            'nuc_circularity_std': circularities.std(),\n",
    "            'nuc_circularity_min': circularities.min(),\n",
    "            'nuc_eccentricity_mean': eccentricities.mean(),\n",
    "            'nuc_eccentricity_std': eccentricities.std(),\n",
    "            'nuc_solidity_mean': solidities.mean(),\n",
    "            'nuc_solidity_std': solidities.std(),\n",
    "            'nuc_convexity_mean': convexities.mean(),\n",
    "            'nuc_convexity_std': convexities.std(),\n",
    "            'nuc_axis_ratio_mean': axis_ratios.mean(),\n",
    "            'nuc_axis_ratio_std': axis_ratios.std(),\n",
    "            'nuc_nn_distance_mean': nn_distances.mean(),\n",
    "            'nuc_nn_distance_std': nn_distances.std(),\n",
    "            'nuc_nn_distance_min': nn_distances.min() if len(nn_distances) > 0 else 0,\n",
    "            'nuc_texture_mean': intensity_vars.mean(),\n",
    "            'nuc_texture_std': intensity_vars.std(),\n",
    "            'nuc_pleomorphism': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_size_range': areas.max() - areas.min(),\n",
    "            'nuc_size_iqr': np.percentile(areas, 75) - np.percentile(areas, 25),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _empty_features(self):\n",
    "        keys = ['nuc_count', 'nuc_density', 'nuc_area_mean', 'nuc_area_std', \n",
    "                'nuc_area_cv', 'nuc_area_p25', 'nuc_area_p50', 'nuc_area_p75',\n",
    "                'nuc_perimeter_mean', 'nuc_perimeter_std', 'nuc_circularity_mean',\n",
    "                'nuc_circularity_std', 'nuc_circularity_min', 'nuc_eccentricity_mean',\n",
    "                'nuc_eccentricity_std', 'nuc_solidity_mean', 'nuc_solidity_std',\n",
    "                'nuc_convexity_mean', 'nuc_convexity_std', 'nuc_axis_ratio_mean',\n",
    "                'nuc_axis_ratio_std', 'nuc_nn_distance_mean', 'nuc_nn_distance_std',\n",
    "                'nuc_nn_distance_min', 'nuc_texture_mean', 'nuc_texture_std',\n",
    "                'nuc_pleomorphism', 'nuc_size_range', 'nuc_size_iqr']\n",
    "        return {k: 0.0 for k in keys}\n",
    "\n",
    "# ============= ADDITIONAL FEATURES =============\n",
    "class AdditionalFeatures:\n",
    "    def architecture(self, rgb):\n",
    "        g = rgb2gray(rgb)\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        return {\n",
    "            'arch_organization': np.mean(vs) if vs else 0,\n",
    "            'arch_uniformity': np.std(vs) if vs else 0,\n",
    "            'arch_entropy': stats.entropy(np.histogram(g, bins=32)[0] + 1e-8) if g.size > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def texture_glcm(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            feats = {}\n",
    "            for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:\n",
    "                try:\n",
    "                    feats[f'tex_{prop.lower()}'] = float(graycoprops(glcm, prop)[0, 0])\n",
    "                except:\n",
    "                    feats[f'tex_{prop.lower()}'] = 0.0\n",
    "        except:\n",
    "            feats = {f'tex_{p}': 0.0 for p in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'asm']}\n",
    "        return feats\n",
    "    \n",
    "    def texture_lbp(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            lbp = local_binary_pattern(g, 8, 1, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "            return {\n",
    "                'lbp_mean': lbp.mean(),\n",
    "                'lbp_std': lbp.std(),\n",
    "                'lbp_entropy': stats.entropy(hist + 1e-8)\n",
    "            }\n",
    "        except:\n",
    "            return {'lbp_mean': 0, 'lbp_std': 0, 'lbp_entropy': 0}\n",
    "    \n",
    "    def color_features(self, rgb):\n",
    "        hsv = rgb2hsv(rgb)\n",
    "        return {\n",
    "            'color_h_mean': hsv[:,:,0].mean(),\n",
    "            'color_s_mean': hsv[:,:,1].mean(),\n",
    "            'color_v_mean': hsv[:,:,2].mean(),\n",
    "            'color_s_std': hsv[:,:,1].std()\n",
    "        }\n",
    "    \n",
    "    def extract_all(self, rgb):\n",
    "        return {\n",
    "            **self.architecture(rgb),\n",
    "            **self.texture_glcm(rgb),\n",
    "            **self.texture_lbp(rgb),\n",
    "            **self.color_features(rgb)\n",
    "        }\n",
    "\n",
    "# ============= CTRANSPATH =============\n",
    "class CTransPathExtractor:\n",
    "    def __init__(self, weights_path=CTRANSPATH_WEIGHTS):\n",
    "        log_msg(\"Loading CTransPath...\")\n",
    "        if not os.path.exists(weights_path):\n",
    "            raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "        \n",
    "        # CTransPath uses Swin-T with specific configuration\n",
    "        # embed_dim=96, depths=[2,2,6,2], num_heads=[3,6,12,24]\n",
    "        try:\n",
    "            # Try with the correct Swin-Tiny architecture\n",
    "            self.model = timm.create_model(\n",
    "                'swin_tiny_patch4_window7_224',\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool=''  # No pooling, get features\n",
    "            )\n",
    "        except:\n",
    "            # Fallback to base model\n",
    "            self.model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model' in checkpoint:\n",
    "            state_dict = checkpoint['model']\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['state_dict']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        # Clean up keys\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            # Remove 'module.' prefix if present\n",
    "            k = k.replace('module.', '')\n",
    "            # Remove 'model.' prefix if present\n",
    "            k = k.replace('model.', '')\n",
    "            new_state_dict[k] = v\n",
    "        \n",
    "        # Load with strict=False to ignore mismatches\n",
    "        missing, unexpected = self.model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        if missing:\n",
    "            log_msg(f\"  Note: {len(missing)} missing keys (usually head layers - OK)\")\n",
    "        if unexpected:\n",
    "            log_msg(f\"  Note: {len(unexpected)} unexpected keys\")\n",
    "        \n",
    "        self.model = self.model.to(DEVICE).eval()\n",
    "        \n",
    "        # Test forward pass to get feature dimension\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "            test_output = self.model(test_input)\n",
    "            if len(test_output.shape) > 2:\n",
    "                # If output has spatial dimensions, pool them\n",
    "                test_output = test_output.mean(dim=[2, 3]) if len(test_output.shape) == 4 else test_output\n",
    "            self.feat_dim = test_output.shape[-1]\n",
    "        \n",
    "        log_msg(f\"âœ… CTransPath loaded ({self.feat_dim}D)\\n\")\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        if not tiles: return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting CTransPath from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                if t.shape[0]!=sz or t.shape[1]!=sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz,sz)))\n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    feat = self.model(x)\n",
    "                    # Handle different output formats\n",
    "                    if len(feat.shape) > 2:\n",
    "                        # If spatial dimensions exist, pool them\n",
    "                        feat = feat.mean(dim=[2, 3]) if len(feat.shape) == 4 else feat.mean(dim=1)\n",
    "                    feat = feat.squeeze().cpu().numpy()\n",
    "                    # Ensure 1D\n",
    "                    if len(feat.shape) == 0:\n",
    "                        feat = np.array([feat])\n",
    "                    fs.append(feat)\n",
    "                if (i+1)%50==0: print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "            except Exception as e:\n",
    "                log_msg(f\"  âš ï¸ Tile {i} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not fs: return None\n",
    "        fs = np.array(fs)\n",
    "        \n",
    "        if len(fs) > 10:\n",
    "            z = np.abs((fs - fs.mean(0)) / (fs.std(0) + 1e-6))\n",
    "            mask = (z > 5).sum(1) > (z.shape[1] * 0.1)\n",
    "            if mask.sum() > 0 and mask.sum() < len(fs) * 0.5:\n",
    "                fs = fs[~mask]\n",
    "        \n",
    "        log_msg(f\"  âœ… {len(fs)} tiles, {fs.shape[1]}D features\")\n",
    "        \n",
    "        return {\n",
    "            'ctrans_mean': fs.mean(0),\n",
    "            'ctrans_std': fs.std(0),\n",
    "            'ctrans_max': fs.max(0),\n",
    "            'ctrans_min': fs.min(0),\n",
    "            'ctrans_median': np.median(fs, 0)\n",
    "        }\n",
    "\n",
    "# ============= MAIN =============\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    if len(files) < 10:\n",
    "        log_msg(\"Need â‰¥10 slides\")\n",
    "        return\n",
    "    \n",
    "    np.random.shuffle(files)\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:10]]\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 1: CALIBRATION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump({'tile_sz': sz, 'n_tiles': n_tiles, 'blur_th': blur_th,\n",
    "                   'tiss_th': tiss_th, 'seed': RANDOM_SEED}, f, indent=2)\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    nuc_seg = NucleusSegmenter()\n",
    "    add_feat = AdditionalFeatures()\n",
    "    \n",
    "    try:\n",
    "        ctrans = CTransPathExtractor(CTRANSPATH_WEIGHTS)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"âš ï¸ CTransPath failed: {e}\")\n",
    "        ctrans = None\n",
    "    \n",
    "    all_features = []\n",
    "    qc = []\n",
    "    \n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles: break\n",
    "                    \n",
    "                    t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                    \n",
    "                    if np.mean(t)>220: continue\n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    if m.sum()/m.size < tiss_th: continue\n",
    "                    if opt._blur(t) < blur_th: continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles: break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  âŒ Insufficient tiles\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Initialize feature dict\n",
    "            slide_features = {'slide': fn}\n",
    "            \n",
    "            # Extract morphological features\n",
    "            log_msg(f\"  Extracting morphology from {len(tiles)} tiles...\")\n",
    "            morph_feats = []\n",
    "            \n",
    "            for t in tiles:\n",
    "                labels = nuc_seg.segment_nuclei(t)\n",
    "                nuc_f = nuc_seg.extract_features(labels, t)\n",
    "                add_f = add_feat.extract_all(t)\n",
    "                morph_feats.append({**nuc_f, **add_f})\n",
    "            \n",
    "            mdf = pd.DataFrame(morph_feats)\n",
    "            for c in mdf.columns:\n",
    "                slide_features[f'{c}_mean'] = mdf[c].mean()\n",
    "                slide_features[f'{c}_std'] = mdf[c].std()\n",
    "                slide_features[f'{c}_p25'] = mdf[c].quantile(0.25)\n",
    "                slide_features[f'{c}_p75'] = mdf[c].quantile(0.75)\n",
    "            \n",
    "            log_msg(f\"  âœ“ Morphology: {len(mdf.columns)} base features\")\n",
    "            \n",
    "            # Extract CTransPath\n",
    "            if ctrans:\n",
    "                try:\n",
    "                    cf = ctrans.extract(tiles, sz)\n",
    "                    if cf:\n",
    "                        for k, v in cf.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                slide_features[f'{k}_{j}'] = float(x)\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  âš ï¸ CTransPath failed: {e}\")\n",
    "            \n",
    "            all_features.append(slide_features)\n",
    "            log_msg(f\"  âœ… Complete - Total features: {len(slide_features)-1}\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Checkpoint save every 10 slides\n",
    "            if i % 10 == 0:\n",
    "                pd.DataFrame(all_features).to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  ðŸ’¾ Checkpoint: {i} slides\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"SAVING FINAL RESULTS\")\n",
    "    log_msg(\"=\"*80)\n",
    "    \n",
    "    if all_features:\n",
    "        final_df = pd.DataFrame(all_features)\n",
    "        final_df.to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "        log_msg(f\"âœ… ALL FEATURES: {len(all_features)} slides Ã— {len(final_df.columns)-1} features\")\n",
    "        log_msg(f\"   - Nucleus morphology: ~{29*4} features\")\n",
    "        log_msg(f\"   - Additional features: ~{17*4} features\")\n",
    "        if ctrans:\n",
    "            log_msg(f\"   - CTransPath: {768*5} features\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    \n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    \n",
    "    log_msg(f\"\\nâœ… COMPLETE: {success}/{len(qc_df)} successful ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"\\nOutput:\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/all_features.csv  â† MAIN OUTPUT (all features)\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/optimization.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf9698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ctranspath_checkpoint(model, weights_path):\n",
    "    \"\"\"\n",
    "    Load CTransPath checkpoint with shape-aware loading\n",
    "    Skips parameters with shape mismatches instead of failing\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from {weights_path}...\")\n",
    "    \n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "    print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    # Extract state dict\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "        print(\"Found 'model' key in checkpoint\")\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        print(\"Found 'state_dict' key in checkpoint\")\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "        print(\"Using checkpoint as state_dict directly\")\n",
    "    \n",
    "    print(f\"State dict has {len(state_dict)} keys\")\n",
    "    \n",
    "    # Clean up checkpoint keys (remove module. and model. prefixes)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace('module.', '').replace('model.', '')\n",
    "        new_state_dict[k] = v\n",
    "    \n",
    "    # Manually load compatible keys, skip mismatches\n",
    "    print(\"\\nLoading weights with shape compatibility check...\")\n",
    "    loaded_keys = 0\n",
    "    skipped_keys = 0\n",
    "    shape_mismatches = []\n",
    "    \n",
    "    model_state = model.state_dict()\n",
    "    \n",
    "    for name, param in new_state_dict.items():\n",
    "        if name in model_state:\n",
    "            if model_state[name].shape == param.shape:\n",
    "                # Shapes match, load it\n",
    "                model_state[name].copy_(param)\n",
    "                loaded_keys += 1\n",
    "            else:\n",
    "                # Shape mismatch, skip it\n",
    "                skipped_keys += 1\n",
    "                shape_mismatches.append(f\"{name}: checkpoint {param.shape} vs model {model_state[name].shape}\")\n",
    "        else:\n",
    "            # Key not in model, skip it\n",
    "            skipped_keys += 1\n",
    "    \n",
    "    print(f\"âœ“ Loaded {loaded_keys} parameters, skipped {skipped_keys} (shape mismatch or missing)\")\n",
    "    \n",
    "    if shape_mismatches:\n",
    "        print(f\"\\nSkipped keys due to shape mismatch ({len(shape_mismatches)} total):\")\n",
    "        for msg in shape_mismatches[:3]:\n",
    "            print(f\"  - {msg}\")\n",
    "        if len(shape_mismatches) > 3:\n",
    "            print(f\"  ... and {len(shape_mismatches)-3} more\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af0972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CTransPath initialization...\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING CTRANSPATH\n",
      "================================================================================\n",
      "Creating CTransPath model...\n",
      "âœ“ Model created successfully\n",
      "Loading checkpoint from D:\\paper\\weights\\ctranspath.pth...\n",
      "Checkpoint keys: ['model']\n",
      "Found 'model' key in checkpoint\n",
      "State dict has 200 keys\n",
      "\n",
      "Loading weights with shape compatibility check...\n",
      "âœ“ Loaded 160 parameters, skipped 40 (shape mismatch or missing)\n",
      "\n",
      "Skipped keys due to shape mismatch (6 total):\n",
      "  - layers.1.downsample.reduction.weight: checkpoint torch.Size([384, 768]) vs model torch.Size([192, 384])\n",
      "  - layers.1.downsample.norm.weight: checkpoint torch.Size([768]) vs model torch.Size([384])\n",
      "  - layers.1.downsample.norm.bias: checkpoint torch.Size([768]) vs model torch.Size([384])\n",
      "  ... and 3 more\n",
      "\n",
      "Determining output feature dimension...\n",
      "âœ… CTransPath initialized: 768D features\n",
      "================================================================================\n",
      "\n",
      "âœ… CTransPath loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test loading\n",
    "print(\"Testing CTransPath initialization...\")\n",
    "try:\n",
    "    ctrans_test = CTransPathExtractor(CTRANSPATH_WEIGHTS)\n",
    "    print(\"âœ… CTransPath loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ CTransPath loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c38fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CTRANSPATH + NUCLEUS SEGMENTATION - FIXED VERSION\n",
      "================================================================================\n",
      "Device: cpu\n",
      "Output: CTRANSPATH_NUCLEUS_UNIFIED_FIXED\n",
      "\n",
      "Found 111 SVS files\n",
      "\n",
      "Initializing CTransPath (FIXED)...\n",
      "âŒ CTransPath init failed: name 'CTransPathExtractorFixed' is not defined\n",
      "\n",
      "[1/111] YG_P8W7SBCME4VH_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 352\u001b[0m\n\u001b[0;32m    349\u001b[0m     log_msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/all_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 352\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 289\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    287\u001b[0m     labels \u001b[38;5;241m=\u001b[39m nuc_seg\u001b[38;5;241m.\u001b[39msegment_nuclei(t)\n\u001b[0;32m    288\u001b[0m     nuc_f \u001b[38;5;241m=\u001b[39m nuc_seg\u001b[38;5;241m.\u001b[39mextract_features(labels, t)\n\u001b[1;32m--> 289\u001b[0m     add_f \u001b[38;5;241m=\u001b[39m \u001b[43madd_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     morph_feats\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnuc_f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madd_f})\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[5], line 214\u001b[0m, in \u001b[0;36mAdditionalFeatures.extract_all\u001b[1;34m(self, rgb)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_all\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb):\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture(rgb), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture_glcm(rgb), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture_lbp(rgb), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m}\n",
      "Cell \u001b[1;32mIn[5], line 205\u001b[0m, in \u001b[0;36mAdditionalFeatures.color_features\u001b[1;34m(self, rgb)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcolor_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb):\n\u001b[1;32m--> 205\u001b[0m     hsv \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2hsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_h_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: hsv[:,:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_s_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: hsv[:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_v_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: hsv[:,:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_s_std\u001b[39m\u001b[38;5;124m'\u001b[39m: hsv[:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n\u001b[0;32m    211\u001b[0m     }\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\_shared\\utils.py:445\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\color\\colorconv.py:334\u001b[0m, in \u001b[0;36mrgb2hsv\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    331\u001b[0m out_h \u001b[38;5;241m=\u001b[39m (out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m6.0\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    332\u001b[0m out_h[delta \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 334\u001b[0m np\u001b[38;5;241m.\u001b[39mseterr(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mold_settings)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# -- output\u001b[39;00m\n\u001b[0;32m    337\u001b[0m out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m out_h\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\numpy\\_core\\_ufunc_config.py:20\u001b[0m, in \u001b[0;36mseterr\u001b[1;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _make_extobj, _get_extobj_dict, _extobj_contextvar\n\u001b[0;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseterr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeterr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetbufsize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetbufsize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseterrcall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeterrcall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrstate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m ]\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mseterr\u001b[39m(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Set how floating-point errors are handled.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     old \u001b[38;5;241m=\u001b[39m _get_extobj_dict()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RUN PIPELINE WITH FIXED CTRANSPATH EXTRACTOR\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import (remove_small_objects, binary_dilation, binary_erosion, disk)\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from scipy.ndimage import distance_transform_edt, maximum_filter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# CONFIG\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUTPUT_DIR = \"CTRANSPATH_NUCLEUS_UNIFIED_FIXED\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CTRANSPATH + NUCLEUS SEGMENTATION - FIXED VERSION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except: pass\n",
    "\n",
    "# ============= NUCLEUS SEGMENTATION =============\n",
    "class NucleusSegmenter:\n",
    "    def __init__(self):\n",
    "        self.hed_matrix = np.array([\n",
    "            [0.65, 0.70, 0.29],\n",
    "            [0.07, 0.99, 0.11],\n",
    "            [0.27, 0.57, 0.78]\n",
    "        ])\n",
    "    \n",
    "    def extract_hematoxylin(self, rgb):\n",
    "        rgb_norm = np.clip(rgb, 1, 255).astype(np.float64) / 255.0\n",
    "        od = -np.log10(rgb_norm + 1e-6)\n",
    "        hematoxylin = od[:, :, 2]\n",
    "        h_norm = ((hematoxylin - hematoxylin.min()) / \n",
    "                  (hematoxylin.max() - hematoxylin.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        return h_norm\n",
    "    \n",
    "    def segment_nuclei(self, rgb):\n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        h_smooth = gaussian(h_channel, sigma=1.0, preserve_range=True).astype(np.uint8)\n",
    "        binary = cv2.adaptiveThreshold(h_smooth, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "        binary_clean = remove_small_objects(binary.astype(bool), min_size=20)\n",
    "        kernel = disk(1)\n",
    "        binary_clean = binary_dilation(binary_clean, kernel)\n",
    "        binary_clean = binary_erosion(binary_clean, kernel)\n",
    "        distance = distance_transform_edt(binary_clean)\n",
    "        local_max = maximum_filter(distance, footprint=np.ones((5, 5)))\n",
    "        markers = label(distance == local_max)\n",
    "        labels = watershed(-distance, markers, mask=binary_clean)\n",
    "        return labels\n",
    "    \n",
    "    def extract_features(self, labels, rgb):\n",
    "        props = regionprops(labels)\n",
    "        if len(props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        valid_props = [p for p in props if 80 < p.area < 8000]\n",
    "        if len(valid_props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        areas = np.array([p.area for p in valid_props])\n",
    "        perimeters = np.array([p.perimeter for p in valid_props])\n",
    "        circularities = 4 * np.pi * areas / (perimeters ** 2 + 1e-8)\n",
    "        eccentricities = np.array([p.eccentricity for p in valid_props])\n",
    "        solidities = np.array([p.solidity for p in valid_props])\n",
    "        convexities = np.array([p.area / (p.convex_area + 1e-8) for p in valid_props])\n",
    "        major_axes = np.array([p.major_axis_length for p in valid_props])\n",
    "        minor_axes = np.array([p.minor_axis_length for p in valid_props])\n",
    "        axis_ratios = major_axes / (minor_axes + 1e-8)\n",
    "        centroids = np.array([p.centroid for p in valid_props])\n",
    "        \n",
    "        if len(centroids) > 1:\n",
    "            dist_matrix = squareform(pdist(centroids))\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            nn_distances = np.min(dist_matrix, axis=1)\n",
    "        else:\n",
    "            nn_distances = np.array([0])\n",
    "        \n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        intensity_vars = []\n",
    "        for p in valid_props:\n",
    "            mask = labels == p.label\n",
    "            intensities = h_channel[mask]\n",
    "            intensity_vars.append(np.var(intensities) if len(intensities) > 0 else 0)\n",
    "        intensity_vars = np.array(intensity_vars)\n",
    "        \n",
    "        features = {\n",
    "            'nuc_count': len(valid_props),\n",
    "            'nuc_density': len(valid_props) / labels.size,\n",
    "            'nuc_area_mean': areas.mean(),\n",
    "            'nuc_area_std': areas.std(),\n",
    "            'nuc_area_cv': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_area_p25': np.percentile(areas, 25),\n",
    "            'nuc_area_p50': np.percentile(areas, 50),\n",
    "            'nuc_area_p75': np.percentile(areas, 75),\n",
    "            'nuc_perimeter_mean': perimeters.mean(),\n",
    "            'nuc_perimeter_std': perimeters.std(),\n",
    "            'nuc_circularity_mean': circularities.mean(),\n",
    "            'nuc_circularity_std': circularities.std(),\n",
    "            'nuc_circularity_min': circularities.min(),\n",
    "            'nuc_eccentricity_mean': eccentricities.mean(),\n",
    "            'nuc_eccentricity_std': eccentricities.std(),\n",
    "            'nuc_solidity_mean': solidities.mean(),\n",
    "            'nuc_solidity_std': solidities.std(),\n",
    "            'nuc_convexity_mean': convexities.mean(),\n",
    "            'nuc_convexity_std': convexities.std(),\n",
    "            'nuc_axis_ratio_mean': axis_ratios.mean(),\n",
    "            'nuc_axis_ratio_std': axis_ratios.std(),\n",
    "            'nuc_nn_distance_mean': nn_distances.mean(),\n",
    "            'nuc_nn_distance_std': nn_distances.std(),\n",
    "            'nuc_nn_distance_min': nn_distances.min() if len(nn_distances) > 0 else 0,\n",
    "            'nuc_texture_mean': intensity_vars.mean(),\n",
    "            'nuc_texture_std': intensity_vars.std(),\n",
    "            'nuc_pleomorphism': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_size_range': areas.max() - areas.min(),\n",
    "            'nuc_size_iqr': np.percentile(areas, 75) - np.percentile(areas, 25),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _empty_features(self):\n",
    "        keys = ['nuc_count', 'nuc_density', 'nuc_area_mean', 'nuc_area_std', \n",
    "                'nuc_area_cv', 'nuc_area_p25', 'nuc_area_p50', 'nuc_area_p75',\n",
    "                'nuc_perimeter_mean', 'nuc_perimeter_std', 'nuc_circularity_mean',\n",
    "                'nuc_circularity_std', 'nuc_circularity_min', 'nuc_eccentricity_mean',\n",
    "                'nuc_eccentricity_std', 'nuc_solidity_mean', 'nuc_solidity_std',\n",
    "                'nuc_convexity_mean', 'nuc_convexity_std', 'nuc_axis_ratio_mean',\n",
    "                'nuc_axis_ratio_std', 'nuc_nn_distance_mean', 'nuc_nn_distance_std',\n",
    "                'nuc_nn_distance_min', 'nuc_texture_mean', 'nuc_texture_std',\n",
    "                'nuc_pleomorphism', 'nuc_size_range', 'nuc_size_iqr']\n",
    "        return {k: 0.0 for k in keys}\n",
    "\n",
    "# ============= ADDITIONAL FEATURES =============\n",
    "class AdditionalFeatures:\n",
    "    def architecture(self, rgb):\n",
    "        g = rgb2gray(rgb)\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) for i in range(0,g.shape[0]-20,20) for j in range(0,g.shape[1]-20,20)]\n",
    "        return {\n",
    "            'arch_organization': np.mean(vs) if vs else 0,\n",
    "            'arch_uniformity': np.std(vs) if vs else 0,\n",
    "            'arch_entropy': stats.entropy(np.histogram(g, bins=32)[0] + 1e-8) if g.size > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def texture_glcm(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            feats = {}\n",
    "            for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:\n",
    "                try:\n",
    "                    feats[f'tex_{prop.lower()}'] = float(graycoprops(glcm, prop)[0, 0])\n",
    "                except:\n",
    "                    feats[f'tex_{prop.lower()}'] = 0.0\n",
    "        except:\n",
    "            feats = {f'tex_{p}': 0.0 for p in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'asm']}\n",
    "        return feats\n",
    "    \n",
    "    def texture_lbp(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            lbp = local_binary_pattern(g, 8, 1, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "            return {'lbp_mean': lbp.mean(), 'lbp_std': lbp.std(), 'lbp_entropy': stats.entropy(hist + 1e-8)}\n",
    "        except:\n",
    "            return {'lbp_mean': 0, 'lbp_std': 0, 'lbp_entropy': 0}\n",
    "    \n",
    "    def color_features(self, rgb):\n",
    "        hsv = rgb2hsv(rgb)\n",
    "        return {\n",
    "            'color_h_mean': hsv[:,:,0].mean(),\n",
    "            'color_s_mean': hsv[:,:,1].mean(),\n",
    "            'color_v_mean': hsv[:,:,2].mean(),\n",
    "            'color_s_std': hsv[:,:,1].std()\n",
    "        }\n",
    "    \n",
    "    def extract_all(self, rgb):\n",
    "        return {**self.architecture(rgb), **self.texture_glcm(rgb), **self.texture_lbp(rgb), **self.color_features(rgb)}\n",
    "\n",
    "# ============= MAIN PIPELINE WITH FIXED CTRANSPATH =============\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    \n",
    "    if len(files) < 10:\n",
    "        log_msg(\"âŒ Need â‰¥10 slides\")\n",
    "        return\n",
    "    \n",
    "    np.random.shuffle(files)\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(f\"Found {len(proc_files)} SVS files\\n\")\n",
    "    \n",
    "    # Initialize extractors\n",
    "    nuc_seg = NucleusSegmenter()\n",
    "    add_feat = AdditionalFeatures()\n",
    "    \n",
    "    log_msg(\"Initializing CTransPath (FIXED)...\")\n",
    "    try:\n",
    "        ctrans = CTransPathExtractorFixed(CTRANSPATH_WEIGHTS)\n",
    "        log_msg(\"âœ… CTransPath ready\\n\")\n",
    "    except Exception as e:\n",
    "        log_msg(f\"âŒ CTransPath init failed: {e}\")\n",
    "        ctrans = None\n",
    "    \n",
    "    all_features = []\n",
    "    qc = []\n",
    "    sz = 224\n",
    "    n_tiles = 150\n",
    "    \n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles) >= n_tiles: break\n",
    "                    \n",
    "                    t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                    \n",
    "                    if np.mean(t) > 220: continue\n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    if m.sum()/m.size < 0.1: continue  # Lenient tissue threshold\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles) >= n_tiles: break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            if len(tiles) < 30:  # Very lenient minimum\n",
    "                log_msg(f\"  âŒ Insufficient tiles: {len(tiles)}\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Initialize feature dict\n",
    "            slide_features = {'slide': fn}\n",
    "            \n",
    "            # Extract morphological features\n",
    "            log_msg(f\"  Extracting morphology from {len(tiles)} tiles...\")\n",
    "            morph_feats = []\n",
    "            \n",
    "            for t in tiles:\n",
    "                try:\n",
    "                    labels = nuc_seg.segment_nuclei(t)\n",
    "                    nuc_f = nuc_seg.extract_features(labels, t)\n",
    "                    add_f = add_feat.extract_all(t)\n",
    "                    morph_feats.append({**nuc_f, **add_f})\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"    âš ï¸ Tile morphology failed: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if morph_feats:\n",
    "                mdf = pd.DataFrame(morph_feats)\n",
    "                for c in mdf.columns:\n",
    "                    slide_features[f'{c}_mean'] = mdf[c].mean()\n",
    "                    slide_features[f'{c}_std'] = mdf[c].std()\n",
    "                    slide_features[f'{c}_p25'] = mdf[c].quantile(0.25)\n",
    "                    slide_features[f'{c}_p75'] = mdf[c].quantile(0.75)\n",
    "                \n",
    "                log_msg(f\"  âœ“ Morphology: {len(mdf.columns)} base features\")\n",
    "            \n",
    "            # Extract CTransPath\n",
    "            if ctrans:\n",
    "                try:\n",
    "                    cf = ctrans.extract(tiles, sz)\n",
    "                    if cf:\n",
    "                        for k, v in cf.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                slide_features[f'{k}_{j}'] = float(x)\n",
    "                        log_msg(f\"  âœ“ CTransPath: {cf['ctrans_mean'].shape[0]}D features\")\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  âš ï¸ CTransPath failed: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            all_features.append(slide_features)\n",
    "            log_msg(f\"  âœ… Complete - Total features: {len(slide_features)-1}\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Checkpoint save\n",
    "            if i % 5 == 0:\n",
    "                pd.DataFrame(all_features).to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  ðŸ’¾ Checkpoint: {i} slides\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  âŒ Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"FINAL RESULTS\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    \n",
    "    if all_features:\n",
    "        final_df = pd.DataFrame(all_features)\n",
    "        final_df.to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "        log_msg(f\"âœ… ALL FEATURES: {len(all_features)} slides Ã— {len(final_df.columns)-1} features\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    \n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    \n",
    "    log_msg(f\"\\nâœ… COMPLETE: {success}/{len(qc_df)} successful ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"\\nOutput: {OUTPUT_DIR}/all_features.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10200ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
