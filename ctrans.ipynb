{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1-READY: LUNIT ATOM + INTERPRETABLE FEATURES (PRODUCTION)\n",
      "================================================================================\n",
      "Device: cpu\n",
      "Seed: 42\n",
      "Output: histology_q1_production_final\n",
      "\n",
      "Found 111 SVS files\n",
      "\n",
      "================================================================================\n",
      "STEP 1: OPTIMIZATION (CALIBRATION)\n",
      "================================================================================\n",
      "Calibration slides: 27 (24.3%)\n",
      "Processing slides: 111 (ALL - no data loss)\n",
      "\n",
      "METHOD 1: Elbow (Tile Count)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPLETE Q1-READY HISTOLOGY PIPELINE - PRODUCTION VERSION\n",
    "# LUNIT ATOM + INTERPRETABLE FEATURES + ROBUST OPTIMIZATION\n",
    "# ALL SLIDES PROCESSED (NO DATA LOSS)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import timm\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# REPRODUCIBILITY\n",
    "# ===============================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# ===============================\n",
    "# CONFIGURATION\n",
    "# ===============================\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "OUTPUT_DIR = \"histology_q1_production_final\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(f\"{OUTPUT_DIR}/figures\").mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: LUNIT ATOM + INTERPRETABLE FEATURES (PRODUCTION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {RANDOM_SEED}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    \"\"\"Thread-safe logging\"\"\"\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ===============================\n",
    "# OPTIMIZER - FIXED & ROBUST\n",
    "# ===============================\n",
    "class Optimizer:\n",
    "    \"\"\"Production-grade optimizer with robust error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t):\n",
    "        \"\"\"Check if tile is background\"\"\"\n",
    "        return np.mean(t) > 220\n",
    "    \n",
    "    def _blur(self, t):\n",
    "        \"\"\"Compute blur score with gradient boost for low contrast\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    \n",
    "    def _mask(self, t):\n",
    "        \"\"\"Tissue segmentation mask\"\"\"\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        m = binary_dilation(m, disk(3))\n",
    "        return m\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        \"\"\"Elbow method for optimal tile count\"\"\"\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                \n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(cnts)<3:\n",
    "            log_msg(\"  ‚ö†Ô∏è Insufficient data, using default: 100\")\n",
    "            return 100\n",
    "        \n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        d2 = np.gradient(np.gradient(vars))\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(d2))]), 200))\n",
    "        \n",
    "        self.results['elbow'] = {'optimal': opt, 'samples': len(cnts)}\n",
    "        log_msg(f\"‚úÖ Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        \"\"\"Youden's J statistic for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        \n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 100:\n",
    "            log_msg(\"  ‚ö†Ô∏è Insufficient data, using default: 0.1\")\n",
    "            return 0.1\n",
    "        \n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        \n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            # Fallback to percentile\n",
    "            opt = float(np.percentile(ba, 5))\n",
    "            log_msg(f\"‚úÖ Blur threshold (percentile): {opt:.4f}\")\n",
    "            return opt\n",
    "        \n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = []\n",
    "        \n",
    "        for th in ths:\n",
    "            tp = (ba[emp]<th).sum()\n",
    "            fn = (ba[tis]>=th).sum()\n",
    "            fp = (ba[tis]<th).sum()\n",
    "            tn = (ba[emp]>=th).sum()\n",
    "            \n",
    "            sensitivity = tp/(tp+fn+1e-8)\n",
    "            specificity = tn/(tn+fp+1e-8)\n",
    "            js.append(sensitivity + specificity - 1)\n",
    "        \n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt, 'j': float(max(js))}\n",
    "        log_msg(f\"‚úÖ Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        \"\"\"\n",
    "        ROBUST tissue threshold using multiple methods\n",
    "        Q1-ready: No arbitrary defaults, data-driven fallbacks\n",
    "        \"\"\"\n",
    "        log_msg(\"METHOD 3: Tissue Threshold (Multi-Method)\")\n",
    "        \n",
    "        tisss = []\n",
    "        \n",
    "        # Collect tissue percentages\n",
    "        for p in self.slides[:5]:  # Use more slides\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600:  # More samples\n",
    "                            break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tp = self._mask(t).sum()/t.size\n",
    "                            tisss.append(tp)\n",
    "                    if len(tisss)>=600:\n",
    "                        break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tisss) < 100:\n",
    "            log_msg(\"  ‚ùå CRITICAL: Insufficient data for tissue threshold\")\n",
    "            return None\n",
    "        \n",
    "        ta = np.array(tisss)\n",
    "        \n",
    "        log_msg(f\"  üìä Tissue % distribution:\")\n",
    "        log_msg(f\"     Samples: {len(ta)}\")\n",
    "        log_msg(f\"     Mean: {ta.mean():.3f}, Std: {ta.std():.3f}\")\n",
    "        log_msg(f\"     Min: {ta.min():.3f}, Max: {ta.max():.3f}\")\n",
    "        log_msg(f\"     P10: {np.percentile(ta, 10):.3f}, P50: {np.percentile(ta, 50):.3f}\")\n",
    "        \n",
    "        # METHOD A: Percentile-based (conservative)\n",
    "        # Use 25th percentile - excludes mostly-background tiles\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        log_msg(f\"  Method A (P25): {method_a:.3f}\")\n",
    "        \n",
    "        # METHOD B: Otsu on tissue distribution\n",
    "        try:\n",
    "            # Bin the tissue percentages\n",
    "            hist, bin_edges = np.histogram(ta, bins=50)\n",
    "            # Find threshold that separates background-heavy from tissue-rich\n",
    "            cumsum = np.cumsum(hist)\n",
    "            total = cumsum[-1]\n",
    "            \n",
    "            max_var = 0\n",
    "            best_th = 0.3\n",
    "            \n",
    "            for i in range(1, len(hist)-1):\n",
    "                w0 = cumsum[i] / total\n",
    "                w1 = 1 - w0\n",
    "                \n",
    "                if w0 == 0 or w1 == 0:\n",
    "                    continue\n",
    "                \n",
    "                m0 = np.average(bin_edges[:i+1], weights=hist[:i+1]) if hist[:i+1].sum() > 0 else 0\n",
    "                m1 = np.average(bin_edges[i+1:], weights=hist[i+1:]) if hist[i+1:].sum() > 0 else 0\n",
    "                \n",
    "                var = w0 * w1 * (m0 - m1)**2\n",
    "                \n",
    "                if var > max_var:\n",
    "                    max_var = var\n",
    "                    best_th = bin_edges[i]\n",
    "            \n",
    "            method_b = float(best_th)\n",
    "            log_msg(f\"  Method B (Otsu): {method_b:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method B failed: {e}\")\n",
    "            method_b = method_a\n",
    "        \n",
    "        # METHOD C: Gap statistic\n",
    "        # Find largest gap in sorted tissue percentages\n",
    "        try:\n",
    "            sorted_ta = np.sort(ta)\n",
    "            gaps = np.diff(sorted_ta)\n",
    "            \n",
    "            # Find gap in range [0.2, 0.6]\n",
    "            valid_gaps = []\n",
    "            for i, gap in enumerate(gaps):\n",
    "                if 0.2 <= sorted_ta[i] <= 0.6:\n",
    "                    valid_gaps.append((gap, sorted_ta[i]))\n",
    "            \n",
    "            if valid_gaps:\n",
    "                max_gap = max(valid_gaps, key=lambda x: x[0])\n",
    "                method_c = float(max_gap[1])\n",
    "                log_msg(f\"  Method C (Gap): {method_c:.3f}\")\n",
    "            else:\n",
    "                method_c = method_a\n",
    "                log_msg(f\"  Method C (Gap): No gap found, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method C failed: {e}\")\n",
    "            method_c = method_a\n",
    "        \n",
    "        # METHOD D: Mixture model (simple 2-component)\n",
    "        try:\n",
    "            # Assume bimodal: background-heavy vs tissue-rich\n",
    "            # Find local minimum between modes\n",
    "            hist, bins = np.histogram(ta, bins=30)\n",
    "            smoothed = np.convolve(hist, np.ones(3)/3, mode='same')\n",
    "            \n",
    "            # Find local minima\n",
    "            minima = []\n",
    "            for i in range(1, len(smoothed)-1):\n",
    "                if smoothed[i] < smoothed[i-1] and smoothed[i] < smoothed[i+1]:\n",
    "                    if 0.2 <= bins[i] <= 0.6:\n",
    "                        minima.append((smoothed[i], bins[i]))\n",
    "            \n",
    "            if minima:\n",
    "                # Use deepest minimum\n",
    "                method_d = float(min(minima, key=lambda x: x[0])[1])\n",
    "                log_msg(f\"  Method D (Mixture): {method_d:.3f}\")\n",
    "            else:\n",
    "                method_d = method_a\n",
    "                log_msg(f\"  Method D (Mixture): No minimum, using P25\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Method D failed: {e}\")\n",
    "            method_d = method_a\n",
    "        \n",
    "        # CONSENSUS: Use median of methods (robust to outliers)\n",
    "        methods = [method_a, method_b, method_c, method_d]\n",
    "        consensus = float(np.median(methods))\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        consensus = max(0.25, min(consensus, 0.65))\n",
    "        \n",
    "        log_msg(f\"\\n  üìä Multi-Method Results:\")\n",
    "        log_msg(f\"     A (P25): {method_a:.3f}\")\n",
    "        log_msg(f\"     B (Otsu): {method_b:.3f}\")\n",
    "        log_msg(f\"     C (Gap): {method_c:.3f}\")\n",
    "        log_msg(f\"     D (Mixture): {method_d:.3f}\")\n",
    "        log_msg(f\"  üéØ Consensus (median): {consensus:.3f}\")\n",
    "        \n",
    "        self.results['tissue_threshold'] = {\n",
    "            'optimal': consensus,\n",
    "            'method_a_p25': method_a,\n",
    "            'method_b_otsu': method_b,\n",
    "            'method_c_gap': method_c,\n",
    "            'method_d_mixture': method_d,\n",
    "            'samples': len(ta),\n",
    "            'distribution': {\n",
    "                'mean': float(ta.mean()),\n",
    "                'std': float(ta.std()),\n",
    "                'p10': float(np.percentile(ta, 10)),\n",
    "                'p25': float(np.percentile(ta, 25)),\n",
    "                'p50': float(np.percentile(ta, 50)),\n",
    "                'p75': float(np.percentile(ta, 75))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        log_msg(f\"‚úÖ Tissue threshold: {consensus:.2f} (robust multi-method)\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz):\n",
    "        \"\"\"ROC-based tissue threshold optimization using Youden's J statistic\"\"\"\n",
    "        log_msg(\"METHOD 3: ROC (Tissue Threshold)\")\n",
    "        \n",
    "        tisss = []\n",
    "        \n",
    "        # First pass: Collect tissue percentages to determine adaptive thresholds\n",
    "        for p in self.slides[:5]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=800:\n",
    "                            break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tp = self._mask(t).sum()/t.size\n",
    "                            tisss.append(tp)\n",
    "                    if len(tisss)>=800:\n",
    "                        break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tisss) < 100:\n",
    "            log_msg(f\"  ‚ö†Ô∏è Insufficient data for ROC, using percentile-based fallback\")\n",
    "            consensus = float(np.percentile(np.array(tisss), 25))\n",
    "            self.results['roc'] = {'method': 'fallback_percentile', 'optimal': consensus}\n",
    "            log_msg(f\"‚úÖ Tissue threshold: {consensus:.3f} (percentile fallback)\")\n",
    "            return consensus\n",
    "        \n",
    "        ta_all = np.array(tisss)\n",
    "        \n",
    "        # ADAPTIVE LABELING: Use percentiles to ensure both classes exist\n",
    "        # Use 75th percentile as tissue-rich threshold (upper quartile)\n",
    "        # Use 25th percentile as background-heavy threshold (lower quartile)\n",
    "        p25 = float(np.percentile(ta_all, 25))\n",
    "        p75 = float(np.percentile(ta_all, 75))\n",
    "        \n",
    "        log_msg(f\"  üìä Tissue % distribution (initial):\")\n",
    "        log_msg(f\"     Samples: {len(ta_all)}\")\n",
    "        log_msg(f\"     P25: {p25:.4f}, P50: {np.percentile(ta_all, 50):.4f}, P75: {p75:.4f}\")\n",
    "        log_msg(f\"     Min: {ta_all.min():.4f}, Max: {ta_all.max():.4f}\")\n",
    "        \n",
    "        # Create binary labels based on adaptive percentiles\n",
    "        labels = []\n",
    "        for tp in ta_all:\n",
    "            if tp >= p75:\n",
    "                labels.append(1)  # Tissue-rich\n",
    "            elif tp < p25:\n",
    "                labels.append(0)  # Background-heavy\n",
    "            else:\n",
    "                labels.append(-1)  # Ambiguous (middle range)\n",
    "        \n",
    "        # Filter out ambiguous samples\n",
    "        la = np.array(labels)\n",
    "        mask = la != -1\n",
    "        ta = ta_all[mask]\n",
    "        la = la[mask]\n",
    "        \n",
    "        # Verify we have both classes\n",
    "        n_tissue = (la == 1).sum()\n",
    "        n_background = (la == 0).sum()\n",
    "        \n",
    "        if len(ta) < 50 or n_tissue < 10 or n_background < 10:\n",
    "            log_msg(f\"  ‚ö†Ô∏è Insufficient balanced data ({len(ta)} total, {n_tissue} tissue, {n_background} background)\")\n",
    "            log_msg(f\"  Using simple median-based threshold instead\")\n",
    "            consensus = float(np.median(ta_all))\n",
    "            self.results['roc'] = {'method': 'fallback_median', 'optimal': consensus}\n",
    "            log_msg(f\"‚úÖ Tissue threshold: {consensus:.3f} (median fallback)\")\n",
    "            return consensus\n",
    "        \n",
    "        # Compute ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(la, ta)\n",
    "        \n",
    "        # Compute Youden's J statistic for each threshold\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = float(thresholds[optimal_idx])\n",
    "        optimal_j = float(j_scores[optimal_idx])\n",
    "        roc_auc = float(auc(fpr, tpr))\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        optimal_threshold = max(0.10, min(optimal_threshold, 0.75))\n",
    "        \n",
    "        log_msg(f\"  üìä ROC Analysis (adaptive labeling):\")\n",
    "        log_msg(f\"     Labeled samples: {len(ta)} (tissue: {n_tissue}, background: {n_background})\")\n",
    "        log_msg(f\"     P25 threshold: {p25:.4f}, P75 threshold: {p75:.4f}\")\n",
    "        log_msg(f\"     AUC: {roc_auc:.4f}\")\n",
    "        log_msg(f\"     Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        log_msg(f\"     Youden's J: {optimal_j:.4f}\")\n",
    "        log_msg(f\"     Sensitivity: {tpr[optimal_idx]:.4f}\")\n",
    "        log_msg(f\"     Specificity: {1-fpr[optimal_idx]:.4f}\")\n",
    "        \n",
    "        self.results['roc'] = {\n",
    "            'optimal': optimal_threshold,\n",
    "            'j_statistic': optimal_j,\n",
    "            'auc': roc_auc,\n",
    "            'sensitivity': float(tpr[optimal_idx]),\n",
    "            'specificity': float(1-fpr[optimal_idx]),\n",
    "            'p25': p25,\n",
    "            'p75': p75,\n",
    "            'samples': len(ta),\n",
    "            'samples_total': len(ta_all)\n",
    "        }\n",
    "        \n",
    "        log_msg(f\"‚úÖ Tissue threshold: {optimal_threshold:.3f} (ROC-optimized with adaptive labeling)\")\n",
    "        return optimal_threshold\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        \"\"\"Bootstrap confidence interval for blur threshold\"\"\"\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        \n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(blurs) < 50:\n",
    "            log_msg(\"  ‚ö†Ô∏è Insufficient data for bootstrap\")\n",
    "            return 0.1, 0.0\n",
    "        \n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        \n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"‚úÖ Bootstrap: {mu:.4f}¬±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        \"\"\"Compute stain normalization targets\"\"\"\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        \n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                \n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                \n",
    "                sl.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Slide error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tiles) < 20:\n",
    "            log_msg(\"  ‚ö†Ô∏è Insufficient data, using defaults\")\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        \n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"‚úÖ Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save optimization results\"\"\"\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'seed': RANDOM_SEED,\n",
    "                    **self.results\n",
    "                }, f, indent=2)\n",
    "            log_msg(f\"‚úÖ Optimization results saved\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"‚ö†Ô∏è Could not save optimization: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# INTERPRETABLE FEATURES\n",
    "# ===============================\n",
    "class InterpExtractor:\n",
    "    \"\"\"Extract interpretable histology features\"\"\"\n",
    "    \n",
    "    def nuclear(self, t):\n",
    "        \"\"\"Nuclear morphology features\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        try:\n",
    "            b = g < threshold_otsu(g)*0.8\n",
    "        except:\n",
    "            b = g < 100\n",
    "        \n",
    "        l = label(b)\n",
    "        r = regionprops(l)\n",
    "        \n",
    "        if not r:\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']}\n",
    "        \n",
    "        a = np.array([x.area for x in r])\n",
    "        c = np.array([4*np.pi*x.area/(x.perimeter**2+1e-8) for x in r])\n",
    "        s = np.array([x.solidity for x in r])\n",
    "        \n",
    "        return {\n",
    "            'nuc_cnt': len(r),\n",
    "            'nuc_area_m': a.mean(),\n",
    "            'nuc_area_s': a.std(),\n",
    "            'nuc_dens': len(r)/b.size,\n",
    "            'nuc_circ': c.mean(),\n",
    "            'nuc_sol': s.mean()\n",
    "        }\n",
    "    \n",
    "    def arch(self, t):\n",
    "        \"\"\"Architectural features (organization uniformity)\"\"\"\n",
    "        g = rgb2gray(t)\n",
    "        sm = gaussian(g, 5)\n",
    "        \n",
    "        # Compute local variance\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        \n",
    "        return {\n",
    "            'arch_org': np.mean(vs) if vs else 0,\n",
    "            'arch_uni': np.std(vs) if vs else 0\n",
    "        }\n",
    "    \n",
    "    def texture(self, t):\n",
    "        \"\"\"Texture features via GLCM\"\"\"\n",
    "        g = (rgb2gray(t)*255).astype(np.uint8)\n",
    "        \n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            f = {}\n",
    "            for p in ['contrast','homogeneity','energy']:\n",
    "                f[f'tex_{p}'] = float(graycoprops(glcm, p)[0,0])\n",
    "        except:\n",
    "            f = {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def extract(self, t):\n",
    "        \"\"\"Extract all interpretable features from tile\"\"\"\n",
    "        try:\n",
    "            return {**self.nuclear(t), **self.arch(t), **self.texture(t)}\n",
    "        except Exception as e:\n",
    "            # Return zeros on error\n",
    "            return {f'nuc_{k}':0 for k in ['cnt','area_m','area_s','dens','circ','sol']} | \\\n",
    "                   {'arch_org':0, 'arch_uni':0} | \\\n",
    "                   {f'tex_{p}':0 for p in ['contrast','homogeneity','energy']}\n",
    "\n",
    "# ===============================\n",
    "# ATOM EXTRACTOR (CTRANSPATH)\n",
    "# ===============================\n",
    "class ATOMExtractor:\n",
    "    \"\"\"Feature extractor using Ctranspath model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        log_msg(\"Loading Ctranspath...\")\n",
    "        try:\n",
    "            # Load Ctranspath model from weights folder\n",
    "            model_path = \"weights/ctranspath.pth\"\n",
    "            \n",
    "            # Create ResNet50 backbone for Ctranspath\n",
    "            self.model = timm.create_model(\n",
    "                'resnet50',\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool='avg'\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            # Load pretrained Ctranspath weights\n",
    "            checkpoint = torch.load(model_path, map_location=DEVICE)\n",
    "            \n",
    "            # Handle different checkpoint formats\n",
    "            if isinstance(checkpoint, dict) and 'model' in checkpoint:\n",
    "                state_dict = checkpoint['model']\n",
    "            elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            # Remove 'module.' prefix if present (from DataParallel)\n",
    "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "            \n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "            self.model.eval()\n",
    "            \n",
    "            log_msg(\"‚úÖ Ctranspath loaded (2048D)\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"‚ùå Ctranspath loading failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        \"\"\"Extract ATOM features from tiles\"\"\"\n",
    "        if not tiles:\n",
    "            return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting ATOM features from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                # Resize if needed\n",
    "                if t.shape[0]!=sz or t.shape[1]!=sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz,sz)))\n",
    "                \n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fs.append(self.model(x).squeeze().cpu().numpy())\n",
    "                \n",
    "                if (i+1)%50==0:\n",
    "                    print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not fs:\n",
    "            log_msg(f\"  ‚ùå No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        fs = np.array(fs)\n",
    "        log_msg(f\"  ‚úì Extracted {len(fs)} tile features\")\n",
    "        \n",
    "        # FIXED: Robust outlier removal\n",
    "        if len(fs) > 10:  # Only remove outliers if we have enough tiles\n",
    "            try:\n",
    "                # Compute z-scores\n",
    "                mean_feat = fs.mean(0)\n",
    "                std_feat = fs.std(0)\n",
    "                \n",
    "                # Avoid division by zero for constant features\n",
    "                std_feat = np.where(std_feat < 1e-6, 1.0, std_feat)\n",
    "                \n",
    "                z = np.abs((fs - mean_feat) / std_feat)\n",
    "                \n",
    "                # More lenient threshold (5 instead of 3)\n",
    "                # AND require multiple features to be outliers (not just 1)\n",
    "                outlier_mask = (z > 5).sum(axis=1) > (z.shape[1] * 0.1)  # >10% features are outliers\n",
    "                \n",
    "                num_outliers = outlier_mask.sum()\n",
    "                \n",
    "                if num_outliers > 0 and num_outliers < len(fs) * 0.5:  # Don't remove >50%\n",
    "                    fs = fs[~outlier_mask]\n",
    "                    log_msg(f\"  üîç Removed {num_outliers} outlier tiles\")\n",
    "                elif num_outliers >= len(fs) * 0.5:\n",
    "                    log_msg(f\"  ‚ö†Ô∏è Too many outliers ({num_outliers}), keeping all tiles\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Outlier detection failed: {e}, keeping all tiles\")\n",
    "        else:\n",
    "            log_msg(f\"  ‚ö†Ô∏è Too few tiles for outlier removal, keeping all\")\n",
    "        \n",
    "        if len(fs) == 0:\n",
    "            log_msg(f\"  ‚ùå All tiles removed as outliers\")\n",
    "            return None\n",
    "        \n",
    "        log_msg(f\"  ‚úÖ Final: {len(fs)} tiles\")\n",
    "        \n",
    "        # Aggregate features (with safety checks)\n",
    "        try:\n",
    "            return {\n",
    "                'atom_m': fs.mean(0),\n",
    "                'atom_s': fs.std(0),\n",
    "                'atom_mx': fs.max(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_mn': fs.min(0) if len(fs) > 0 else np.zeros(fs.shape[1]),\n",
    "                'atom_md': np.median(fs, 0) if len(fs) > 0 else np.zeros(fs.shape[1])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            log_msg(f\"  ‚ùå Aggregation error: {e}\")\n",
    "            return None\n",
    "\n",
    "# ===============================\n",
    "# MAIN PIPELINE\n",
    "# ===============================\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    \n",
    "    if len(files) < 10:\n",
    "        log_msg(\"‚ùå Need ‚â•10 slides for calibration\")\n",
    "        return\n",
    "    \n",
    "    log_msg(f\"Found {len(files)} SVS files\")\n",
    "    \n",
    "    # CALIBRATION: Use 25% of slides (for robust optimization)\n",
    "    # For 111 slides: 111 // 4 = 27 slides\n",
    "    np.random.shuffle(files)\n",
    "    n_calib = max(20, min(30, len(files) // 4))  # 20-30 slides depending on total\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:n_calib]]\n",
    "    \n",
    "    # PROCESSING: Use ALL slides (NO DATA LOSS)\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 1: OPTIMIZATION (CALIBRATION)\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"Calibration slides: {len(cal_paths)} ({len(cal_paths)/len(files)*100:.1f}%)\")\n",
    "    log_msg(f\"Processing slides: {len(proc_files)} (ALL - no data loss)\\n\")\n",
    "    \n",
    "    # Run optimization\n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    # Save parameters\n",
    "    params = {\n",
    "        'tile_sz': sz,\n",
    "        'n_tiles': n_tiles,\n",
    "        'blur_th': blur_th,\n",
    "        'tiss_th': tiss_th,\n",
    "        'stain_m': stain_m.tolist(),\n",
    "        'stain_s': stain_s.tolist(),\n",
    "        'seed': RANDOM_SEED,\n",
    "        'calibration_slides': len(cal_paths),\n",
    "        'calibration_percentage': len(cal_paths) / len(proc_files) * 100,\n",
    "        'processing_slides': len(proc_files)\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize extractors\n",
    "    interp = InterpExtractor()\n",
    "    \n",
    "    try:\n",
    "        atom = ATOMExtractor()\n",
    "    except:\n",
    "        log_msg(\"‚ö†Ô∏è ATOM loading failed, continuing with interpretable features only\")\n",
    "        atom = None\n",
    "    \n",
    "    # Storage\n",
    "    interp_res, atom_res, qc = [], [], []\n",
    "    \n",
    "    # Process all slides\n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            \n",
    "            # Extract tiles\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles:\n",
    "                        break\n",
    "                    \n",
    "                    t = np.array(sl.read_region(\n",
    "                        (int(x*ds), int(y*ds)),\n",
    "                        lv,\n",
    "                        (sz,sz)\n",
    "                    ).convert(\"RGB\"))\n",
    "                    \n",
    "                    # QC checks\n",
    "                    if np.mean(t)>220:  # Background\n",
    "                        continue\n",
    "                    \n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    \n",
    "                    if m.sum()/m.size < tiss_th:  # Tissue percentage\n",
    "                        continue\n",
    "                    \n",
    "                    if opt._blur(t) < blur_th:  # Blur\n",
    "                        continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles:\n",
    "                    break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            # Check minimum tiles\n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  ‚ùå Insufficient tiles: {len(tiles)}\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'reason': 'insufficient_tiles', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Extract interpretable features\n",
    "            ifs = [interp.extract(t) for t in tiles]\n",
    "            idf = pd.DataFrame(ifs)\n",
    "            \n",
    "            iagg = {'slide': fn}\n",
    "            for c in idf.columns:\n",
    "                iagg[f'{c}_m'] = idf[c].mean()\n",
    "                iagg[f'{c}_s'] = idf[c].std()\n",
    "            \n",
    "            interp_res.append(iagg)\n",
    "            \n",
    "            # Extract ATOM features\n",
    "            if atom:\n",
    "                try:\n",
    "                    af = atom.extract(tiles, sz)\n",
    "                    if af:\n",
    "                        aagg = {'slide': fn}\n",
    "                        for k, v in af.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                aagg[f'{k}_{j}'] = float(x)\n",
    "                        atom_res.append(aagg)\n",
    "                    else:\n",
    "                        log_msg(f\"  ‚ö†Ô∏è ATOM extraction returned None\")\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  ‚ö†Ô∏è ATOM extraction failed: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            log_msg(f\"  ‚úÖ Success: {len(tiles)} tiles\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Periodic save\n",
    "            if i%10==0:\n",
    "                pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "                if atom_res:\n",
    "                    pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  üíæ Checkpoint saved ({i} slides processed)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  ‚ùå Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'reason': str(e), 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"FINAL SAVE\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    \n",
    "    if interp_res:\n",
    "        pd.DataFrame(interp_res).to_csv(f\"{OUTPUT_DIR}/interpretable.csv\", index=False)\n",
    "        log_msg(f\"‚úÖ Interpretable features: {len(interp_res)} slides\")\n",
    "    \n",
    "    if atom_res:\n",
    "        pd.DataFrame(atom_res).to_csv(f\"{OUTPUT_DIR}/atom.csv\", index=False)\n",
    "        log_msg(f\"‚úÖ ATOM features: {len(atom_res)} slides\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    log_msg(f\"‚úÖ QC report saved\")\n",
    "    \n",
    "    # Summary\n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    failed = (qc_df['status']=='fail').sum()\n",
    "    \n",
    "    log_msg(f\"\\n{'='*80}\")\n",
    "    log_msg(\"PIPELINE COMPLETED\")\n",
    "    log_msg(f\"{'='*80}\")\n",
    "    log_msg(f\"‚úÖ Successful: {success}/{len(qc_df)} ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"‚ùå Failed: {failed}/{len(qc_df)}\")\n",
    "    log_msg(f\"\\nOutput files:\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/interpretable.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/atom.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/optimization.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4e582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
