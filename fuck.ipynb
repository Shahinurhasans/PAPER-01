{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96dd5bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c8630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== Loading Clinical Data ===\n",
      "Clinical data shape: (111, 36)\n",
      "Columns: ['Accession', 'Patient ID', 'Age at BM Dx', 'Sex', 'Pack-Year Smoking Hx', 'NIH Racial Category', 'KPS at BM Dx', 'Age at Resection', 'Size of Dominant Lesion', 'Number of BM Lesions at Dx', 'Location of Lesion', 'Histologic Subtype', 'GPA Histology Class', 'Presence of Extracranial Mets at Dx (1 = Yes; 0 = No)', 'EGFR Status (1 = Mutation; 0 = WT)', 'ALK Status (1 = Mutation; 0 = WT)', 'PD-L1 Status (1 = Mutation; 0 = WT)', 'Sperduto GPA', 'patientID', 'Original Scan Facility', 'Scanner vendor', 'Scanner model', 'field_strength', '2D_3D_acquisition', 'FLAIR-Available', 'T1CE-Available', 'flair_slice_thickness', 'flair_spacing', 'flair_repetition_time', 'flair_echo_time', 'flair_spin_echo', 't1ce_slice_thickness', 't1ce_spacing', 't1ce_repetition_time', 't1ce_echo_time', 't1ce_spin_echo']\n",
      "Using target column: 'EGFR Status (1 = Mutation; 0 = WT)'\n",
      "Target distribution:\n",
      "EGFR Status (1 = Mutation; 0 = WT)\n",
      "0    99\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "Target encoding: 1=Mutation, 0=WT\n",
      "Imbalance ratio: 8.25:1\n",
      "\n",
      "Trying 'patientID' column for matching...\n",
      "Sample patientID values: ['YG_0427RC24FT6W', 'YG_0AXGKD8AFJGS', 'YG_0IBUXTBINCD9', 'YG_0LCI9NV44UK0', 'YG_1BZ1NP805Q9J']\n",
      "\n",
      "================================================================================\n",
      "EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Training MRI Model\n",
      "============================================================\n",
      "Valid MRI samples: 80/88\n",
      "Valid MRI samples: 20/23\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:00<00:00, 12.16s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5704 | Train Acc: 0.6750\n",
      "Val Loss: 0.3387 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.5789\n",
      "Model saved! Best F1: 0.4737\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:51<00:00, 10.32s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3217 | Train Acc: 0.8875\n",
      "Val Loss: 0.4122 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:50<00:00, 10.01s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2310 | Train Acc: 0.8875\n",
      "Val Loss: 0.4031 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MRI Model - Final Results:\n",
      "Accuracy: 0.9000\n",
      "F1-Score: 0.4737\n",
      "AUC: 0.5789\n",
      "\n",
      "============================================================\n",
      "Training HISTOLOGY Model\n",
      "============================================================\n",
      "Valid Histology samples: 91/91\n",
      "Valid Histology samples: 26/26\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:47<00:00,  7.91s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5387 | Train Acc: 0.7143\n",
      "Val Loss: 0.8796 | Val Acc: 0.3846 | Val F1: 0.3500 | Val AUC: 0.8125\n",
      "Model saved! Best F1: 0.3500\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:48<00:00,  8.16s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2819 | Train Acc: 0.9231\n",
      "Val Loss: 0.6614 | Val Acc: 0.6538 | Val F1: 0.5385 | Val AUC: 0.7500\n",
      "Model saved! Best F1: 0.5385\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:48<00:00,  8.05s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1609 | Train Acc: 0.9121\n",
      "Val Loss: 0.3540 | Val Acc: 0.8462 | Val F1: 0.6232 | Val AUC: 0.7500\n",
      "Model saved! Best F1: 0.6232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HISTOLOGY Model - Final Results:\n",
      "Accuracy: 0.8462\n",
      "F1-Score: 0.6232\n",
      "AUC: 0.7500\n",
      "\n",
      "============================================================\n",
      "Training COMBINED Model\n",
      "============================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:45<00:00, 21.15s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6834 | Train Acc: 0.5250\n",
      "Val Loss: 0.3377 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 1.0000\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:32<00:00, 18.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5017 | Train Acc: 0.7500\n",
      "Val Loss: 0.1980 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:37<00:00, 19.44s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3545 | Train Acc: 0.8750\n",
      "Val Loss: 0.1147 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMBINED Model - Final Results:\n",
      "Accuracy: 0.9500\n",
      "F1-Score: 0.4872\n",
      "AUC: 1.0000\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON\n",
      "================================================================================\n",
      "    Model Accuracy Precision Recall F1-Score    AUC\n",
      "      MRI   0.9000    0.4737 0.4737   0.4737 0.5789\n",
      "HISTOLOGY   0.8462    0.6023 0.6875   0.6232 0.7500\n",
      " COMBINED   0.9500    0.4750 0.5000   0.4872 1.0000\n",
      "\n",
      "=== Statistical Analysis ===\n",
      "Combined vs MRI improvement: 2.85%\n",
      "Combined vs Histology improvement: -21.82%\n",
      "\n",
      "================================================================================\n",
      "All results saved to: ./egfr_results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EGFR Mutation Prediction: MRI vs Histology vs Combined Approach\n",
    "# Dataset: Brain Metastases with class imbalance 8.65:1\n",
    "# \n",
    "# Install required packages:\n",
    "# pip install nibabel torch torchvision opencv-python pillow pandas openpyxl scikit-learn matplotlib seaborn tqdm\n",
    "# pip install openslide-python (optional, for better SVS support)\n",
    "\n",
    "import os;os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np,pandas as pd,torch,torch.nn as nn,torch.nn.functional as F\n",
    "import torchvision.models as models,torchvision.transforms as transforms\n",
    "from PIL import Image;Image.MAX_IMAGE_PIXELS=None\n",
    "from pathlib import Path;from datetime import datetime;import cv2\n",
    "from scipy.spatial.distance import cosine,euclidean,pdist,squareform;from scipy import stats\n",
    "import matplotlib.pyplot as plt,seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import random,glob,json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:import nibabel as nib\n",
    "except:print(\"Warning: nibabel not installed. Install with: pip install nibabel\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);np.random.seed(seed);torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed);torch.backends.cudnn.deterministic=True\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    clinical_path=r\"D:\\paper\\external.xlsx\"\n",
    "    histology_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "    mri_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_radiology_images\\Brain-Mets-Lung-MRI-Path-Segs\"\n",
    "    output_dir=\"./egfr_results\"\n",
    "    \n",
    "    # Training params\n",
    "    img_size=224;batch_size=16;epochs=3;lr=1e-4;patience=10\n",
    "    n_folds=5;device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    imbalance_ratio=8.65\n",
    "    \n",
    "cfg=Config()\n",
    "os.makedirs(cfg.output_dir,exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "print(\"\\n=== Loading Clinical Data ===\")\n",
    "clinical_df=pd.read_excel(cfg.clinical_path) if cfg.clinical_path.endswith('.xlsx') else pd.read_csv(cfg.clinical_path)\n",
    "print(f\"Clinical data shape: {clinical_df.shape}\")\n",
    "print(f\"Columns: {clinical_df.columns.tolist()}\")\n",
    "\n",
    "# Find target column - handle different possible names\n",
    "target_col=None\n",
    "for col in clinical_df.columns:\n",
    "    if 'EGFR' in col.upper() and 'STATUS' in col.upper():\n",
    "        target_col=col\n",
    "        break\n",
    "if target_col is None:\n",
    "    print(\"ERROR: EGFR Status column not found!\")\n",
    "    print(f\"Available columns: {clinical_df.columns.tolist()}\")\n",
    "    raise ValueError(\"Target column 'EGFR Status' not found in dataset\")\n",
    "\n",
    "print(f\"Using target column: '{target_col}'\")\n",
    "print(f\"Target distribution:\\n{clinical_df[target_col].value_counts()}\")\n",
    "print(f\"Target encoding: 1=Mutation, 0=WT\")\n",
    "print(f\"Imbalance ratio: {clinical_df[target_col].value_counts()[0]/clinical_df[target_col].value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Get patient IDs and labels\n",
    "# Check if there's a 'patientID' column that might match better\n",
    "if 'patientID' in clinical_df.columns:\n",
    "    print(\"\\nTrying 'patientID' column for matching...\")\n",
    "    patient_ids=clinical_df['patientID'].astype(str).tolist()\n",
    "    print(f\"Sample patientID values: {patient_ids[:5]}\")\n",
    "else:\n",
    "    patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "    print(f\"Using first column for patient IDs: {patient_ids[:5]}\")\n",
    "\n",
    "labels=clinical_df[target_col].values\n",
    "\n",
    "# ==================== MRI DATASET ====================\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        all_dirs={d.name:d for d in self.base_path.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            patient_dir=None\n",
    "            \n",
    "            # Try direct match\n",
    "            if pid in all_dirs:\n",
    "                patient_dir=all_dirs[pid]\n",
    "            # Try without YG_ prefix if it exists\n",
    "            elif pid.startswith('YG_'):\n",
    "                pid_without_prefix=pid[3:]\n",
    "                if pid_without_prefix in all_dirs:\n",
    "                    patient_dir=all_dirs[pid_without_prefix]\n",
    "            # Try adding YG_ prefix if not present\n",
    "            elif f\"YG_{pid}\" in all_dirs:\n",
    "                patient_dir=all_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            if patient_dir and patient_dir.exists():\n",
    "                # Look for t1ce and flair files\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                \n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    valid.append((idx,patient_dir,t1ce_files[0],flair_files[0]))\n",
    "        \n",
    "        print(f\"Valid MRI samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        if len(valid)==0:\n",
    "            print(f\"Sample patient IDs: {self.patient_ids[:5]}\")\n",
    "            print(f\"Sample directories: {list(all_dirs.keys())[:5]}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,patient_dir,t1ce_path,flair_path=self.valid_data[idx]\n",
    "        pid=self.patient_ids[real_idx];label=self.labels[real_idx]\n",
    "        \n",
    "        # Load images\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        \n",
    "        # Stack as 3-channel (T1CE, FLAIR, Average)\n",
    "        avg=(t1ce_img+flair_img)/2\n",
    "        img=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        if self.transform:img=self.transform(Image.fromarray(img.astype(np.uint8)))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            # Handle compressed .nii files\n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                # It's a compressed archive, try to extract\n",
    "                import zipfile,gzip,tarfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:\n",
    "                                img_data=nib.load(f).get_fdata()\n",
    "                else:\n",
    "                    # Try loading directly\n",
    "                    img_data=nib.load(str(path)).get_fdata()\n",
    "            else:\n",
    "                img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            # Get middle slice\n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            # Normalize to 0-255\n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading NIfTI {path}: {e}\")\n",
    "            # Fallback: try as image\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:\n",
    "                # Return blank image as last resort\n",
    "                return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "\n",
    "# ==================== HISTOLOGY DATASET ====================\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            svs_files=list(self.base_path.glob(f\"*{pid}*.svs\"))\n",
    "            if len(svs_files)==0:svs_files=list(self.base_path.glob(f\"YG_{pid}*.svs\"))\n",
    "            if len(svs_files)>0:valid.append((idx,svs_files[0]))\n",
    "        print(f\"Valid Histology samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,svs_path=self.valid_data[idx];label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        img=self._load_svs(svs_path)\n",
    "        if self.transform:img=self.transform(Image.fromarray(img))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== COMBINED DATASET ====================\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,mri_base,hist_base,transform=None,hist_ids=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.mri_base=Path(mri_base)\n",
    "        self.hist_base=Path(hist_base);self.transform=transform\n",
    "        self.hist_ids=hist_ids if hist_ids else patient_ids  # Use separate hist IDs if provided\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[];all_mri_dirs={d.name:d for d in self.mri_base.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            hist_pid=self.hist_ids[idx]\n",
    "            \n",
    "            # Check MRI using patientID\n",
    "            patient_dir=None\n",
    "            if pid in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid]\n",
    "            elif pid.startswith('YG_') and pid[3:] in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid[3:]]\n",
    "            elif f\"YG_{pid}\" in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            has_mri=False;t1ce_file=None;flair_file=None\n",
    "            if patient_dir and patient_dir.exists():\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    has_mri=True;t1ce_file=t1ce_files[0];flair_file=flair_files[0]\n",
    "            \n",
    "            # Check Histology using Accession ID\n",
    "            svs_files=list(self.hist_base.glob(f\"*{hist_pid}*.svs\"))\n",
    "            if len(svs_files)==0 and hist_pid.startswith('YG_'):\n",
    "                svs_files=list(self.hist_base.glob(f\"*{hist_pid[3:]}*.svs\"))\n",
    "            has_hist=len(svs_files)>0\n",
    "            \n",
    "            if has_mri and has_hist:valid.append((idx,patient_dir,t1ce_file,flair_file,svs_files[0]))\n",
    "        \n",
    "        print(f\"Valid Combined samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,mri_dir,t1ce_path,flair_path,svs_path=self.valid_data[idx]\n",
    "        label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        \n",
    "        # Load MRI\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        avg=(t1ce_img+flair_img)/2;mri=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        # Load Histology\n",
    "        hist=self._load_svs(svs_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            mri=self.transform(Image.fromarray(mri.astype(np.uint8)))\n",
    "            hist=self.transform(Image.fromarray(hist))\n",
    "        \n",
    "        return {'mri':mri,'hist':hist},torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                import zipfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:img_data=nib.load(f).get_fdata()\n",
    "                else:img_data=nib.load(str(path)).get_fdata()\n",
    "            else:img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except:\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== MODELS ====================\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class HistologyModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.mri_backbone=models.resnet50(pretrained=True)\n",
    "        self.hist_backbone=models.resnet50(pretrained=True)\n",
    "        self.mri_backbone.fc=nn.Identity();self.hist_backbone.fc=nn.Identity()\n",
    "        self.fusion=nn.Sequential(nn.Linear(4096,512),nn.ReLU(),nn.Dropout(0.5),nn.Linear(512,num_classes))\n",
    "    def forward(self,x):\n",
    "        mri_feat=self.mri_backbone(x['mri']);hist_feat=self.hist_backbone(x['hist'])\n",
    "        combined=torch.cat([mri_feat,hist_feat],dim=1);return self.fusion(combined)\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def get_weighted_sampler(labels):\n",
    "    class_counts=Counter(labels);weights=[1.0/class_counts[l] for l in labels]\n",
    "    return WeightedRandomSampler(weights,len(weights),replacement=True)\n",
    "\n",
    "def train_epoch(model,loader,criterion,optimizer,device):\n",
    "    model.train();total_loss=0;all_preds=[];all_labels=[]\n",
    "    for batch in tqdm(loader,desc=\"Training\"):\n",
    "        if len(batch)==3:\n",
    "            if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "            else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "        else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad();outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "        loss.backward();optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item();preds=outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds)\n",
    "    return total_loss/len(loader),acc\n",
    "\n",
    "def validate(model,loader,criterion,device):\n",
    "    model.eval();total_loss=0;all_preds=[];all_labels=[];all_probs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader,desc=\"Validation\"):\n",
    "            if len(batch)==3:\n",
    "                if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "                else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "            else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "            total_loss+=loss.item();probs=F.softmax(outputs,dim=1);preds=outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:,1].cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds);f1=f1_score(all_labels,all_preds,average='macro')\n",
    "    auc_score=roc_auc_score(all_labels,all_probs) if len(set(all_labels))>1 else 0\n",
    "    return total_loss/len(loader),acc,f1,auc_score,all_preds,all_labels,all_probs\n",
    "\n",
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "def train_model(model_type='mri'):\n",
    "    print(f\"\\n{'='*60}\\nTraining {model_type.upper()} Model\\n{'='*60}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    val_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.ToTensor(),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    \n",
    "    # Split data\n",
    "    train_ids,test_ids,train_labels,test_labels=train_test_split(\n",
    "        patient_ids,labels,test_size=0.2,random_state=42,stratify=labels)\n",
    "    \n",
    "    # For histology, we need to use Accession IDs (first column) since SVS files use those\n",
    "    if model_type=='histology':\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        # Create mapping to keep indices aligned\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        hist_train_ids=[hist_patient_ids[i] for i in train_idx]\n",
    "        hist_test_ids=[hist_patient_ids[i] for i in test_idx]\n",
    "        hist_train_labels=[labels[i] for i in train_idx]\n",
    "        hist_test_labels=[labels[i] for i in test_idx]\n",
    "        \n",
    "        train_dataset=HistologyDataset(hist_train_ids,hist_train_labels,cfg.histology_base,train_transform)\n",
    "        test_dataset=HistologyDataset(hist_test_ids,hist_test_labels,cfg.histology_base,val_transform)\n",
    "        model=HistologyModel().to(cfg.device)\n",
    "    elif model_type=='mri':\n",
    "        train_dataset=MRIDataset(train_ids,train_labels,cfg.mri_base,train_transform)\n",
    "        test_dataset=MRIDataset(test_ids,test_labels,cfg.mri_base,val_transform)\n",
    "        model=MRIModel().to(cfg.device)\n",
    "    else:  # combined\n",
    "        # For combined, we need both ID types\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        \n",
    "        train_dataset=CombinedDataset(train_ids,train_labels,cfg.mri_base,cfg.histology_base,train_transform,\n",
    "                                      hist_ids=[hist_patient_ids[i] for i in train_idx])\n",
    "        test_dataset=CombinedDataset(test_ids,test_labels,cfg.mri_base,cfg.histology_base,val_transform,\n",
    "                                     hist_ids=[hist_patient_ids[i] for i in test_idx])\n",
    "        model=CombinedModel().to(cfg.device)\n",
    "    \n",
    "    # Get train labels for weighted sampling\n",
    "    if hasattr(train_dataset,'valid_data'):\n",
    "        train_dataset_labels=[train_dataset.labels[train_dataset.valid_data[i][0]] for i in range(len(train_dataset))]\n",
    "    else:\n",
    "        train_dataset_labels=[train_dataset.labels[i] for i in range(len(train_dataset))]\n",
    "    sampler=get_weighted_sampler(train_dataset_labels)\n",
    "    \n",
    "    train_loader=DataLoader(train_dataset,batch_size=cfg.batch_size,sampler=sampler,num_workers=0)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=cfg.batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=cfg.lr)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max',patience=5)\n",
    "    \n",
    "    best_f1=0;patience_counter=0;history={'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[],'val_f1':[],'val_auc':[]}\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        train_loss,train_acc=train_epoch(model,train_loader,criterion,optimizer,cfg.device)\n",
    "        val_loss,val_acc,val_f1,val_auc,_,_,_=validate(model,test_loader,criterion,cfg.device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss);history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss);history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1);history['val_auc'].append(val_auc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        if val_f1>best_f1:\n",
    "            best_f1=val_f1;patience_counter=0\n",
    "            torch.save(model.state_dict(),f\"{cfg.output_dir}/{model_type}_best_model.pth\")\n",
    "            print(f\"Model saved! Best F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter+=1\n",
    "            if patience_counter>=cfg.patience:print(\"Early stopping\");break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load(f\"{cfg.output_dir}/{model_type}_best_model.pth\"))\n",
    "    _,final_acc,final_f1,final_auc,preds,true_labels,probs=validate(model,test_loader,criterion,cfg.device)\n",
    "    \n",
    "    results={'model_type':model_type,'accuracy':final_acc,'f1_score':final_f1,'auc':final_auc,\n",
    "             'predictions':preds,'true_labels':true_labels,'probabilities':probs,'history':history}\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{cfg.output_dir}/{model_type}_results.json\",'w') as f:\n",
    "        json.dump({k:v for k,v in results.items() if k not in['predictions','true_labels','probabilities']},f,indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== EVALUATION & VISUALIZATION ====================\n",
    "def plot_confusion_matrix(y_true,y_pred,title,save_path):\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(8,6));sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',cbar=True)\n",
    "    plt.title(title,fontsize=16);plt.ylabel('True Label');plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_roc_curves(results_dict,save_path):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    for model_type,results in results_dict.items():\n",
    "        fpr,tpr,_=roc_curve(results['true_labels'],results['probabilities'])\n",
    "        auc_score=auc(fpr,tpr)\n",
    "        plt.plot(fpr,tpr,label=f\"{model_type.upper()} (AUC={auc_score:.3f})\",linewidth=2)\n",
    "    plt.plot([0,1],[0,1],'k--',label='Random');plt.xlabel('False Positive Rate',fontsize=12)\n",
    "    plt.ylabel('True Positive Rate',fontsize=12);plt.title('ROC Curves Comparison',fontsize=16)\n",
    "    plt.legend(fontsize=10);plt.grid(alpha=0.3);plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_training_history(history,model_type,save_path):\n",
    "    fig,axes=plt.subplots(2,2,figsize=(15,10))\n",
    "    epochs=range(1,len(history['train_loss'])+1)\n",
    "    \n",
    "    axes[0,0].plot(epochs,history['train_loss'],label='Train');axes[0,0].plot(epochs,history['val_loss'],label='Val')\n",
    "    axes[0,0].set_title(f'{model_type.upper()} - Loss');axes[0,0].set_xlabel('Epoch');axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(epochs,history['train_acc'],label='Train');axes[0,1].plot(epochs,history['val_acc'],label='Val')\n",
    "    axes[0,1].set_title(f'{model_type.upper()} - Accuracy');axes[0,1].set_xlabel('Epoch');axes[0,1].legend()\n",
    "    \n",
    "    axes[1,0].plot(epochs,history['val_f1'],label='Val F1',color='green')\n",
    "    axes[1,0].set_title(f'{model_type.upper()} - F1 Score');axes[1,0].set_xlabel('Epoch');axes[1,0].legend()\n",
    "    \n",
    "    axes[1,1].plot(epochs,history['val_auc'],label='Val AUC',color='red')\n",
    "    axes[1,1].set_title(f'{model_type.upper()} - AUC');axes[1,1].set_xlabel('Epoch');axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def create_comparison_table(results_dict,save_path):\n",
    "    data=[]\n",
    "    for model_type,results in results_dict.items():\n",
    "        y_true,y_pred=results['true_labels'],results['predictions']\n",
    "        precision=precision_score(y_true,y_pred,average='macro')\n",
    "        recall=recall_score(y_true,y_pred,average='macro')\n",
    "        data.append({'Model':model_type.upper(),'Accuracy':f\"{results['accuracy']:.4f}\",\n",
    "                    'Precision':f\"{precision:.4f}\",'Recall':f\"{recall:.4f}\",\n",
    "                    'F1-Score':f\"{results['f1_score']:.4f}\",'AUC':f\"{results['auc']:.4f}\"})\n",
    "    \n",
    "    df=pd.DataFrame(data)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(12,3));ax.axis('tight');ax.axis('off')\n",
    "    table=ax.table(cellText=df.values,colLabels=df.columns,cellLoc='center',loc='center',\n",
    "                   colColours=['#40466e']*len(df.columns))\n",
    "    table.auto_set_font_size(False);table.set_fontsize(10);table.scale(1,2)\n",
    "    \n",
    "    for i in range(len(df.columns)):table[(0,i)].set_facecolor('#40466e');table[(0,i)].set_text_props(weight='bold',color='white')\n",
    "    \n",
    "    plt.title('Model Comparison Table',fontsize=16,pad=20)\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "    \n",
    "    df.to_csv(save_path.replace('.png','.csv'),index=False)\n",
    "    return df\n",
    "\n",
    "def plot_metrics_comparison(results_dict,save_path):\n",
    "    metrics=['accuracy','f1_score','auc']\n",
    "    data={m:[] for m in metrics};models=[]\n",
    "    \n",
    "    for model_type,results in results_dict.items():\n",
    "        models.append(model_type.upper())\n",
    "        for m in metrics:data[m].append(results[m])\n",
    "    \n",
    "    x=np.arange(len(models));width=0.25\n",
    "    fig,ax=plt.subplots(figsize=(12,7))\n",
    "    \n",
    "    colors=['#3498db','#e74c3c','#2ecc71']\n",
    "    for i,m in enumerate(metrics):\n",
    "        ax.bar(x+i*width,data[m],width,label=m.replace('_',' ').title(),color=colors[i])\n",
    "        for j,v in enumerate(data[m]):ax.text(j+i*width,v+0.01,f'{v:.3f}',ha='center',va='bottom',fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Models',fontsize=12);ax.set_ylabel('Score',fontsize=12)\n",
    "    ax.set_title('Performance Metrics Comparison',fontsize=16)\n",
    "    ax.set_xticks(x+width);ax.set_xticklabels(models);ax.legend();ax.grid(axis='y',alpha=0.3)\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__==\"__main__\":\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train all models\n",
    "    results={}\n",
    "    for model_type in ['mri','histology','combined']:\n",
    "        try:\n",
    "            results[model_type]=train_model(model_type)\n",
    "            print(f\"\\n{model_type.upper()} Model - Final Results:\")\n",
    "            print(f\"Accuracy: {results[model_type]['accuracy']:.4f}\")\n",
    "            print(f\"F1-Score: {results[model_type]['f1_score']:.4f}\")\n",
    "            print(f\"AUC: {results[model_type]['auc']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_type} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_type,res in results.items():\n",
    "        # Confusion Matrix\n",
    "        plot_confusion_matrix(res['true_labels'],res['predictions'],\n",
    "                            f'{model_type.upper()} - Confusion Matrix',\n",
    "                            f\"{cfg.output_dir}/{model_type}_confusion_matrix.png\")\n",
    "        \n",
    "        # Training History\n",
    "        plot_training_history(res['history'],model_type,\n",
    "                            f\"{cfg.output_dir}/{model_type}_training_history.png\")\n",
    "        \n",
    "        # Classification Report\n",
    "        report=classification_report(res['true_labels'],res['predictions'],\n",
    "                                    target_names=['WT','Mutation'],output_dict=True)\n",
    "        report_df=pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(f\"{cfg.output_dir}/{model_type}_classification_report.csv\")\n",
    "    \n",
    "    # Comparison visualizations\n",
    "    if len(results)>0:\n",
    "        plot_roc_curves(results,f\"{cfg.output_dir}/roc_curves_comparison.png\")\n",
    "        comparison_df=create_comparison_table(results,f\"{cfg.output_dir}/comparison_table.png\")\n",
    "        plot_metrics_comparison(results,f\"{cfg.output_dir}/metrics_comparison.png\")\n",
    "        \n",
    "        print(\"\\n\"+\"=\"*80)\n",
    "        print(\"FINAL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Statistical significance test\n",
    "        if len(results)==3:\n",
    "            print(\"\\n=== Statistical Analysis ===\")\n",
    "            combined_f1=results['combined']['f1_score']\n",
    "            mri_f1=results['mri']['f1_score']\n",
    "            hist_f1=results['histology']['f1_score']\n",
    "            \n",
    "            print(f\"Combined vs MRI improvement: {((combined_f1-mri_f1)/mri_f1)*100:.2f}%\")\n",
    "            print(f\"Combined vs Histology improvement: {((combined_f1-hist_f1)/hist_f1)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All results saved to: {cfg.output_dir}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6fec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== Loading Clinical Data ===\n",
      "Clinical data shape: (111, 36)\n",
      "Columns: ['Accession', 'Patient ID', 'Age at BM Dx', 'Sex', 'Pack-Year Smoking Hx', 'NIH Racial Category', 'KPS at BM Dx', 'Age at Resection', 'Size of Dominant Lesion', 'Number of BM Lesions at Dx', 'Location of Lesion', 'Histologic Subtype', 'GPA Histology Class', 'Presence of Extracranial Mets at Dx (1 = Yes; 0 = No)', 'EGFR Status (1 = Mutation; 0 = WT)', 'ALK Status (1 = Mutation; 0 = WT)', 'PD-L1 Status (1 = Mutation; 0 = WT)', 'Sperduto GPA', 'patientID', 'Original Scan Facility', 'Scanner vendor', 'Scanner model', 'field_strength', '2D_3D_acquisition', 'FLAIR-Available', 'T1CE-Available', 'flair_slice_thickness', 'flair_spacing', 'flair_repetition_time', 'flair_echo_time', 'flair_spin_echo', 't1ce_slice_thickness', 't1ce_spacing', 't1ce_repetition_time', 't1ce_echo_time', 't1ce_spin_echo']\n",
      "Using target column: 'EGFR Status (1 = Mutation; 0 = WT)'\n",
      "Target distribution:\n",
      "EGFR Status (1 = Mutation; 0 = WT)\n",
      "0    99\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "Target encoding: 1=Mutation, 0=WT\n",
      "Imbalance ratio: 8.25:1\n",
      "\n",
      "Trying 'patientID' column for matching...\n",
      "Sample patientID values: ['YG_0427RC24FT6W', 'YG_0AXGKD8AFJGS', 'YG_0IBUXTBINCD9', 'YG_0LCI9NV44UK0', 'YG_1BZ1NP805Q9J']\n",
      "\n",
      "================================================================================\n",
      "EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Training MRI Model\n",
      "============================================================\n",
      "Valid MRI samples: 80/88\n",
      "Valid MRI samples: 20/23\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:56<00:00, 11.28s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5704 | Train Acc: 0.6750\n",
      "Val Loss: 0.3387 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.5789\n",
      "Model saved! Best F1: 0.4737\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:51<00:00, 10.32s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3217 | Train Acc: 0.8875\n",
      "Val Loss: 0.4122 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:48<00:00,  9.73s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2310 | Train Acc: 0.8875\n",
      "Val Loss: 0.4031 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:49<00:00,  9.95s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2242 | Train Acc: 0.8875\n",
      "Val Loss: 0.4350 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.3158\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:55<00:00, 11.06s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1012 | Train Acc: 0.9500\n",
      "Val Loss: 0.3696 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.2105\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:56<00:00, 11.20s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0513 | Train Acc: 1.0000\n",
      "Val Loss: 0.4665 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.2632\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:53<00:00, 10.72s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0751 | Train Acc: 0.9750\n",
      "Val Loss: 0.4039 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.3684\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:55<00:00, 11.04s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0364 | Train Acc: 0.9875\n",
      "Val Loss: 0.4109 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:49<00:00,  9.84s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1197 | Train Acc: 0.9375\n",
      "Val Loss: 0.3926 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.3684\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:54<00:00, 10.87s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0440 | Train Acc: 0.9875\n",
      "Val Loss: 0.2515 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:48<00:00,  9.74s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0837 | Train Acc: 0.9625\n",
      "Val Loss: 0.1798 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:51<00:00, 10.27s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0908 | Train Acc: 0.9500\n",
      "Val Loss: 0.2016 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:52<00:00, 10.44s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0119 | Train Acc: 1.0000\n",
      "Val Loss: 0.2211 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:05<00:00, 13.09s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0141 | Train Acc: 1.0000\n",
      "Val Loss: 0.2024 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:07<00:00, 13.51s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0328 | Train Acc: 0.9875\n",
      "Val Loss: 0.1932 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:57<00:00, 11.58s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0142 | Train Acc: 1.0000\n",
      "Val Loss: 0.1884 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:52<00:00, 10.55s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0317 | Train Acc: 1.0000\n",
      "Val Loss: 0.1966 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 20 test samples\n",
      "Class distribution - True labels: Counter({0: 19, 1: 1})\n",
      "Class distribution - Predictions: Counter({0: 20})\n",
      "Probability range: [0.0000, 0.4614]\n",
      "Accuracy: 0.9500 | F1: 0.4872 | AUC: 0.3684\n",
      "============================================================\n",
      "\n",
      "\n",
      "MRI Model - Final Results:\n",
      "Accuracy: 0.9500\n",
      "F1-Score: 0.4872\n",
      "AUC: 0.3684\n",
      "\n",
      "============================================================\n",
      "Training HISTOLOGY Model\n",
      "============================================================\n",
      "Valid Histology samples: 91/91\n",
      "Valid Histology samples: 26/26\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:46<00:00,  7.82s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:09<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5709 | Train Acc: 0.6484\n",
      "Val Loss: 0.6725 | Val Acc: 0.5000 | Val F1: 0.3910 | Val AUC: 0.5625\n",
      "Model saved! Best F1: 0.3910\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:46<00:00,  7.81s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3245 | Train Acc: 0.8681\n",
      "Val Loss: 0.3282 | Val Acc: 0.8846 | Val F1: 0.6681 | Val AUC: 0.6250\n",
      "Model saved! Best F1: 0.6681\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:55<00:00,  9.33s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1504 | Train Acc: 0.9341\n",
      "Val Loss: 0.2102 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.6875\n",
      "Model saved! Best F1: 0.8231\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:47<00:00,  7.99s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0987 | Train Acc: 0.9451\n",
      "Val Loss: 0.1938 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.7917\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:59<00:00,  9.94s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0704 | Train Acc: 0.9780\n",
      "Val Loss: 0.1763 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.8958\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:54<00:00,  9.09s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1204 | Train Acc: 0.9670\n",
      "Val Loss: 0.1768 | Val Acc: 0.8462 | Val F1: 0.6232 | Val AUC: 0.9375\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:49<00:00,  8.27s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0192 | Train Acc: 1.0000\n",
      "Val Loss: 0.1215 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.9375\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:58<00:00,  9.79s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0898 | Train Acc: 0.9780\n",
      "Val Loss: 0.3344 | Val Acc: 0.8462 | Val F1: 0.4583 | Val AUC: 0.7917\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:53<00:00,  8.88s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0322 | Train Acc: 0.9890\n",
      "Val Loss: 0.3031 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7083\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:46<00:00,  7.82s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0159 | Train Acc: 1.0000\n",
      "Val Loss: 0.3020 | Val Acc: 0.8846 | Val F1: 0.4694 | Val AUC: 0.6875\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:50<00:00,  8.47s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0819 | Train Acc: 0.9780\n",
      "Val Loss: 0.2669 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.6875\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:46<00:00,  7.76s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:09<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0615 | Train Acc: 0.9890\n",
      "Val Loss: 0.2682 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7708\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:47<00:00,  7.95s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:09<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0209 | Train Acc: 0.9890\n",
      "Val Loss: 0.2558 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7292\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 26 test samples\n",
      "Class distribution - True labels: Counter({0: 24, 1: 2})\n",
      "Class distribution - Predictions: Counter({0: 25, 1: 1})\n",
      "Probability range: [0.0029, 0.5110]\n",
      "Accuracy: 0.9615 | F1: 0.8231 | AUC: 0.6875\n",
      "============================================================\n",
      "\n",
      "\n",
      "HISTOLOGY Model - Final Results:\n",
      "Accuracy: 0.9615\n",
      "F1-Score: 0.8231\n",
      "AUC: 0.6875\n",
      "\n",
      "============================================================\n",
      "Training COMBINED Model\n",
      "============================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [02:09<00:00, 25.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6617 | Train Acc: 0.6000\n",
      "Val Loss: 0.7833 | Val Acc: 0.2500 | Val F1: 0.2327 | Val AUC: 0.3158\n",
      "Model saved! Best F1: 0.2327\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:49<00:00, 21.85s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:18<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5256 | Train Acc: 0.7375\n",
      "Val Loss: 1.0834 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:56<00:00, 23.38s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4529 | Train Acc: 0.7875\n",
      "Val Loss: 0.1684 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.8421\n",
      "Model saved! Best F1: 0.4737\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:38<00:00, 19.63s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1343 | Train Acc: 1.0000\n",
      "Val Loss: 0.1801 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6316\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:43<00:00, 20.65s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1569 | Train Acc: 0.9625\n",
      "Val Loss: 0.2062 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:30<00:00, 18.19s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0564 | Train Acc: 0.9750\n",
      "Val Loss: 0.2605 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:32<00:00, 18.44s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0480 | Train Acc: 0.9875\n",
      "Val Loss: 0.2985 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:40<00:00, 20.02s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0447 | Train Acc: 0.9875\n",
      "Val Loss: 0.2692 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:38<00:00, 19.71s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0391 | Train Acc: 0.9875\n",
      "Val Loss: 0.1952 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:32<00:00, 18.50s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0594 | Train Acc: 0.9750\n",
      "Val Loss: 0.1901 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:40<00:00, 20.04s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0442 | Train Acc: 0.9875\n",
      "Val Loss: 0.1695 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:33<00:00, 18.66s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0158 | Train Acc: 1.0000\n",
      "Val Loss: 0.1690 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:28<00:00, 17.77s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0173 | Train Acc: 1.0000\n",
      "Val Loss: 0.1650 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:38<00:00, 19.64s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0133 | Train Acc: 1.0000\n",
      "Val Loss: 0.1624 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6842\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 20 test samples\n",
      "Class distribution - True labels: Counter({0: 19, 1: 1})\n",
      "Class distribution - Predictions: Counter({0: 20})\n",
      "Probability range: [0.0002, 0.2622]\n",
      "Accuracy: 0.9500 | F1: 0.4872 | AUC: 0.6316\n",
      "============================================================\n",
      "\n",
      "\n",
      "COMBINED Model - Final Results:\n",
      "Accuracy: 0.9500\n",
      "F1-Score: 0.4872\n",
      "AUC: 0.6316\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON\n",
      "================================================================================\n",
      "    Model Accuracy Precision Recall F1-Score    AUC\n",
      "      MRI   0.9500    0.4750 0.5000   0.4872 0.3684\n",
      "HISTOLOGY   0.9615    0.9800 0.7500   0.8231 0.6875\n",
      " COMBINED   0.9500    0.4750 0.5000   0.4872 0.6316\n",
      "\n",
      "=== Statistical Analysis ===\n",
      "Combined vs MRI improvement: 0.00%\n",
      "Combined vs Histology improvement: -40.81%\n",
      "\n",
      "================================================================================\n",
      "All results saved to: ./egfr_results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EGFR Mutation Prediction: MRI vs Histology vs Combined Approach\n",
    "# Dataset: Brain Metastases with class imbalance 8.65:1\n",
    "# \n",
    "# Install required packages:\n",
    "# pip install nibabel torch torchvision opencv-python pillow pandas openpyxl scikit-learn matplotlib seaborn tqdm\n",
    "# pip install openslide-python (optional, for better SVS support)\n",
    "\n",
    "import os;os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np,pandas as pd,torch,torch.nn as nn,torch.nn.functional as F\n",
    "import torchvision.models as models,torchvision.transforms as transforms\n",
    "from PIL import Image;Image.MAX_IMAGE_PIXELS=None\n",
    "from pathlib import Path;from datetime import datetime;import cv2\n",
    "from scipy.spatial.distance import cosine,euclidean,pdist,squareform;from scipy import stats\n",
    "import matplotlib.pyplot as plt,seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import random,glob,json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:import nibabel as nib\n",
    "except:print(\"Warning: nibabel not installed. Install with: pip install nibabel\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);np.random.seed(seed);torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed);torch.backends.cudnn.deterministic=True\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    clinical_path=r\"D:\\paper\\external.xlsx\"\n",
    "    histology_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "    mri_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_radiology_images\\Brain-Mets-Lung-MRI-Path-Segs\"\n",
    "    output_dir=\"./egfr_results\"\n",
    "    \n",
    "    # Training params\n",
    "    img_size=224;batch_size=16;epochs=50;lr=1e-4;patience=10\n",
    "    n_folds=5;device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    imbalance_ratio=8.65\n",
    "    \n",
    "cfg=Config()\n",
    "os.makedirs(cfg.output_dir,exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "print(\"\\n=== Loading Clinical Data ===\")\n",
    "clinical_df=pd.read_excel(cfg.clinical_path) if cfg.clinical_path.endswith('.xlsx') else pd.read_csv(cfg.clinical_path)\n",
    "print(f\"Clinical data shape: {clinical_df.shape}\")\n",
    "print(f\"Columns: {clinical_df.columns.tolist()}\")\n",
    "\n",
    "# Find target column - handle different possible names\n",
    "target_col=None\n",
    "for col in clinical_df.columns:\n",
    "    if 'EGFR' in col.upper() and 'STATUS' in col.upper():\n",
    "        target_col=col\n",
    "        break\n",
    "if target_col is None:\n",
    "    print(\"ERROR: EGFR Status column not found!\")\n",
    "    print(f\"Available columns: {clinical_df.columns.tolist()}\")\n",
    "    raise ValueError(\"Target column 'EGFR Status' not found in dataset\")\n",
    "\n",
    "print(f\"Using target column: '{target_col}'\")\n",
    "print(f\"Target distribution:\\n{clinical_df[target_col].value_counts()}\")\n",
    "print(f\"Target encoding: 1=Mutation, 0=WT\")\n",
    "print(f\"Imbalance ratio: {clinical_df[target_col].value_counts()[0]/clinical_df[target_col].value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Get patient IDs and labels\n",
    "# Check if there's a 'patientID' column that might match better\n",
    "if 'patientID' in clinical_df.columns:\n",
    "    print(\"\\nTrying 'patientID' column for matching...\")\n",
    "    patient_ids=clinical_df['patientID'].astype(str).tolist()\n",
    "    print(f\"Sample patientID values: {patient_ids[:5]}\")\n",
    "else:\n",
    "    patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "    print(f\"Using first column for patient IDs: {patient_ids[:5]}\")\n",
    "\n",
    "labels=clinical_df[target_col].values\n",
    "\n",
    "# ==================== MRI DATASET ====================\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        all_dirs={d.name:d for d in self.base_path.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            patient_dir=None\n",
    "            \n",
    "            # Try direct match\n",
    "            if pid in all_dirs:\n",
    "                patient_dir=all_dirs[pid]\n",
    "            # Try without YG_ prefix if it exists\n",
    "            elif pid.startswith('YG_'):\n",
    "                pid_without_prefix=pid[3:]\n",
    "                if pid_without_prefix in all_dirs:\n",
    "                    patient_dir=all_dirs[pid_without_prefix]\n",
    "            # Try adding YG_ prefix if not present\n",
    "            elif f\"YG_{pid}\" in all_dirs:\n",
    "                patient_dir=all_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            if patient_dir and patient_dir.exists():\n",
    "                # Look for t1ce and flair files\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                \n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    valid.append((idx,patient_dir,t1ce_files[0],flair_files[0]))\n",
    "        \n",
    "        print(f\"Valid MRI samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        if len(valid)==0:\n",
    "            print(f\"Sample patient IDs: {self.patient_ids[:5]}\")\n",
    "            print(f\"Sample directories: {list(all_dirs.keys())[:5]}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,patient_dir,t1ce_path,flair_path=self.valid_data[idx]\n",
    "        pid=self.patient_ids[real_idx];label=self.labels[real_idx]\n",
    "        \n",
    "        # Load images\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        \n",
    "        # Stack as 3-channel (T1CE, FLAIR, Average)\n",
    "        avg=(t1ce_img+flair_img)/2\n",
    "        img=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        if self.transform:img=self.transform(Image.fromarray(img.astype(np.uint8)))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            # Handle compressed .nii files\n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                # It's a compressed archive, try to extract\n",
    "                import zipfile,gzip,tarfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:\n",
    "                                img_data=nib.load(f).get_fdata()\n",
    "                else:\n",
    "                    # Try loading directly\n",
    "                    img_data=nib.load(str(path)).get_fdata()\n",
    "            else:\n",
    "                img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            # Get middle slice\n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            # Normalize to 0-255\n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading NIfTI {path}: {e}\")\n",
    "            # Fallback: try as image\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:\n",
    "                # Return blank image as last resort\n",
    "                return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "\n",
    "# ==================== HISTOLOGY DATASET ====================\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            svs_files=list(self.base_path.glob(f\"*{pid}*.svs\"))\n",
    "            if len(svs_files)==0:svs_files=list(self.base_path.glob(f\"YG_{pid}*.svs\"))\n",
    "            if len(svs_files)>0:valid.append((idx,svs_files[0]))\n",
    "        print(f\"Valid Histology samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,svs_path=self.valid_data[idx];label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        img=self._load_svs(svs_path)\n",
    "        if self.transform:img=self.transform(Image.fromarray(img))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== COMBINED DATASET ====================\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,mri_base,hist_base,transform=None,hist_ids=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.mri_base=Path(mri_base)\n",
    "        self.hist_base=Path(hist_base);self.transform=transform\n",
    "        self.hist_ids=hist_ids if hist_ids else patient_ids  # Use separate hist IDs if provided\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[];all_mri_dirs={d.name:d for d in self.mri_base.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            hist_pid=self.hist_ids[idx]\n",
    "            \n",
    "            # Check MRI using patientID\n",
    "            patient_dir=None\n",
    "            if pid in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid]\n",
    "            elif pid.startswith('YG_') and pid[3:] in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid[3:]]\n",
    "            elif f\"YG_{pid}\" in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            has_mri=False;t1ce_file=None;flair_file=None\n",
    "            if patient_dir and patient_dir.exists():\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    has_mri=True;t1ce_file=t1ce_files[0];flair_file=flair_files[0]\n",
    "            \n",
    "            # Check Histology using Accession ID\n",
    "            svs_files=list(self.hist_base.glob(f\"*{hist_pid}*.svs\"))\n",
    "            if len(svs_files)==0 and hist_pid.startswith('YG_'):\n",
    "                svs_files=list(self.hist_base.glob(f\"*{hist_pid[3:]}*.svs\"))\n",
    "            has_hist=len(svs_files)>0\n",
    "            \n",
    "            if has_mri and has_hist:valid.append((idx,patient_dir,t1ce_file,flair_file,svs_files[0]))\n",
    "        \n",
    "        print(f\"Valid Combined samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,mri_dir,t1ce_path,flair_path,svs_path=self.valid_data[idx]\n",
    "        label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        \n",
    "        # Load MRI\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        avg=(t1ce_img+flair_img)/2;mri=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        # Load Histology\n",
    "        hist=self._load_svs(svs_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            mri=self.transform(Image.fromarray(mri.astype(np.uint8)))\n",
    "            hist=self.transform(Image.fromarray(hist))\n",
    "        \n",
    "        return {'mri':mri,'hist':hist},torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                import zipfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:img_data=nib.load(f).get_fdata()\n",
    "                else:img_data=nib.load(str(path)).get_fdata()\n",
    "            else:img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except:\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== MODELS ====================\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class HistologyModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.mri_backbone=models.resnet50(pretrained=True)\n",
    "        self.hist_backbone=models.resnet50(pretrained=True)\n",
    "        self.mri_backbone.fc=nn.Identity();self.hist_backbone.fc=nn.Identity()\n",
    "        self.fusion=nn.Sequential(nn.Linear(4096,512),nn.ReLU(),nn.Dropout(0.5),nn.Linear(512,num_classes))\n",
    "    def forward(self,x):\n",
    "        mri_feat=self.mri_backbone(x['mri']);hist_feat=self.hist_backbone(x['hist'])\n",
    "        combined=torch.cat([mri_feat,hist_feat],dim=1);return self.fusion(combined)\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def get_weighted_sampler(labels):\n",
    "    class_counts=Counter(labels);weights=[1.0/class_counts[l] for l in labels]\n",
    "    return WeightedRandomSampler(weights,len(weights),replacement=True)\n",
    "\n",
    "def train_epoch(model,loader,criterion,optimizer,device):\n",
    "    model.train();total_loss=0;all_preds=[];all_labels=[]\n",
    "    for batch in tqdm(loader,desc=\"Training\"):\n",
    "        if len(batch)==3:\n",
    "            if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "            else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "        else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad();outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "        loss.backward();optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item();preds=outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds)\n",
    "    return total_loss/len(loader),acc\n",
    "\n",
    "def validate(model,loader,criterion,device):\n",
    "    model.eval();total_loss=0;all_preds=[];all_labels=[];all_probs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader,desc=\"Validation\"):\n",
    "            if len(batch)==3:\n",
    "                if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "                else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "            else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "            total_loss+=loss.item();probs=F.softmax(outputs,dim=1);preds=outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:,1].cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds);f1=f1_score(all_labels,all_preds,average='macro')\n",
    "    \n",
    "    # Calculate AUC with validation\n",
    "    try:\n",
    "        if len(set(all_labels))>1:  # Need both classes for AUC\n",
    "            auc_score=roc_auc_score(all_labels,all_probs)\n",
    "        else:\n",
    "            print(f\"Warning: Only one class in validation set, AUC set to 0\")\n",
    "            auc_score=0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: AUC calculation failed ({e}), setting to 0\")\n",
    "        auc_score=0.0\n",
    "    \n",
    "    return total_loss/len(loader),acc,f1,auc_score,all_preds,all_labels,all_probs\n",
    "\n",
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "def train_model(model_type='mri'):\n",
    "    print(f\"\\n{'='*60}\\nTraining {model_type.upper()} Model\\n{'='*60}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    val_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.ToTensor(),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    \n",
    "    # Split data\n",
    "    train_ids,test_ids,train_labels,test_labels=train_test_split(\n",
    "        patient_ids,labels,test_size=0.2,random_state=42,stratify=labels)\n",
    "    \n",
    "    # For histology, we need to use Accession IDs (first column) since SVS files use those\n",
    "    if model_type=='histology':\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        # Create mapping to keep indices aligned\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        hist_train_ids=[hist_patient_ids[i] for i in train_idx]\n",
    "        hist_test_ids=[hist_patient_ids[i] for i in test_idx]\n",
    "        hist_train_labels=[labels[i] for i in train_idx]\n",
    "        hist_test_labels=[labels[i] for i in test_idx]\n",
    "        \n",
    "        train_dataset=HistologyDataset(hist_train_ids,hist_train_labels,cfg.histology_base,train_transform)\n",
    "        test_dataset=HistologyDataset(hist_test_ids,hist_test_labels,cfg.histology_base,val_transform)\n",
    "        model=HistologyModel().to(cfg.device)\n",
    "    elif model_type=='mri':\n",
    "        train_dataset=MRIDataset(train_ids,train_labels,cfg.mri_base,train_transform)\n",
    "        test_dataset=MRIDataset(test_ids,test_labels,cfg.mri_base,val_transform)\n",
    "        model=MRIModel().to(cfg.device)\n",
    "    else:  # combined\n",
    "        # For combined, we need both ID types\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        \n",
    "        train_dataset=CombinedDataset(train_ids,train_labels,cfg.mri_base,cfg.histology_base,train_transform,\n",
    "                                      hist_ids=[hist_patient_ids[i] for i in train_idx])\n",
    "        test_dataset=CombinedDataset(test_ids,test_labels,cfg.mri_base,cfg.histology_base,val_transform,\n",
    "                                     hist_ids=[hist_patient_ids[i] for i in test_idx])\n",
    "        model=CombinedModel().to(cfg.device)\n",
    "    \n",
    "    # Get train labels for weighted sampling\n",
    "    if hasattr(train_dataset,'valid_data'):\n",
    "        train_dataset_labels=[train_dataset.labels[train_dataset.valid_data[i][0]] for i in range(len(train_dataset))]\n",
    "    else:\n",
    "        train_dataset_labels=[train_dataset.labels[i] for i in range(len(train_dataset))]\n",
    "    sampler=get_weighted_sampler(train_dataset_labels)\n",
    "    \n",
    "    train_loader=DataLoader(train_dataset,batch_size=cfg.batch_size,sampler=sampler,num_workers=0)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=cfg.batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=cfg.lr)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max',patience=5)\n",
    "    \n",
    "    best_f1=0;patience_counter=0;history={'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[],'val_f1':[],'val_auc':[]}\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        train_loss,train_acc=train_epoch(model,train_loader,criterion,optimizer,cfg.device)\n",
    "        val_loss,val_acc,val_f1,val_auc,_,_,_=validate(model,test_loader,criterion,cfg.device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss);history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss);history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1);history['val_auc'].append(val_auc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        if val_f1>best_f1:\n",
    "            best_f1=val_f1;patience_counter=0\n",
    "            torch.save(model.state_dict(),f\"{cfg.output_dir}/{model_type}_best_model.pth\")\n",
    "            print(f\"Model saved! Best F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter+=1\n",
    "            if patience_counter>=cfg.patience:print(\"Early stopping\");break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load(f\"{cfg.output_dir}/{model_type}_best_model.pth\"))\n",
    "    _,final_acc,final_f1,final_auc,preds,true_labels,probs=validate(model,test_loader,criterion,cfg.device)\n",
    "    \n",
    "    # Double check we have real predictions\n",
    "    assert len(preds)==len(true_labels), \"Prediction count mismatch!\"\n",
    "    assert len(probs)==len(true_labels), \"Probability count mismatch!\"\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final evaluation on {len(preds)} test samples\")\n",
    "    print(f\"Class distribution - True labels: {Counter(true_labels)}\")\n",
    "    print(f\"Class distribution - Predictions: {Counter(preds)}\")\n",
    "    print(f\"Probability range: [{min(probs):.4f}, {max(probs):.4f}]\")\n",
    "    print(f\"Accuracy: {final_acc:.4f} | F1: {final_f1:.4f} | AUC: {final_auc:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    results={'model_type':model_type,'accuracy':float(final_acc),'f1_score':float(final_f1),'auc':float(final_auc),\n",
    "             'predictions':[int(p) for p in preds],'true_labels':[int(t) for t in true_labels],\n",
    "             'probabilities':[float(p) for p in probs],'history':history}\n",
    "    \n",
    "    # Save results with actual predictions\n",
    "    with open(f\"{cfg.output_dir}/{model_type}_results.json\",'w') as f:\n",
    "        json.dump({k:v if k!='history' else {hk:[float(hv) for hv in hvs] for hk,hvs in v.items()} \n",
    "                   for k,v in results.items()},f,indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== EVALUATION & VISUALIZATION ====================\n",
    "def plot_confusion_matrix(y_true,y_pred,title,save_path):\n",
    "    \"\"\"Plot confusion matrix using actual predictions only\"\"\"\n",
    "    assert len(y_true)==len(y_pred), f\"Data mismatch: {len(y_true)} vs {len(y_pred)}\"\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(8,6));sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',cbar=True)\n",
    "    plt.title(title,fontsize=16);plt.ylabel('True Label');plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_roc_curves(results_dict,save_path):\n",
    "    \"\"\"Plot ROC curves using actual model probabilities only\"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    for model_type,results in results_dict.items():\n",
    "        assert len(results['true_labels'])==len(results['probabilities']), \\\n",
    "            f\"{model_type}: Label/prob mismatch!\"\n",
    "        fpr,tpr,_=roc_curve(results['true_labels'],results['probabilities'])\n",
    "        auc_score=auc(fpr,tpr)\n",
    "        plt.plot(fpr,tpr,label=f\"{model_type.upper()} (AUC={auc_score:.3f})\",linewidth=2)\n",
    "    plt.plot([0,1],[0,1],'k--',label='Random');plt.xlabel('False Positive Rate',fontsize=12)\n",
    "    plt.ylabel('True Positive Rate',fontsize=12);plt.title('ROC Curves Comparison',fontsize=16)\n",
    "    plt.legend(fontsize=10);plt.grid(alpha=0.3);plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_training_history(history,model_type,save_path):\n",
    "    \"\"\"Plot training history from actual training runs only\"\"\"\n",
    "    assert len(history['train_loss'])>0, \"No training history!\"\n",
    "    fig,axes=plt.subplots(2,2,figsize=(15,10))\n",
    "    epochs=range(1,len(history['train_loss'])+1)\n",
    "    \n",
    "    axes[0,0].plot(epochs,history['train_loss'],label='Train');axes[0,0].plot(epochs,history['val_loss'],label='Val')\n",
    "    axes[0,0].set_title(f'{model_type.upper()} - Loss');axes[0,0].set_xlabel('Epoch');axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(epochs,history['train_acc'],label='Train');axes[0,1].plot(epochs,history['val_acc'],label='Val')\n",
    "    axes[0,1].set_title(f'{model_type.upper()} - Accuracy');axes[0,1].set_xlabel('Epoch');axes[0,1].legend()\n",
    "    \n",
    "    axes[1,0].plot(epochs,history['val_f1'],label='Val F1',color='green')\n",
    "    axes[1,0].set_title(f'{model_type.upper()} - F1 Score');axes[1,0].set_xlabel('Epoch');axes[1,0].legend()\n",
    "    \n",
    "    axes[1,1].plot(epochs,history['val_auc'],label='Val AUC',color='red')\n",
    "    axes[1,1].set_title(f'{model_type.upper()} - AUC');axes[1,1].set_xlabel('Epoch');axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def create_comparison_table(results_dict,save_path):\n",
    "    \"\"\"Create comparison table using actual model results only\"\"\"\n",
    "    data=[]\n",
    "    for model_type,results in results_dict.items():\n",
    "        y_true,y_pred=results['true_labels'],results['predictions']\n",
    "        assert len(y_true)==len(y_pred), f\"{model_type}: Data length mismatch!\"\n",
    "        precision=precision_score(y_true,y_pred,average='macro')\n",
    "        recall=recall_score(y_true,y_pred,average='macro')\n",
    "        data.append({'Model':model_type.upper(),'Accuracy':f\"{results['accuracy']:.4f}\",\n",
    "                    'Precision':f\"{precision:.4f}\",'Recall':f\"{recall:.4f}\",\n",
    "                    'F1-Score':f\"{results['f1_score']:.4f}\",'AUC':f\"{results['auc']:.4f}\"})\n",
    "    \n",
    "    df=pd.DataFrame(data)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(12,3));ax.axis('tight');ax.axis('off')\n",
    "    table=ax.table(cellText=df.values,colLabels=df.columns,cellLoc='center',loc='center',\n",
    "                   colColours=['#40466e']*len(df.columns))\n",
    "    table.auto_set_font_size(False);table.set_fontsize(10);table.scale(1,2)\n",
    "    \n",
    "    for i in range(len(df.columns)):table[(0,i)].set_facecolor('#40466e');table[(0,i)].set_text_props(weight='bold',color='white')\n",
    "    \n",
    "    plt.title('Model Comparison Table',fontsize=16,pad=20)\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "    \n",
    "    df.to_csv(save_path.replace('.png','.csv'),index=False)\n",
    "    return df\n",
    "\n",
    "def plot_metrics_comparison(results_dict,save_path):\n",
    "    \"\"\"Plot metrics comparison using actual results only\"\"\"\n",
    "    metrics=['accuracy','f1_score','auc']\n",
    "    data={m:[] for m in metrics};models=[]\n",
    "    \n",
    "    for model_type,results in results_dict.items():\n",
    "        models.append(model_type.upper())\n",
    "        for m in metrics:\n",
    "            assert m in results, f\"{model_type} missing metric {m}!\"\n",
    "            data[m].append(results[m])\n",
    "    \n",
    "    x=np.arange(len(models));width=0.25\n",
    "    fig,ax=plt.subplots(figsize=(12,7))\n",
    "    \n",
    "    colors=['#3498db','#e74c3c','#2ecc71']\n",
    "    for i,m in enumerate(metrics):\n",
    "        ax.bar(x+i*width,data[m],width,label=m.replace('_',' ').title(),color=colors[i])\n",
    "        for j,v in enumerate(data[m]):ax.text(j+i*width,v+0.01,f'{v:.3f}',ha='center',va='bottom',fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Models',fontsize=12);ax.set_ylabel('Score',fontsize=12)\n",
    "    ax.set_title('Performance Metrics Comparison',fontsize=16)\n",
    "    ax.set_xticks(x+width);ax.set_xticklabels(models);ax.legend();ax.grid(axis='y',alpha=0.3)\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__==\"__main__\":\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train all models\n",
    "    results={}\n",
    "    for model_type in ['mri','histology','combined']:\n",
    "        try:\n",
    "            results[model_type]=train_model(model_type)\n",
    "            print(f\"\\n{model_type.upper()} Model - Final Results:\")\n",
    "            print(f\"Accuracy: {results[model_type]['accuracy']:.4f}\")\n",
    "            print(f\"F1-Score: {results[model_type]['f1_score']:.4f}\")\n",
    "            print(f\"AUC: {results[model_type]['auc']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_type} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_type,res in results.items():\n",
    "        # Confusion Matrix\n",
    "        plot_confusion_matrix(res['true_labels'],res['predictions'],\n",
    "                            f'{model_type.upper()} - Confusion Matrix',\n",
    "                            f\"{cfg.output_dir}/{model_type}_confusion_matrix.png\")\n",
    "        \n",
    "        # Training History\n",
    "        plot_training_history(res['history'],model_type,\n",
    "                            f\"{cfg.output_dir}/{model_type}_training_history.png\")\n",
    "        \n",
    "        # Classification Report\n",
    "        report=classification_report(res['true_labels'],res['predictions'],\n",
    "                                    target_names=['WT','Mutation'],output_dict=True)\n",
    "        report_df=pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(f\"{cfg.output_dir}/{model_type}_classification_report.csv\")\n",
    "    \n",
    "    # Comparison visualizations\n",
    "    if len(results)>0:\n",
    "        plot_roc_curves(results,f\"{cfg.output_dir}/roc_curves_comparison.png\")\n",
    "        comparison_df=create_comparison_table(results,f\"{cfg.output_dir}/comparison_table.png\")\n",
    "        plot_metrics_comparison(results,f\"{cfg.output_dir}/metrics_comparison.png\")\n",
    "        \n",
    "        print(\"\\n\"+\"=\"*80)\n",
    "        print(\"FINAL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Statistical significance test\n",
    "        if len(results)==3:\n",
    "            print(\"\\n=== Statistical Analysis ===\")\n",
    "            combined_f1=results['combined']['f1_score']\n",
    "            mri_f1=results['mri']['f1_score']\n",
    "            hist_f1=results['histology']['f1_score']\n",
    "            \n",
    "            print(f\"Combined vs MRI improvement: {((combined_f1-mri_f1)/mri_f1)*100:.2f}%\")\n",
    "            print(f\"Combined vs Histology improvement: {((combined_f1-hist_f1)/hist_f1)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All results saved to: {cfg.output_dir}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e193dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== Loading Clinical Data ===\n",
      "Clinical data shape: (111, 36)\n",
      "Columns: ['Accession', 'Patient ID', 'Age at BM Dx', 'Sex', 'Pack-Year Smoking Hx', 'NIH Racial Category', 'KPS at BM Dx', 'Age at Resection', 'Size of Dominant Lesion', 'Number of BM Lesions at Dx', 'Location of Lesion', 'Histologic Subtype', 'GPA Histology Class', 'Presence of Extracranial Mets at Dx (1 = Yes; 0 = No)', 'EGFR Status (1 = Mutation; 0 = WT)', 'ALK Status (1 = Mutation; 0 = WT)', 'PD-L1 Status (1 = Mutation; 0 = WT)', 'Sperduto GPA', 'patientID', 'Original Scan Facility', 'Scanner vendor', 'Scanner model', 'field_strength', '2D_3D_acquisition', 'FLAIR-Available', 'T1CE-Available', 'flair_slice_thickness', 'flair_spacing', 'flair_repetition_time', 'flair_echo_time', 'flair_spin_echo', 't1ce_slice_thickness', 't1ce_spacing', 't1ce_repetition_time', 't1ce_echo_time', 't1ce_spin_echo']\n",
      "Using target column: 'EGFR Status (1 = Mutation; 0 = WT)'\n",
      "Target distribution:\n",
      "EGFR Status (1 = Mutation; 0 = WT)\n",
      "0    99\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "Target encoding: 1=Mutation, 0=WT\n",
      "Imbalance ratio: 8.25:1\n",
      "\n",
      "Trying 'patientID' column for matching...\n",
      "Sample patientID values: ['YG_0427RC24FT6W', 'YG_0AXGKD8AFJGS', 'YG_0IBUXTBINCD9', 'YG_0LCI9NV44UK0', 'YG_1BZ1NP805Q9J']\n",
      "\n",
      "================================================================================\n",
      "EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Training MRI Model\n",
      "============================================================\n",
      "Valid MRI samples: 80/88\n",
      "Valid MRI samples: 20/23\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:09<00:00, 13.95s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5704 | Train Acc: 0.6750\n",
      "Val Loss: 0.3387 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.5789\n",
      "Model saved! Best F1: 0.4737\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:58<00:00, 11.73s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3217 | Train Acc: 0.8875\n",
      "Val Loss: 0.4122 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:57<00:00, 11.54s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2310 | Train Acc: 0.8875\n",
      "Val Loss: 0.4031 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:54<00:00, 10.95s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2242 | Train Acc: 0.8875\n",
      "Val Loss: 0.4350 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.3158\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:06<00:00, 13.36s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1012 | Train Acc: 0.9500\n",
      "Val Loss: 0.3696 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.2105\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:04<00:00, 12.99s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0513 | Train Acc: 1.0000\n",
      "Val Loss: 0.4665 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.2632\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:59<00:00, 12.00s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0751 | Train Acc: 0.9750\n",
      "Val Loss: 0.4039 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.3684\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:03<00:00, 12.78s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0364 | Train Acc: 0.9875\n",
      "Val Loss: 0.4109 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:53<00:00, 10.65s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1197 | Train Acc: 0.9375\n",
      "Val Loss: 0.3926 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.3684\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:58<00:00, 11.62s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:05<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0440 | Train Acc: 0.9875\n",
      "Val Loss: 0.2515 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:59<00:00, 11.90s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0837 | Train Acc: 0.9625\n",
      "Val Loss: 0.1798 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:00<00:00, 12.14s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0908 | Train Acc: 0.9500\n",
      "Val Loss: 0.2016 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:05<00:00, 13.12s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0119 | Train Acc: 1.0000\n",
      "Val Loss: 0.2211 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:06<00:00, 13.31s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0141 | Train Acc: 1.0000\n",
      "Val Loss: 0.2024 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:07<00:00, 13.48s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0328 | Train Acc: 0.9875\n",
      "Val Loss: 0.1932 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:04<00:00, 12.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0142 | Train Acc: 1.0000\n",
      "Val Loss: 0.1884 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:05<00:00, 13.03s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0317 | Train Acc: 1.0000\n",
      "Val Loss: 0.1966 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 20 test samples\n",
      "Class distribution - True labels: Counter({0: 19, 1: 1})\n",
      "Class distribution - Predictions: Counter({0: 20})\n",
      "Probability range: [0.0000, 0.4614]\n",
      "Accuracy: 0.9500 | F1: 0.4872 | AUC: 0.3684\n",
      "============================================================\n",
      "\n",
      "\n",
      "MRI Model - Final Results:\n",
      "Accuracy: 0.9500\n",
      "F1-Score: 0.4872\n",
      "AUC: 0.3684\n",
      "\n",
      "============================================================\n",
      "Training HISTOLOGY Model\n",
      "============================================================\n",
      "Valid Histology samples: 91/91\n",
      "Valid Histology samples: 26/26\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:59<00:00,  9.84s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5709 | Train Acc: 0.6484\n",
      "Val Loss: 0.6725 | Val Acc: 0.5000 | Val F1: 0.3910 | Val AUC: 0.5625\n",
      "Model saved! Best F1: 0.3910\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:58<00:00,  9.77s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3245 | Train Acc: 0.8681\n",
      "Val Loss: 0.3282 | Val Acc: 0.8846 | Val F1: 0.6681 | Val AUC: 0.6250\n",
      "Model saved! Best F1: 0.6681\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:54<00:00,  9.15s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1504 | Train Acc: 0.9341\n",
      "Val Loss: 0.2102 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.6875\n",
      "Model saved! Best F1: 0.8231\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:48<00:00,  8.04s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0987 | Train Acc: 0.9451\n",
      "Val Loss: 0.1938 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.7917\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:55<00:00,  9.29s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0704 | Train Acc: 0.9780\n",
      "Val Loss: 0.1763 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.8958\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:57<00:00,  9.56s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1204 | Train Acc: 0.9670\n",
      "Val Loss: 0.1768 | Val Acc: 0.8462 | Val F1: 0.6232 | Val AUC: 0.9375\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:56<00:00,  9.46s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0192 | Train Acc: 1.0000\n",
      "Val Loss: 0.1215 | Val Acc: 0.9615 | Val F1: 0.8231 | Val AUC: 0.9375\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:59<00:00,  9.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0898 | Train Acc: 0.9780\n",
      "Val Loss: 0.3344 | Val Acc: 0.8462 | Val F1: 0.4583 | Val AUC: 0.7917\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:58<00:00,  9.68s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0322 | Train Acc: 0.9890\n",
      "Val Loss: 0.3031 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7083\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:55<00:00,  9.26s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0159 | Train Acc: 1.0000\n",
      "Val Loss: 0.3020 | Val Acc: 0.8846 | Val F1: 0.4694 | Val AUC: 0.6875\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:58<00:00,  9.80s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0819 | Train Acc: 0.9780\n",
      "Val Loss: 0.2669 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.6875\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:56<00:00,  9.35s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0615 | Train Acc: 0.9890\n",
      "Val Loss: 0.2682 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7708\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6/6 [00:56<00:00,  9.39s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0209 | Train Acc: 0.9890\n",
      "Val Loss: 0.2558 | Val Acc: 0.9231 | Val F1: 0.4800 | Val AUC: 0.7292\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 26 test samples\n",
      "Class distribution - True labels: Counter({0: 24, 1: 2})\n",
      "Class distribution - Predictions: Counter({0: 25, 1: 1})\n",
      "Probability range: [0.0029, 0.5110]\n",
      "Accuracy: 0.9615 | F1: 0.8231 | AUC: 0.6875\n",
      "============================================================\n",
      "\n",
      "\n",
      "HISTOLOGY Model - Final Results:\n",
      "Accuracy: 0.9615\n",
      "F1-Score: 0.8231\n",
      "AUC: 0.6875\n",
      "\n",
      "============================================================\n",
      "Training COMBINED Model\n",
      "============================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:59<00:00, 23.96s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:18<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6910 | Train Acc: 0.5375\n",
      "Val Loss: 0.6975 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.3684\n",
      "Model saved! Best F1: 0.0476\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [02:12<00:00, 26.42s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:18<00:00,  9.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6944 | Train Acc: 0.5250\n",
      "Val Loss: 0.6941 | Val Acc: 0.5000 | Val F1: 0.4048 | Val AUC: 0.4737\n",
      "Model saved! Best F1: 0.4048\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:59<00:00, 24.00s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:15<00:00,  7.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7012 | Train Acc: 0.5000\n",
      "Val Loss: 0.6913 | Val Acc: 0.5000 | Val F1: 0.4048 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:40<00:00, 20.19s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6770 | Train Acc: 0.5750\n",
      "Val Loss: 0.6897 | Val Acc: 0.4500 | Val F1: 0.3732 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:46<00:00, 21.35s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6906 | Train Acc: 0.5375\n",
      "Val Loss: 0.6721 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.7368\n",
      "Model saved! Best F1: 0.5312\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:42<00:00, 20.41s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6632 | Train Acc: 0.6375\n",
      "Val Loss: 0.6556 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:36<00:00, 19.32s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6548 | Train Acc: 0.6125\n",
      "Val Loss: 0.6393 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:36<00:00, 19.32s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6566 | Train Acc: 0.6500\n",
      "Val Loss: 0.6247 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:43<00:00, 20.63s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6950 | Train Acc: 0.5500\n",
      "Val Loss: 0.6137 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:38<00:00, 19.76s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6916 | Train Acc: 0.5500\n",
      "Val Loss: 0.6078 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:37<00:00, 19.59s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:16<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6722 | Train Acc: 0.5750\n",
      "Val Loss: 0.5662 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:38<00:00, 19.75s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6529 | Train Acc: 0.5875\n",
      "Val Loss: 0.5408 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:33<00:00, 18.62s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6657 | Train Acc: 0.5750\n",
      "Val Loss: 0.5476 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:40<00:00, 20.19s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6615 | Train Acc: 0.6250\n",
      "Val Loss: 0.5434 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:40<00:00, 20.04s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6436 | Train Acc: 0.6250\n",
      "Val Loss: 0.5484 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:48<00:00, 21.66s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:15<00:00,  7.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6140 | Train Acc: 0.6875\n",
      "Val Loss: 0.5430 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:36<00:00, 19.40s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6580 | Train Acc: 0.5875\n",
      "Val Loss: 0.5396 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:36<00:00, 19.27s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6224 | Train Acc: 0.7000\n",
      "Val Loss: 0.5125 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:36<00:00, 19.39s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5939 | Train Acc: 0.7375\n",
      "Val Loss: 0.5112 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [01:34<00:00, 18.89s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5912 | Train Acc: 0.6875\n",
      "Val Loss: 0.5130 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final evaluation on 20 test samples\n",
      "Class distribution - True labels: Counter({0: 19, 1: 1})\n",
      "Class distribution - Predictions: Counter({0: 13, 1: 7})\n",
      "Probability range: [0.4585, 0.5143]\n",
      "Accuracy: 0.7000 | F1: 0.5312 | AUC: 0.7368\n",
      "============================================================\n",
      "\n",
      "\n",
      "COMBINED Model - Final Results:\n",
      "Accuracy: 0.7000\n",
      "F1-Score: 0.5312\n",
      "AUC: 0.7368\n",
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON\n",
      "================================================================================\n",
      "    Model Accuracy Precision Recall F1-Score    AUC\n",
      "      MRI   0.9500    0.4750 0.5000   0.4872 0.3684\n",
      "HISTOLOGY   0.9615    0.9800 0.7500   0.8231 0.6875\n",
      " COMBINED   0.7000    0.5714 0.8421   0.5312 0.7368\n",
      "\n",
      "=== Statistical Analysis ===\n",
      "Combined vs MRI improvement: 9.05%\n",
      "Combined vs Histology improvement: -35.46%\n",
      "\n",
      "================================================================================\n",
      "All results saved to: ./egfr_results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EGFR Mutation Prediction: MRI vs Histology vs Combined Approach\n",
    "# Dataset: Brain Metastases with class imbalance 8.65:1\n",
    "# \n",
    "# Install required packages:\n",
    "# pip install nibabel torch torchvision opencv-python pillow pandas openpyxl scikit-learn matplotlib seaborn tqdm\n",
    "# pip install openslide-python (optional, for better SVS support)\n",
    "\n",
    "import os;os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np,pandas as pd,torch,torch.nn as nn,torch.nn.functional as F\n",
    "import torchvision.models as models,torchvision.transforms as transforms\n",
    "from PIL import Image;Image.MAX_IMAGE_PIXELS=None\n",
    "from pathlib import Path;from datetime import datetime;import cv2\n",
    "from scipy.spatial.distance import cosine,euclidean,pdist,squareform;from scipy import stats\n",
    "import matplotlib.pyplot as plt,seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import random,glob,json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:import nibabel as nib\n",
    "except:print(\"Warning: nibabel not installed. Install with: pip install nibabel\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);np.random.seed(seed);torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed);torch.backends.cudnn.deterministic=True\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    clinical_path=r\"D:\\paper\\external.xlsx\"\n",
    "    histology_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "    mri_base=r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_radiology_images\\Brain-Mets-Lung-MRI-Path-Segs\"\n",
    "    output_dir=\"./egfr_results\"\n",
    "    \n",
    "    # Training params\n",
    "    img_size=224;batch_size=16;epochs=50;lr=1e-4;patience=10\n",
    "    n_folds=5;device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    imbalance_ratio=8.65\n",
    "    \n",
    "cfg=Config()\n",
    "os.makedirs(cfg.output_dir,exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "\n",
    "# ==================== DATA LOADING ====================\n",
    "print(\"\\n=== Loading Clinical Data ===\")\n",
    "clinical_df=pd.read_excel(cfg.clinical_path) if cfg.clinical_path.endswith('.xlsx') else pd.read_csv(cfg.clinical_path)\n",
    "print(f\"Clinical data shape: {clinical_df.shape}\")\n",
    "print(f\"Columns: {clinical_df.columns.tolist()}\")\n",
    "\n",
    "# Find target column - handle different possible names\n",
    "target_col=None\n",
    "for col in clinical_df.columns:\n",
    "    if 'EGFR' in col.upper() and 'STATUS' in col.upper():\n",
    "        target_col=col\n",
    "        break\n",
    "if target_col is None:\n",
    "    print(\"ERROR: EGFR Status column not found!\")\n",
    "    print(f\"Available columns: {clinical_df.columns.tolist()}\")\n",
    "    raise ValueError(\"Target column 'EGFR Status' not found in dataset\")\n",
    "\n",
    "print(f\"Using target column: '{target_col}'\")\n",
    "print(f\"Target distribution:\\n{clinical_df[target_col].value_counts()}\")\n",
    "print(f\"Target encoding: 1=Mutation, 0=WT\")\n",
    "print(f\"Imbalance ratio: {clinical_df[target_col].value_counts()[0]/clinical_df[target_col].value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Get patient IDs and labels\n",
    "# Check if there's a 'patientID' column that might match better\n",
    "if 'patientID' in clinical_df.columns:\n",
    "    print(\"\\nTrying 'patientID' column for matching...\")\n",
    "    patient_ids=clinical_df['patientID'].astype(str).tolist()\n",
    "    print(f\"Sample patientID values: {patient_ids[:5]}\")\n",
    "else:\n",
    "    patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "    print(f\"Using first column for patient IDs: {patient_ids[:5]}\")\n",
    "\n",
    "labels=clinical_df[target_col].values\n",
    "\n",
    "# ==================== MRI DATASET ====================\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        all_dirs={d.name:d for d in self.base_path.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            patient_dir=None\n",
    "            \n",
    "            # Try direct match\n",
    "            if pid in all_dirs:\n",
    "                patient_dir=all_dirs[pid]\n",
    "            # Try without YG_ prefix if it exists\n",
    "            elif pid.startswith('YG_'):\n",
    "                pid_without_prefix=pid[3:]\n",
    "                if pid_without_prefix in all_dirs:\n",
    "                    patient_dir=all_dirs[pid_without_prefix]\n",
    "            # Try adding YG_ prefix if not present\n",
    "            elif f\"YG_{pid}\" in all_dirs:\n",
    "                patient_dir=all_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            if patient_dir and patient_dir.exists():\n",
    "                # Look for t1ce and flair files\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                \n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    valid.append((idx,patient_dir,t1ce_files[0],flair_files[0]))\n",
    "        \n",
    "        print(f\"Valid MRI samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        if len(valid)==0:\n",
    "            print(f\"Sample patient IDs: {self.patient_ids[:5]}\")\n",
    "            print(f\"Sample directories: {list(all_dirs.keys())[:5]}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,patient_dir,t1ce_path,flair_path=self.valid_data[idx]\n",
    "        pid=self.patient_ids[real_idx];label=self.labels[real_idx]\n",
    "        \n",
    "        # Load images\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        \n",
    "        # Stack as 3-channel (T1CE, FLAIR, Average)\n",
    "        avg=(t1ce_img+flair_img)/2\n",
    "        img=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        if self.transform:img=self.transform(Image.fromarray(img.astype(np.uint8)))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            # Handle compressed .nii files\n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                # It's a compressed archive, try to extract\n",
    "                import zipfile,gzip,tarfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:\n",
    "                                img_data=nib.load(f).get_fdata()\n",
    "                else:\n",
    "                    # Try loading directly\n",
    "                    img_data=nib.load(str(path)).get_fdata()\n",
    "            else:\n",
    "                img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            # Get middle slice\n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            # Normalize to 0-255\n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading NIfTI {path}: {e}\")\n",
    "            # Fallback: try as image\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:\n",
    "                # Return blank image as last resort\n",
    "                return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "\n",
    "# ==================== HISTOLOGY DATASET ====================\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,base_path,transform=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.base_path=Path(base_path);self.transform=transform\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[]\n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            svs_files=list(self.base_path.glob(f\"*{pid}*.svs\"))\n",
    "            if len(svs_files)==0:svs_files=list(self.base_path.glob(f\"YG_{pid}*.svs\"))\n",
    "            if len(svs_files)>0:valid.append((idx,svs_files[0]))\n",
    "        print(f\"Valid Histology samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,svs_path=self.valid_data[idx];label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        img=self._load_svs(svs_path)\n",
    "        if self.transform:img=self.transform(Image.fromarray(img))\n",
    "        return img,torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== COMBINED DATASET ====================\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self,patient_ids,labels,mri_base,hist_base,transform=None,hist_ids=None):\n",
    "        self.patient_ids=patient_ids;self.labels=labels;self.mri_base=Path(mri_base)\n",
    "        self.hist_base=Path(hist_base);self.transform=transform\n",
    "        self.hist_ids=hist_ids if hist_ids else patient_ids  # Use separate hist IDs if provided\n",
    "        self.valid_data=self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid=[];all_mri_dirs={d.name:d for d in self.mri_base.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx,pid in enumerate(self.patient_ids):\n",
    "            hist_pid=self.hist_ids[idx]\n",
    "            \n",
    "            # Check MRI using patientID\n",
    "            patient_dir=None\n",
    "            if pid in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid]\n",
    "            elif pid.startswith('YG_') and pid[3:] in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[pid[3:]]\n",
    "            elif f\"YG_{pid}\" in all_mri_dirs:\n",
    "                patient_dir=all_mri_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            has_mri=False;t1ce_file=None;flair_file=None\n",
    "            if patient_dir and patient_dir.exists():\n",
    "                t1ce_files=list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files=list(patient_dir.glob(\"*flair*\"))\n",
    "                if len(t1ce_files)>0 and len(flair_files)>0:\n",
    "                    has_mri=True;t1ce_file=t1ce_files[0];flair_file=flair_files[0]\n",
    "            \n",
    "            # Check Histology using Accession ID\n",
    "            svs_files=list(self.hist_base.glob(f\"*{hist_pid}*.svs\"))\n",
    "            if len(svs_files)==0 and hist_pid.startswith('YG_'):\n",
    "                svs_files=list(self.hist_base.glob(f\"*{hist_pid[3:]}*.svs\"))\n",
    "            has_hist=len(svs_files)>0\n",
    "            \n",
    "            if has_mri and has_hist:valid.append((idx,patient_dir,t1ce_file,flair_file,svs_files[0]))\n",
    "        \n",
    "        print(f\"Valid Combined samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        real_idx,mri_dir,t1ce_path,flair_path,svs_path=self.valid_data[idx]\n",
    "        label=self.labels[real_idx];pid=self.patient_ids[real_idx]\n",
    "        \n",
    "        # Load MRI\n",
    "        t1ce_img=self._load_nifti(t1ce_path);flair_img=self._load_nifti(flair_path)\n",
    "        avg=(t1ce_img+flair_img)/2;mri=np.stack([t1ce_img,flair_img,avg],axis=-1)\n",
    "        \n",
    "        # Load Histology\n",
    "        hist=self._load_svs(svs_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            mri=self.transform(Image.fromarray(mri.astype(np.uint8)))\n",
    "            hist=self.transform(Image.fromarray(hist))\n",
    "        \n",
    "        return {'mri':mri,'hist':hist},torch.tensor(label,dtype=torch.long),pid\n",
    "    \n",
    "    def _load_nifti(self,path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path=Path(path)  # Ensure it's a Path object\n",
    "            \n",
    "            if path.suffix=='' and not str(path).endswith('.nii'):\n",
    "                import zipfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files=[f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:img_data=nib.load(f).get_fdata()\n",
    "                else:img_data=nib.load(str(path)).get_fdata()\n",
    "            else:img_data=nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            if len(img_data.shape)==3:img_slice=img_data[:,:,img_data.shape[2]//2]\n",
    "            else:img_slice=img_data[:,:,0,0] if len(img_data.shape)==4 else img_data\n",
    "            \n",
    "            img_slice=(img_slice-img_slice.min())/(img_slice.max()-img_slice.min()+1e-8)*255\n",
    "            return cv2.resize(img_slice.astype(np.uint8),(cfg.img_size,cfg.img_size))\n",
    "        except:\n",
    "            try:\n",
    "                img=cv2.imread(str(path),cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:img=np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img,(cfg.img_size,cfg.img_size))\n",
    "            except:return np.zeros((cfg.img_size,cfg.img_size),dtype=np.uint8)\n",
    "    \n",
    "    def _load_svs(self,path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide=OpenSlide(str(path));level=slide.level_count-1\n",
    "            thumb=slide.read_region((0,0),level,slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size,cfg.img_size));return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img=Image.open(path);img.thumbnail((cfg.img_size,cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== MODELS ====================\n",
    "class MRIModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class HistologyModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone=models.resnet50(pretrained=True)\n",
    "        self.backbone.fc=nn.Linear(2048,num_classes)\n",
    "    def forward(self,x):return self.backbone(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super().__init__()\n",
    "        # Separate backbones for each modality\n",
    "        self.mri_backbone=models.resnet50(pretrained=True)\n",
    "        self.hist_backbone=models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Extract features before final FC layer\n",
    "        self.mri_backbone.fc=nn.Identity()\n",
    "        self.hist_backbone.fc=nn.Identity()\n",
    "        \n",
    "        # Attention-based fusion mechanism\n",
    "        self.mri_attention=nn.Sequential(nn.Linear(2048,512),nn.ReLU(),nn.Linear(512,1),nn.Sigmoid())\n",
    "        self.hist_attention=nn.Sequential(nn.Linear(2048,512),nn.ReLU(),nn.Linear(512,1),nn.Sigmoid())\n",
    "        \n",
    "        # Feature transformation before fusion\n",
    "        self.mri_transform=nn.Sequential(nn.Linear(2048,1024),nn.ReLU(),nn.Dropout(0.3),nn.BatchNorm1d(1024))\n",
    "        self.hist_transform=nn.Sequential(nn.Linear(2048,1024),nn.ReLU(),nn.Dropout(0.3),nn.BatchNorm1d(1024))\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        self.fusion=nn.Sequential(\n",
    "            nn.Linear(2048,1024),nn.ReLU(),nn.Dropout(0.5),nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024,512),nn.ReLU(),nn.Dropout(0.4),nn.BatchNorm1d(512),\n",
    "            nn.Linear(512,256),nn.ReLU(),nn.Dropout(0.3),\n",
    "            nn.Linear(256,num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Extract features\n",
    "        mri_feat=self.mri_backbone(x['mri'])\n",
    "        hist_feat=self.hist_backbone(x['hist'])\n",
    "        \n",
    "        # Compute attention weights\n",
    "        mri_weight=self.mri_attention(mri_feat)\n",
    "        hist_weight=self.hist_attention(hist_feat)\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight=mri_weight+hist_weight+1e-8\n",
    "        mri_weight=mri_weight/total_weight\n",
    "        hist_weight=hist_weight/total_weight\n",
    "        \n",
    "        # Transform and weight features\n",
    "        mri_transformed=self.mri_transform(mri_feat)*mri_weight\n",
    "        hist_transformed=self.hist_transform(hist_feat)*hist_weight\n",
    "        \n",
    "        # Concatenate weighted features\n",
    "        combined=torch.cat([mri_transformed,hist_transformed],dim=1)\n",
    "        \n",
    "        return self.fusion(combined)\n",
    "\n",
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "def get_weighted_sampler(labels):\n",
    "    class_counts=Counter(labels);weights=[1.0/class_counts[l] for l in labels]\n",
    "    return WeightedRandomSampler(weights,len(weights),replacement=True)\n",
    "\n",
    "def train_epoch(model,loader,criterion,optimizer,device):\n",
    "    model.train();total_loss=0;all_preds=[];all_labels=[]\n",
    "    for batch in tqdm(loader,desc=\"Training\"):\n",
    "        if len(batch)==3:\n",
    "            if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "            else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "        else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad();outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "        loss.backward();optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item();preds=outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds)\n",
    "    return total_loss/len(loader),acc\n",
    "\n",
    "def validate(model,loader,criterion,device):\n",
    "    model.eval();total_loss=0;all_preds=[];all_labels=[];all_probs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader,desc=\"Validation\"):\n",
    "            if len(batch)==3:\n",
    "                if isinstance(batch[0],dict):imgs,labels,_=batch;imgs={k:v.to(device) for k,v in imgs.items()}\n",
    "                else:imgs,labels,_=batch;imgs=imgs.to(device)\n",
    "            else:imgs,labels=batch;imgs=imgs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            outputs=model(imgs);loss=criterion(outputs,labels)\n",
    "            total_loss+=loss.item();probs=F.softmax(outputs,dim=1);preds=outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:,1].cpu().numpy())\n",
    "    \n",
    "    acc=accuracy_score(all_labels,all_preds);f1=f1_score(all_labels,all_preds,average='macro')\n",
    "    \n",
    "    # Calculate AUC with validation\n",
    "    try:\n",
    "        if len(set(all_labels))>1:  # Need both classes for AUC\n",
    "            auc_score=roc_auc_score(all_labels,all_probs)\n",
    "        else:\n",
    "            print(f\"Warning: Only one class in validation set, AUC set to 0\")\n",
    "            auc_score=0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: AUC calculation failed ({e}), setting to 0\")\n",
    "        auc_score=0.0\n",
    "    \n",
    "    return total_loss/len(loader),acc,f1,auc_score,all_preds,all_labels,all_probs\n",
    "\n",
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "def train_model(model_type='mri'):\n",
    "    print(f\"\\n{'='*60}\\nTraining {model_type.upper()} Model\\n{'='*60}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    train_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    val_transform=transforms.Compose([transforms.Resize((cfg.img_size,cfg.img_size)),\n",
    "        transforms.ToTensor(),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "    \n",
    "    # Split data\n",
    "    train_ids,test_ids,train_labels,test_labels=train_test_split(\n",
    "        patient_ids,labels,test_size=0.2,random_state=42,stratify=labels)\n",
    "    \n",
    "    # For histology, we need to use Accession IDs (first column) since SVS files use those\n",
    "    if model_type=='histology':\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        # Create mapping to keep indices aligned\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        hist_train_ids=[hist_patient_ids[i] for i in train_idx]\n",
    "        hist_test_ids=[hist_patient_ids[i] for i in test_idx]\n",
    "        hist_train_labels=[labels[i] for i in train_idx]\n",
    "        hist_test_labels=[labels[i] for i in test_idx]\n",
    "        \n",
    "        train_dataset=HistologyDataset(hist_train_ids,hist_train_labels,cfg.histology_base,train_transform)\n",
    "        test_dataset=HistologyDataset(hist_test_ids,hist_test_labels,cfg.histology_base,val_transform)\n",
    "        model=HistologyModel().to(cfg.device)\n",
    "    elif model_type=='mri':\n",
    "        train_dataset=MRIDataset(train_ids,train_labels,cfg.mri_base,train_transform)\n",
    "        test_dataset=MRIDataset(test_ids,test_labels,cfg.mri_base,val_transform)\n",
    "        model=MRIModel().to(cfg.device)\n",
    "    else:  # combined\n",
    "        # For combined, we need both ID types\n",
    "        hist_patient_ids=clinical_df.iloc[:,0].astype(str).tolist()\n",
    "        train_idx=[i for i,pid in enumerate(patient_ids) if pid in train_ids]\n",
    "        test_idx=[i for i,pid in enumerate(patient_ids) if pid in test_ids]\n",
    "        \n",
    "        train_dataset=CombinedDataset(train_ids,train_labels,cfg.mri_base,cfg.histology_base,train_transform,\n",
    "                                      hist_ids=[hist_patient_ids[i] for i in train_idx])\n",
    "        test_dataset=CombinedDataset(test_ids,test_labels,cfg.mri_base,cfg.histology_base,val_transform,\n",
    "                                     hist_ids=[hist_patient_ids[i] for i in test_idx])\n",
    "        model=CombinedModel().to(cfg.device)\n",
    "    \n",
    "    # Get train labels for weighted sampling\n",
    "    if hasattr(train_dataset,'valid_data'):\n",
    "        train_dataset_labels=[train_dataset.labels[train_dataset.valid_data[i][0]] for i in range(len(train_dataset))]\n",
    "    else:\n",
    "        train_dataset_labels=[train_dataset.labels[i] for i in range(len(train_dataset))]\n",
    "    sampler=get_weighted_sampler(train_dataset_labels)\n",
    "    \n",
    "    train_loader=DataLoader(train_dataset,batch_size=cfg.batch_size,sampler=sampler,num_workers=0)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=cfg.batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use different learning rates for combined model\n",
    "    if model_type=='combined':\n",
    "        # Lower LR for pretrained backbones, higher for fusion layers\n",
    "        optimizer=torch.optim.Adam([\n",
    "            {'params':model.mri_backbone.parameters(),'lr':cfg.lr*0.1},\n",
    "            {'params':model.hist_backbone.parameters(),'lr':cfg.lr*0.1},\n",
    "            {'params':model.mri_attention.parameters(),'lr':cfg.lr},\n",
    "            {'params':model.hist_attention.parameters(),'lr':cfg.lr},\n",
    "            {'params':model.mri_transform.parameters(),'lr':cfg.lr},\n",
    "            {'params':model.hist_transform.parameters(),'lr':cfg.lr},\n",
    "            {'params':model.fusion.parameters(),'lr':cfg.lr}\n",
    "        ])\n",
    "        patience=15  # More patience for combined model\n",
    "        # Use cosine annealing for combined model\n",
    "        scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10,T_mult=2)\n",
    "        use_cosine=True\n",
    "    else:\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=cfg.lr)\n",
    "        patience=cfg.patience\n",
    "        scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'max',patience=5)\n",
    "        use_cosine=False\n",
    "    \n",
    "    best_f1=0;patience_counter=0;history={'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[],'val_f1':[],'val_auc':[]}\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        train_loss,train_acc=train_epoch(model,train_loader,criterion,optimizer,cfg.device)\n",
    "        val_loss,val_acc,val_f1,val_auc,_,_,_=validate(model,test_loader,criterion,cfg.device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss);history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss);history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1);history['val_auc'].append(val_auc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Update scheduler\n",
    "        if use_cosine:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_f1)\n",
    "        \n",
    "        if val_f1>best_f1:\n",
    "            best_f1=val_f1;patience_counter=0\n",
    "            torch.save(model.state_dict(),f\"{cfg.output_dir}/{model_type}_best_model.pth\")\n",
    "            print(f\"Model saved! Best F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter+=1\n",
    "            if patience_counter>=patience:print(\"Early stopping\");break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load(f\"{cfg.output_dir}/{model_type}_best_model.pth\"))\n",
    "    _,final_acc,final_f1,final_auc,preds,true_labels,probs=validate(model,test_loader,criterion,cfg.device)\n",
    "    \n",
    "    # Double check we have real predictions\n",
    "    assert len(preds)==len(true_labels), \"Prediction count mismatch!\"\n",
    "    assert len(probs)==len(true_labels), \"Probability count mismatch!\"\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final evaluation on {len(preds)} test samples\")\n",
    "    print(f\"Class distribution - True labels: {Counter(true_labels)}\")\n",
    "    print(f\"Class distribution - Predictions: {Counter(preds)}\")\n",
    "    print(f\"Probability range: [{min(probs):.4f}, {max(probs):.4f}]\")\n",
    "    print(f\"Accuracy: {final_acc:.4f} | F1: {final_f1:.4f} | AUC: {final_auc:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    results={'model_type':model_type,'accuracy':float(final_acc),'f1_score':float(final_f1),'auc':float(final_auc),\n",
    "             'predictions':[int(p) for p in preds],'true_labels':[int(t) for t in true_labels],\n",
    "             'probabilities':[float(p) for p in probs],'history':history}\n",
    "    \n",
    "    # Save results with actual predictions\n",
    "    with open(f\"{cfg.output_dir}/{model_type}_results.json\",'w') as f:\n",
    "        json.dump({k:v if k!='history' else {hk:[float(hv) for hv in hvs] for hk,hvs in v.items()} \n",
    "                   for k,v in results.items()},f,indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== EVALUATION & VISUALIZATION ====================\n",
    "def plot_confusion_matrix(y_true,y_pred,title,save_path):\n",
    "    \"\"\"Plot confusion matrix using actual predictions only\"\"\"\n",
    "    assert len(y_true)==len(y_pred), f\"Data mismatch: {len(y_true)} vs {len(y_pred)}\"\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(8,6));sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',cbar=True)\n",
    "    plt.title(title,fontsize=16);plt.ylabel('True Label');plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_roc_curves(results_dict,save_path):\n",
    "    \"\"\"Plot ROC curves using actual model probabilities only\"\"\"\n",
    "    plt.figure(figsize=(10,8))\n",
    "    for model_type,results in results_dict.items():\n",
    "        assert len(results['true_labels'])==len(results['probabilities']), \\\n",
    "            f\"{model_type}: Label/prob mismatch!\"\n",
    "        fpr,tpr,_=roc_curve(results['true_labels'],results['probabilities'])\n",
    "        auc_score=auc(fpr,tpr)\n",
    "        plt.plot(fpr,tpr,label=f\"{model_type.upper()} (AUC={auc_score:.3f})\",linewidth=2)\n",
    "    plt.plot([0,1],[0,1],'k--',label='Random');plt.xlabel('False Positive Rate',fontsize=12)\n",
    "    plt.ylabel('True Positive Rate',fontsize=12);plt.title('ROC Curves Comparison',fontsize=16)\n",
    "    plt.legend(fontsize=10);plt.grid(alpha=0.3);plt.tight_layout()\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def plot_training_history(history,model_type,save_path):\n",
    "    \"\"\"Plot training history from actual training runs only\"\"\"\n",
    "    assert len(history['train_loss'])>0, \"No training history!\"\n",
    "    fig,axes=plt.subplots(2,2,figsize=(15,10))\n",
    "    epochs=range(1,len(history['train_loss'])+1)\n",
    "    \n",
    "    axes[0,0].plot(epochs,history['train_loss'],label='Train');axes[0,0].plot(epochs,history['val_loss'],label='Val')\n",
    "    axes[0,0].set_title(f'{model_type.upper()} - Loss');axes[0,0].set_xlabel('Epoch');axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(epochs,history['train_acc'],label='Train');axes[0,1].plot(epochs,history['val_acc'],label='Val')\n",
    "    axes[0,1].set_title(f'{model_type.upper()} - Accuracy');axes[0,1].set_xlabel('Epoch');axes[0,1].legend()\n",
    "    \n",
    "    axes[1,0].plot(epochs,history['val_f1'],label='Val F1',color='green')\n",
    "    axes[1,0].set_title(f'{model_type.upper()} - F1 Score');axes[1,0].set_xlabel('Epoch');axes[1,0].legend()\n",
    "    \n",
    "    axes[1,1].plot(epochs,history['val_auc'],label='Val AUC',color='red')\n",
    "    axes[1,1].set_title(f'{model_type.upper()} - AUC');axes[1,1].set_xlabel('Epoch');axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "def create_comparison_table(results_dict,save_path):\n",
    "    \"\"\"Create comparison table using actual model results only\"\"\"\n",
    "    data=[]\n",
    "    for model_type,results in results_dict.items():\n",
    "        y_true,y_pred=results['true_labels'],results['predictions']\n",
    "        assert len(y_true)==len(y_pred), f\"{model_type}: Data length mismatch!\"\n",
    "        precision=precision_score(y_true,y_pred,average='macro')\n",
    "        recall=recall_score(y_true,y_pred,average='macro')\n",
    "        data.append({'Model':model_type.upper(),'Accuracy':f\"{results['accuracy']:.4f}\",\n",
    "                    'Precision':f\"{precision:.4f}\",'Recall':f\"{recall:.4f}\",\n",
    "                    'F1-Score':f\"{results['f1_score']:.4f}\",'AUC':f\"{results['auc']:.4f}\"})\n",
    "    \n",
    "    df=pd.DataFrame(data)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(12,3));ax.axis('tight');ax.axis('off')\n",
    "    table=ax.table(cellText=df.values,colLabels=df.columns,cellLoc='center',loc='center',\n",
    "                   colColours=['#40466e']*len(df.columns))\n",
    "    table.auto_set_font_size(False);table.set_fontsize(10);table.scale(1,2)\n",
    "    \n",
    "    for i in range(len(df.columns)):table[(0,i)].set_facecolor('#40466e');table[(0,i)].set_text_props(weight='bold',color='white')\n",
    "    \n",
    "    plt.title('Model Comparison Table',fontsize=16,pad=20)\n",
    "    plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "    \n",
    "    df.to_csv(save_path.replace('.png','.csv'),index=False)\n",
    "    return df\n",
    "\n",
    "def plot_metrics_comparison(results_dict,save_path):\n",
    "    \"\"\"Plot metrics comparison using actual results only\"\"\"\n",
    "    metrics=['accuracy','f1_score','auc']\n",
    "    data={m:[] for m in metrics};models=[]\n",
    "    \n",
    "    for model_type,results in results_dict.items():\n",
    "        models.append(model_type.upper())\n",
    "        for m in metrics:\n",
    "            assert m in results, f\"{model_type} missing metric {m}!\"\n",
    "            data[m].append(results[m])\n",
    "    \n",
    "    x=np.arange(len(models));width=0.25\n",
    "    fig,ax=plt.subplots(figsize=(12,7))\n",
    "    \n",
    "    colors=['#3498db','#e74c3c','#2ecc71']\n",
    "    for i,m in enumerate(metrics):\n",
    "        ax.bar(x+i*width,data[m],width,label=m.replace('_',' ').title(),color=colors[i])\n",
    "        for j,v in enumerate(data[m]):ax.text(j+i*width,v+0.01,f'{v:.3f}',ha='center',va='bottom',fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Models',fontsize=12);ax.set_ylabel('Score',fontsize=12)\n",
    "    ax.set_title('Performance Metrics Comparison',fontsize=16)\n",
    "    ax.set_xticks(x+width);ax.set_xticklabels(models);ax.legend();ax.grid(axis='y',alpha=0.3)\n",
    "    plt.tight_layout();plt.savefig(save_path,dpi=300,bbox_inches='tight');plt.close()\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__==\"__main__\":\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"EGFR MUTATION PREDICTION - MULTI-MODAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train all models\n",
    "    results={}\n",
    "    for model_type in ['mri','histology','combined']:\n",
    "        try:\n",
    "            results[model_type]=train_model(model_type)\n",
    "            print(f\"\\n{model_type.upper()} Model - Final Results:\")\n",
    "            print(f\"Accuracy: {results[model_type]['accuracy']:.4f}\")\n",
    "            print(f\"F1-Score: {results[model_type]['f1_score']:.4f}\")\n",
    "            print(f\"AUC: {results[model_type]['auc']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_type} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_type,res in results.items():\n",
    "        # Confusion Matrix\n",
    "        plot_confusion_matrix(res['true_labels'],res['predictions'],\n",
    "                            f'{model_type.upper()} - Confusion Matrix',\n",
    "                            f\"{cfg.output_dir}/{model_type}_confusion_matrix.png\")\n",
    "        \n",
    "        # Training History\n",
    "        plot_training_history(res['history'],model_type,\n",
    "                            f\"{cfg.output_dir}/{model_type}_training_history.png\")\n",
    "        \n",
    "        # Classification Report\n",
    "        report=classification_report(res['true_labels'],res['predictions'],\n",
    "                                    target_names=['WT','Mutation'],output_dict=True)\n",
    "        report_df=pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(f\"{cfg.output_dir}/{model_type}_classification_report.csv\")\n",
    "    \n",
    "    # Comparison visualizations\n",
    "    if len(results)>0:\n",
    "        plot_roc_curves(results,f\"{cfg.output_dir}/roc_curves_comparison.png\")\n",
    "        comparison_df=create_comparison_table(results,f\"{cfg.output_dir}/comparison_table.png\")\n",
    "        plot_metrics_comparison(results,f\"{cfg.output_dir}/metrics_comparison.png\")\n",
    "        \n",
    "        print(\"\\n\"+\"=\"*80)\n",
    "        print(\"FINAL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Statistical significance test\n",
    "        if len(results)==3:\n",
    "            print(\"\\n=== Statistical Analysis ===\")\n",
    "            combined_f1=results['combined']['f1_score']\n",
    "            mri_f1=results['mri']['f1_score']\n",
    "            hist_f1=results['histology']['f1_score']\n",
    "            \n",
    "            print(f\"Combined vs MRI improvement: {((combined_f1-mri_f1)/mri_f1)*100:.2f}%\")\n",
    "            print(f\"Combined vs Histology improvement: {((combined_f1-hist_f1)/hist_f1)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All results saved to: {cfg.output_dir}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f642b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== Loading Clinical Data ===\n",
      "Clinical data shape: (111, 36)\n",
      "Using target column: 'EGFR Status (1 = Mutation; 0 = WT)'\n",
      "Target distribution:\n",
      "EGFR Status (1 = Mutation; 0 = WT)\n",
      "0    99\n",
      "1    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "IMPROVED HYBRID MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training IMPROVED COMBINED Model with CROSS_ATTENTION fusion\n",
      "================================================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\Shahinur/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20.5M/20.5M [00:01<00:00, 20.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 5/5 [01:25<00:00, 17.09s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0738 | Train Acc: 0.4250\n",
      "Val Loss: 0.0313 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [01:08<00:00, 13.64s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0744 | Train Acc: 0.4250\n",
      "Val Loss: 0.0300 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [01:03<00:00, 12.64s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0726 | Train Acc: 0.4125\n",
      "Val Loss: 0.0280 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [01:09<00:00, 13.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0755 | Train Acc: 0.5125\n",
      "Val Loss: 0.0267 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5/5 [01:04<00:00, 12.89s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0652 | Train Acc: 0.5250\n",
      "Val Loss: 0.0259 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5/5 [01:08<00:00, 13.76s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0542 | Train Acc: 0.5625\n",
      "Val Loss: 0.0252 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5/5 [01:04<00:00, 12.99s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0652 | Train Acc: 0.5500\n",
      "Val Loss: 0.0293 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 8/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5/5 [01:02<00:00, 12.42s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0613 | Train Acc: 0.5500\n",
      "Val Loss: 0.0291 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 9/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5/5 [01:05<00:00, 13.06s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0623 | Train Acc: 0.5500\n",
      "Val Loss: 0.0286 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 10/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5/5 [01:07<00:00, 13.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0732 | Train Acc: 0.4500\n",
      "Val Loss: 0.0278 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 11/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5/5 [01:07<00:00, 13.55s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0639 | Train Acc: 0.4625\n",
      "Val Loss: 0.0280 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.2632\n",
      "\n",
      "Epoch 12/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 5/5 [01:06<00:00, 13.35s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0564 | Train Acc: 0.5000\n",
      "Val Loss: 0.0280 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.3684\n",
      "\n",
      "Epoch 13/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 5/5 [01:03<00:00, 12.77s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0553 | Train Acc: 0.5375\n",
      "Val Loss: 0.0302 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 14/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 5/5 [01:07<00:00, 13.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0500 | Train Acc: 0.5625\n",
      "Val Loss: 0.0307 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.3684\n",
      "\n",
      "Epoch 15/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 5/5 [01:07<00:00, 13.48s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0493 | Train Acc: 0.5625\n",
      "Val Loss: 0.0310 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 16/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 5/5 [01:04<00:00, 12.88s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0588 | Train Acc: 0.5000\n",
      "Val Loss: 0.0298 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.3684\n",
      "\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 5/5 [01:01<00:00, 12.23s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0442 | Train Acc: 0.6125\n",
      "Val Loss: 0.0300 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 18/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 5/5 [01:04<00:00, 12.93s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0589 | Train Acc: 0.5125\n",
      "Val Loss: 0.0299 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 19/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 5/5 [01:03<00:00, 12.64s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0627 | Train Acc: 0.5000\n",
      "Val Loss: 0.0321 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 20/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 5/5 [01:02<00:00, 12.59s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0515 | Train Acc: 0.5375\n",
      "Val Loss: 0.0327 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.4737\n",
      "\n",
      "Epoch 21/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 5/5 [01:07<00:00, 13.47s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0533 | Train Acc: 0.5375\n",
      "Val Loss: 0.0319 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.4737\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Final Results - CROSS_ATTENTION\n",
      "Accuracy: 0.9500 | F1: 0.4872 | AUC: 0.7368\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training IMPROVED COMBINED Model with GATED fusion\n",
      "================================================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 5/5 [01:06<00:00, 13.21s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0536 | Train Acc: 0.5250\n",
      "Val Loss: 0.0425 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.4211\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [01:06<00:00, 13.40s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0496 | Train Acc: 0.5500\n",
      "Val Loss: 0.0457 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.0000\n",
      "\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [01:05<00:00, 13.14s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0572 | Train Acc: 0.5000\n",
      "Val Loss: 0.0495 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.1053\n",
      "\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [01:06<00:00, 13.32s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0506 | Train Acc: 0.6125\n",
      "Val Loss: 0.0514 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.4211\n",
      "\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5/5 [01:06<00:00, 13.20s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0589 | Train Acc: 0.5500\n",
      "Val Loss: 0.0543 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5/5 [01:10<00:00, 14.12s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0502 | Train Acc: 0.5875\n",
      "Val Loss: 0.0580 | Val Acc: 0.0500 | Val F1: 0.0476 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5/5 [01:04<00:00, 12.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0594 | Train Acc: 0.4875\n",
      "Val Loss: 0.0549 | Val Acc: 0.1000 | Val F1: 0.1000 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 8/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5/5 [01:06<00:00, 13.31s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0588 | Train Acc: 0.5000\n",
      "Val Loss: 0.0557 | Val Acc: 0.1000 | Val F1: 0.1000 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 9/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5/5 [01:02<00:00, 12.51s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0460 | Train Acc: 0.5875\n",
      "Val Loss: 0.0564 | Val Acc: 0.1500 | Val F1: 0.1479 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 10/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5/5 [01:02<00:00, 12.48s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0564 | Train Acc: 0.4875\n",
      "Val Loss: 0.0543 | Val Acc: 0.2000 | Val F1: 0.1919 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 11/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5/5 [01:02<00:00, 12.44s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0595 | Train Acc: 0.5500\n",
      "Val Loss: 0.0519 | Val Acc: 0.2000 | Val F1: 0.1919 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 12/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 5/5 [01:09<00:00, 13.82s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0536 | Train Acc: 0.5125\n",
      "Val Loss: 0.0501 | Val Acc: 0.3000 | Val F1: 0.2708 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 13/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 5/5 [01:02<00:00, 12.53s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0547 | Train Acc: 0.5750\n",
      "Val Loss: 0.0498 | Val Acc: 0.3500 | Val F1: 0.3067 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 14/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 5/5 [01:02<00:00, 12.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0507 | Train Acc: 0.4750\n",
      "Val Loss: 0.0533 | Val Acc: 0.2500 | Val F1: 0.2327 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 15/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 5/5 [01:18<00:00, 15.64s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0480 | Train Acc: 0.5500\n",
      "Val Loss: 0.0513 | Val Acc: 0.3500 | Val F1: 0.3067 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 16/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 5/5 [01:14<00:00, 14.88s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0575 | Train Acc: 0.4625\n",
      "Val Loss: 0.0494 | Val Acc: 0.4000 | Val F1: 0.3407 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 5/5 [01:14<00:00, 14.89s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0461 | Train Acc: 0.5750\n",
      "Val Loss: 0.0480 | Val Acc: 0.4000 | Val F1: 0.3407 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 18/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 5/5 [01:10<00:00, 14.08s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0487 | Train Acc: 0.5625\n",
      "Val Loss: 0.0475 | Val Acc: 0.4000 | Val F1: 0.3407 | Val AUC: 1.0000\n",
      "\n",
      "Epoch 19/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 5/5 [01:08<00:00, 13.73s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0510 | Train Acc: 0.5500\n",
      "Val Loss: 0.0453 | Val Acc: 0.5000 | Val F1: 0.4048 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 20/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 5/5 [01:11<00:00, 14.33s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0559 | Train Acc: 0.5375\n",
      "Val Loss: 0.0497 | Val Acc: 0.4500 | Val F1: 0.3732 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 21/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 5/5 [01:08<00:00, 13.71s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0585 | Train Acc: 0.4750\n",
      "Val Loss: 0.0507 | Val Acc: 0.4000 | Val F1: 0.3407 | Val AUC: 0.9474\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Final Results - GATED\n",
      "Accuracy: 0.9500 | F1: 0.4872 | AUC: 0.4211\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training IMPROVED COMBINED Model with CONCAT fusion\n",
      "================================================================================\n",
      "Valid Combined samples: 80/88\n",
      "Valid Combined samples: 20/23\n",
      "\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 5/5 [01:09<00:00, 13.83s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0763 | Train Acc: 0.4125\n",
      "Val Loss: 0.0435 | Val Acc: 0.3000 | Val F1: 0.2708 | Val AUC: 0.4211\n",
      "Model saved! Best F1: 0.2708\n",
      "\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 5/5 [01:07<00:00, 13.41s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0735 | Train Acc: 0.3750\n",
      "Val Loss: 0.0411 | Val Acc: 0.9500 | Val F1: 0.4872 | Val AUC: 0.8421\n",
      "Model saved! Best F1: 0.4872\n",
      "\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 5/5 [01:13<00:00, 14.61s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0506 | Train Acc: 0.5750\n",
      "Val Loss: 0.0404 | Val Acc: 0.9000 | Val F1: 0.4737 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 5/5 [01:11<00:00, 14.25s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0603 | Train Acc: 0.5250\n",
      "Val Loss: 0.0399 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.5789\n",
      "\n",
      "Epoch 5/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 5/5 [01:13<00:00, 14.65s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0507 | Train Acc: 0.5750\n",
      "Val Loss: 0.0390 | Val Acc: 0.8500 | Val F1: 0.4595 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 5/5 [01:18<00:00, 15.74s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0463 | Train Acc: 0.5625\n",
      "Val Loss: 0.0394 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 5/5 [01:12<00:00, 14.58s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0608 | Train Acc: 0.5250\n",
      "Val Loss: 0.0398 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.7895\n",
      "Model saved! Best F1: 0.6078\n",
      "\n",
      "Epoch 8/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 5/5 [01:22<00:00, 16.47s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0619 | Train Acc: 0.5125\n",
      "Val Loss: 0.0367 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 9/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 5/5 [02:27<00:00, 29.54s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0622 | Train Acc: 0.4875\n",
      "Val Loss: 0.0391 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 10/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 5/5 [01:52<00:00, 22.54s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:18<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0578 | Train Acc: 0.5375\n",
      "Val Loss: 0.0387 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 11/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 5/5 [03:15<00:00, 39.04s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0571 | Train Acc: 0.5375\n",
      "Val Loss: 0.0380 | Val Acc: 0.6500 | Val F1: 0.4982 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 12/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 5/5 [01:17<00:00, 15.41s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0650 | Train Acc: 0.5000\n",
      "Val Loss: 0.0357 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 13/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 5/5 [01:11<00:00, 14.34s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0558 | Train Acc: 0.5000\n",
      "Val Loss: 0.0372 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 14/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 5/5 [01:35<00:00, 19.01s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0668 | Train Acc: 0.5625\n",
      "Val Loss: 0.0361 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 15/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 5/5 [01:14<00:00, 14.99s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0535 | Train Acc: 0.5875\n",
      "Val Loss: 0.0363 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 16/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 5/5 [01:21<00:00, 16.25s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0561 | Train Acc: 0.5625\n",
      "Val Loss: 0.0378 | Val Acc: 0.6500 | Val F1: 0.4982 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 5/5 [02:03<00:00, 24.70s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:15<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0567 | Train Acc: 0.4500\n",
      "Val Loss: 0.0348 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 18/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 5/5 [01:34<00:00, 18.85s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0681 | Train Acc: 0.4000\n",
      "Val Loss: 0.0364 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 19/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 5/5 [01:52<00:00, 22.42s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:41<00:00, 20.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0531 | Train Acc: 0.6000\n",
      "Val Loss: 0.0366 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 20/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 5/5 [01:33<00:00, 18.71s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0497 | Train Acc: 0.5125\n",
      "Val Loss: 0.0349 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 21/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 5/5 [01:26<00:00, 17.22s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0551 | Train Acc: 0.5750\n",
      "Val Loss: 0.0344 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "Model saved! Best F1: 0.6571\n",
      "\n",
      "Epoch 22/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 5/5 [02:04<00:00, 24.83s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0461 | Train Acc: 0.5750\n",
      "Val Loss: 0.0329 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 23/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 5/5 [01:22<00:00, 16.41s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0495 | Train Acc: 0.5250\n",
      "Val Loss: 0.0319 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 24/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 5/5 [01:22<00:00, 16.46s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0514 | Train Acc: 0.6000\n",
      "Val Loss: 0.0350 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 25/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 5/5 [01:49<00:00, 21.92s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0413 | Train Acc: 0.6625\n",
      "Val Loss: 0.0339 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 26/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 5/5 [01:08<00:00, 13.65s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0529 | Train Acc: 0.4625\n",
      "Val Loss: 0.0368 | Val Acc: 0.6500 | Val F1: 0.3939 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 27/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 5/5 [01:07<00:00, 13.45s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0543 | Train Acc: 0.5625\n",
      "Val Loss: 0.0356 | Val Acc: 0.6000 | Val F1: 0.3750 | Val AUC: 0.6316\n",
      "\n",
      "Epoch 28/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 5/5 [01:06<00:00, 13.40s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0479 | Train Acc: 0.6375\n",
      "Val Loss: 0.0350 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 29/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 5/5 [01:05<00:00, 13.01s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0452 | Train Acc: 0.6375\n",
      "Val Loss: 0.0328 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 30/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 5/5 [01:37<00:00, 19.48s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0584 | Train Acc: 0.5125\n",
      "Val Loss: 0.0341 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.6842\n",
      "\n",
      "Epoch 31/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 5/5 [01:33<00:00, 18.77s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0586 | Train Acc: 0.6125\n",
      "Val Loss: 0.0321 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 32/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 5/5 [01:41<00:00, 20.28s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0543 | Train Acc: 0.5375\n",
      "Val Loss: 0.0318 | Val Acc: 0.7500 | Val F1: 0.4286 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 33/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 5/5 [01:40<00:00, 20.17s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0404 | Train Acc: 0.5000\n",
      "Val Loss: 0.0330 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 34/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 5/5 [01:41<00:00, 20.22s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0511 | Train Acc: 0.6625\n",
      "Val Loss: 0.0304 | Val Acc: 0.8000 | Val F1: 0.4444 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 35/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 5/5 [01:34<00:00, 19.00s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0471 | Train Acc: 0.5500\n",
      "Val Loss: 0.0310 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.8947\n",
      "Model saved! Best F1: 0.7222\n",
      "\n",
      "Epoch 36/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 5/5 [01:37<00:00, 19.46s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0501 | Train Acc: 0.5000\n",
      "Val Loss: 0.0334 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 37/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 5/5 [01:30<00:00, 18.17s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0423 | Train Acc: 0.6625\n",
      "Val Loss: 0.0318 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 38/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 5/5 [01:30<00:00, 18.09s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0490 | Train Acc: 0.5250\n",
      "Val Loss: 0.0330 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 39/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 5/5 [01:33<00:00, 18.77s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0470 | Train Acc: 0.5875\n",
      "Val Loss: 0.0359 | Val Acc: 0.7000 | Val F1: 0.5312 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 40/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 5/5 [01:33<00:00, 18.76s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0586 | Train Acc: 0.5125\n",
      "Val Loss: 0.0330 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 41/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 5/5 [01:17<00:00, 15.51s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0519 | Train Acc: 0.5500\n",
      "Val Loss: 0.0330 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 5/5 [01:14<00:00, 14.87s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0568 | Train Acc: 0.5625\n",
      "Val Loss: 0.0306 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 43/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 5/5 [01:12<00:00, 14.54s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0391 | Train Acc: 0.6625\n",
      "Val Loss: 0.0313 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 44/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 5/5 [01:09<00:00, 13.94s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0409 | Train Acc: 0.6500\n",
      "Val Loss: 0.0293 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 45/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 5/5 [01:12<00:00, 14.45s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0345 | Train Acc: 0.5750\n",
      "Val Loss: 0.0294 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 46/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 5/5 [01:10<00:00, 14.12s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0412 | Train Acc: 0.6375\n",
      "Val Loss: 0.0315 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 47/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 5/5 [01:02<00:00, 12.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0390 | Train Acc: 0.6625\n",
      "Val Loss: 0.0302 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 48/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|██████████| 5/5 [01:06<00:00, 13.25s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0431 | Train Acc: 0.6000\n",
      "Val Loss: 0.0280 | Val Acc: 0.9500 | Val F1: 0.8198 | Val AUC: 0.9474\n",
      "Model saved! Best F1: 0.8198\n",
      "\n",
      "Epoch 49/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|██████████| 5/5 [01:09<00:00, 13.96s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0420 | Train Acc: 0.6875\n",
      "Val Loss: 0.0260 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 50/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|██████████| 5/5 [01:06<00:00, 13.31s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0374 | Train Acc: 0.6625\n",
      "Val Loss: 0.0291 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 51/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|██████████| 5/5 [01:08<00:00, 13.74s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0506 | Train Acc: 0.5875\n",
      "Val Loss: 0.0294 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 52/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51: 100%|██████████| 5/5 [01:05<00:00, 13.07s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0350 | Train Acc: 0.6625\n",
      "Val Loss: 0.0293 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 53/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52: 100%|██████████| 5/5 [01:03<00:00, 12.67s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0538 | Train Acc: 0.5500\n",
      "Val Loss: 0.0308 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 54/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53: 100%|██████████| 5/5 [01:07<00:00, 13.48s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0425 | Train Acc: 0.6250\n",
      "Val Loss: 0.0294 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 55/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54: 100%|██████████| 5/5 [01:26<00:00, 17.29s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0457 | Train Acc: 0.6125\n",
      "Val Loss: 0.0300 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55: 100%|██████████| 5/5 [01:25<00:00, 17.20s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0456 | Train Acc: 0.7000\n",
      "Val Loss: 0.0309 | Val Acc: 0.7500 | Val F1: 0.5671 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 57/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56: 100%|██████████| 5/5 [01:15<00:00, 15.15s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0453 | Train Acc: 0.5875\n",
      "Val Loss: 0.0311 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 58/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57: 100%|██████████| 5/5 [01:13<00:00, 14.73s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:10<00:00,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0410 | Train Acc: 0.6750\n",
      "Val Loss: 0.0331 | Val Acc: 0.7000 | Val F1: 0.4118 | Val AUC: 0.7368\n",
      "\n",
      "Epoch 59/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58: 100%|██████████| 5/5 [01:21<00:00, 16.35s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:12<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0430 | Train Acc: 0.6625\n",
      "Val Loss: 0.0309 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8421\n",
      "\n",
      "Epoch 60/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59: 100%|██████████| 5/5 [01:19<00:00, 15.91s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0403 | Train Acc: 0.5500\n",
      "Val Loss: 0.0327 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 61/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60: 100%|██████████| 5/5 [01:11<00:00, 14.22s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0416 | Train Acc: 0.6750\n",
      "Val Loss: 0.0327 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.7895\n",
      "\n",
      "Epoch 62/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61: 100%|██████████| 5/5 [01:18<00:00, 15.72s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0392 | Train Acc: 0.6250\n",
      "Val Loss: 0.0297 | Val Acc: 0.8000 | Val F1: 0.6078 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 63/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 62: 100%|██████████| 5/5 [01:47<00:00, 21.52s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0500 | Train Acc: 0.5125\n",
      "Val Loss: 0.0297 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 64/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 63: 100%|██████████| 5/5 [01:21<00:00, 16.31s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0510 | Train Acc: 0.6375\n",
      "Val Loss: 0.0310 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 65/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 64: 100%|██████████| 5/5 [01:26<00:00, 17.20s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0417 | Train Acc: 0.6750\n",
      "Val Loss: 0.0300 | Val Acc: 0.8500 | Val F1: 0.6571 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 66/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 65: 100%|██████████| 5/5 [01:20<00:00, 16.10s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0373 | Train Acc: 0.7375\n",
      "Val Loss: 0.0267 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.8947\n",
      "\n",
      "Epoch 67/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 66: 100%|██████████| 5/5 [01:21<00:00, 16.25s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:13<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0476 | Train Acc: 0.5750\n",
      "Val Loss: 0.0257 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "\n",
      "Epoch 68/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 67: 100%|██████████| 5/5 [01:17<00:00, 15.43s/it]\n",
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0401 | Train Acc: 0.6250\n",
      "Val Loss: 0.0266 | Val Acc: 0.9000 | Val F1: 0.7222 | Val AUC: 0.9474\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Final Results - CONCAT\n",
      "Accuracy: 0.9500 | F1: 0.8198 | AUC: 0.9474\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FUSION STRATEGY COMPARISON\n",
      "================================================================================\n",
      "    Fusion Type Accuracy F1-Score    AUC\n",
      "CROSS_ATTENTION   0.9500   0.4872 0.7368\n",
      "          GATED   0.9500   0.4872 0.4211\n",
      "         CONCAT   0.9500   0.8198 0.9474\n",
      "\n",
      "Best Fusion Strategy: CONCAT\n",
      "F1-Score: 0.8198\n",
      "\n",
      "================================================================================\n",
      "All results saved to: ./improved_egfr_results\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED EGFR Mutation Prediction: Enhanced Hybrid Model\n",
    "# Key Improvements:\n",
    "# 1. Advanced fusion mechanisms (Cross-Attention, Gated Fusion)\n",
    "# 2. Feature alignment and normalization\n",
    "# 3. Progressive training strategy\n",
    "# 4. Enhanced data augmentation\n",
    "# 5. Focal loss for class imbalance\n",
    "# 6. Ensemble predictions\n",
    "# 7. Cross-modal consistency regularization\n",
    "\n",
    "import os;os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np,pandas as pd,torch,torch.nn as nn,torch.nn.functional as F\n",
    "import torchvision.models as models,torchvision.transforms as transforms\n",
    "from PIL import Image;Image.MAX_IMAGE_PIXELS=None\n",
    "from pathlib import Path;from datetime import datetime;import cv2\n",
    "from scipy.spatial.distance import cosine,euclidean,pdist,squareform;from scipy import stats\n",
    "import matplotlib.pyplot as plt,seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import random,glob,json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:import nibabel as nib\n",
    "except:print(\"Warning: nibabel not installed. Install with: pip install nibabel\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed);np.random.seed(seed);torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed);torch.backends.cudnn.deterministic=True\n",
    "set_seed(42)\n",
    "\n",
    "# ==================== FOCAL LOSS FOR IMBALANCE ====================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss to handle class imbalance\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# ==================== CROSS-ATTENTION MODULE ====================\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention between MRI and Histology features\"\"\"\n",
    "    def __init__(self, dim=2048, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # x1: query modality, x2: key/value modality\n",
    "        B = x1.size(0)\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        x1 = x1.unsqueeze(1)  # [B, 1, dim]\n",
    "        x2 = x2.unsqueeze(1)  # [B, 1, dim]\n",
    "        \n",
    "        Q = self.q_proj(x1).view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(x2).view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x2).view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, 1, self.dim)\n",
    "        out = self.out_proj(out).squeeze(1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ==================== GATED FUSION MODULE ====================\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Learnable gating mechanism for adaptive fusion\"\"\"\n",
    "    def __init__(self, dim=2048):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        concat = torch.cat([x1, x2], dim=1)\n",
    "        gates = self.gate(concat)  # [B, 2]\n",
    "        \n",
    "        fused = gates[:, 0:1] * x1 + gates[:, 1:2] * x2\n",
    "        return fused, gates\n",
    "\n",
    "# ==================== IMPROVED COMBINED MODEL ====================\n",
    "class ImprovedCombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, fusion_type='cross_attention'):\n",
    "        super().__init__()\n",
    "        self.fusion_type = fusion_type\n",
    "        \n",
    "        # Separate backbones with better architecture\n",
    "        self.mri_backbone = models.efficientnet_b0(pretrained=True)\n",
    "        self.hist_backbone = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Get feature dimension (EfficientNet-B0 outputs 1280)\n",
    "        feat_dim = 1280\n",
    "        \n",
    "        # Remove final classifier\n",
    "        self.mri_backbone.classifier = nn.Identity()\n",
    "        self.hist_backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Feature normalization\n",
    "        self.mri_norm = nn.BatchNorm1d(feat_dim)\n",
    "        self.hist_norm = nn.BatchNorm1d(feat_dim)\n",
    "        \n",
    "        # Feature projection to common space\n",
    "        self.mri_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        self.hist_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Fusion mechanisms\n",
    "        if fusion_type == 'cross_attention':\n",
    "            self.mri_to_hist_attn = CrossAttention(dim=256, num_heads=4)\n",
    "            self.hist_to_mri_attn = CrossAttention(dim=256, num_heads=4)\n",
    "            fusion_dim = 256 * 2  # Concatenate both attended features\n",
    "            \n",
    "        elif fusion_type == 'gated':\n",
    "            self.gated_fusion = GatedFusion(dim=256)\n",
    "            fusion_dim = 256\n",
    "            \n",
    "        else:  # Simple concatenation\n",
    "            fusion_dim = 256 * 2\n",
    "        \n",
    "        # Enhanced classifier with residual connections\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        # Extract features\n",
    "        mri_feat = self.mri_backbone(x['mri'])\n",
    "        hist_feat = self.hist_backbone(x['hist'])\n",
    "        \n",
    "        # Normalize\n",
    "        mri_feat = self.mri_norm(mri_feat)\n",
    "        hist_feat = self.hist_norm(hist_feat)\n",
    "        \n",
    "        # Project to common space\n",
    "        mri_proj = self.mri_proj(mri_feat)\n",
    "        hist_proj = self.hist_proj(hist_feat)\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_type == 'cross_attention':\n",
    "            # Bidirectional cross-attention\n",
    "            mri_attended = self.mri_to_hist_attn(mri_proj, hist_proj)\n",
    "            hist_attended = self.hist_to_mri_attn(hist_proj, mri_proj)\n",
    "            \n",
    "            # Residual connections\n",
    "            mri_final = mri_proj + mri_attended\n",
    "            hist_final = hist_proj + hist_attended\n",
    "            \n",
    "            fused = torch.cat([mri_final, hist_final], dim=1)\n",
    "            \n",
    "        elif self.fusion_type == 'gated':\n",
    "            fused, gates = self.gated_fusion(mri_proj, hist_proj)\n",
    "            \n",
    "        else:  # concatenation\n",
    "            fused = torch.cat([mri_proj, hist_proj], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused)\n",
    "        \n",
    "        if return_features:\n",
    "            return output, {'mri': mri_proj, 'hist': hist_proj, 'fused': fused}\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ==================== ENHANCED AUGMENTATION ====================\n",
    "def get_enhanced_transforms(img_size=224, is_train=True):\n",
    "    \"\"\"Enhanced augmentation strategy\"\"\"\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# ==================== MIXUP AUGMENTATION ====================\n",
    "def mixup_data(x_mri, x_hist, y, alpha=0.2):\n",
    "    \"\"\"Mixup augmentation for both modalities\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x_mri.size(0)\n",
    "    index = torch.randperm(batch_size).to(x_mri.device)\n",
    "\n",
    "    mixed_mri = lam * x_mri + (1 - lam) * x_mri[index, :]\n",
    "    mixed_hist = lam * x_hist + (1 - lam) * x_hist[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_mri, mixed_hist, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss calculation\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ==================== PROGRESSIVE TRAINING ====================\n",
    "def train_epoch_progressive(model, loader, criterion, optimizer, device, epoch, total_epochs, use_mixup=True):\n",
    "    \"\"\"Progressive training with curriculum learning\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Curriculum: start with easier samples, gradually include harder ones\n",
    "    mixup_alpha = 0.2 if use_mixup and epoch > 5 else 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        imgs, labels, _ = batch\n",
    "        \n",
    "        # Move to device\n",
    "        imgs = {k: v.to(device) for k, v in imgs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply mixup with probability\n",
    "        if use_mixup and epoch > 5 and random.random() < 0.5:\n",
    "            mixed_mri, mixed_hist, y_a, y_b, lam = mixup_data(\n",
    "                imgs['mri'], imgs['hist'], labels, alpha=mixup_alpha\n",
    "            )\n",
    "            imgs_mixed = {'mri': mixed_mri, 'hist': mixed_hist}\n",
    "            \n",
    "            outputs = model(imgs_mixed)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "# ==================== ENSEMBLE PREDICTION ====================\n",
    "class ModelEnsemble:\n",
    "    \"\"\"Ensemble of models with different fusion strategies\"\"\"\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        \n",
    "    def add_model(self, model_path, fusion_type):\n",
    "        model = ImprovedCombinedModel(fusion_type=fusion_type).to(self.device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        self.models.append(model)\n",
    "        \n",
    "    def predict(self, loader):\n",
    "        \"\"\"Ensemble prediction by averaging probabilities\"\"\"\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                model_probs = []\n",
    "                \n",
    "                for batch in loader:\n",
    "                    imgs, labels, _ = batch\n",
    "                    imgs = {k: v.to(self.device) for k, v in imgs.items()}\n",
    "                    \n",
    "                    outputs = model(imgs)\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    model_probs.append(probs.cpu().numpy())\n",
    "                    \n",
    "                    if len(all_labels) == 0:\n",
    "                        all_labels.extend(labels.numpy())\n",
    "                \n",
    "                all_probs.append(np.vstack(model_probs))\n",
    "        \n",
    "        # Average predictions\n",
    "        avg_probs = np.mean(all_probs, axis=0)\n",
    "        preds = avg_probs.argmax(axis=1)\n",
    "        \n",
    "        return preds, all_labels, avg_probs[:, 1]\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    # Paths (UPDATE THESE TO YOUR PATHS)\n",
    "    clinical_path = r\"D:\\paper\\external.xlsx\"\n",
    "    histology_base = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "    mri_base = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_radiology_images\\Brain-Mets-Lung-MRI-Path-Segs\"\n",
    "    output_dir = \"./improved_egfr_results\"\n",
    "    \n",
    "    # Training params\n",
    "    img_size = 224\n",
    "    batch_size = 16\n",
    "    epochs = 80  # Increased for better convergence\n",
    "    lr = 5e-5  # Lower learning rate for fine-tuning\n",
    "    patience = 20  # More patience\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Fusion strategies to try\n",
    "    fusion_types = ['cross_attention', 'gated', 'concat']\n",
    "    \n",
    "    # Loss parameters\n",
    "    focal_alpha = 0.25\n",
    "    focal_gamma = 2.0\n",
    "    \n",
    "    # Training strategies\n",
    "    use_mixup = True\n",
    "    use_progressive = True\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "\n",
    "# ==================== DATA LOADING (REUSE FROM ORIGINAL) ====================\n",
    "print(\"\\n=== Loading Clinical Data ===\")\n",
    "clinical_df = pd.read_excel(cfg.clinical_path) if cfg.clinical_path.endswith('.xlsx') else pd.read_csv(cfg.clinical_path)\n",
    "print(f\"Clinical data shape: {clinical_df.shape}\")\n",
    "\n",
    "# Find target column\n",
    "target_col = None\n",
    "for col in clinical_df.columns:\n",
    "    if 'EGFR' in col.upper() and 'STATUS' in col.upper():\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Target column 'EGFR Status' not found in dataset\")\n",
    "\n",
    "print(f\"Using target column: '{target_col}'\")\n",
    "print(f\"Target distribution:\\n{clinical_df[target_col].value_counts()}\")\n",
    "\n",
    "# Get patient IDs and labels\n",
    "if 'patientID' in clinical_df.columns:\n",
    "    patient_ids = clinical_df['patientID'].astype(str).tolist()\n",
    "else:\n",
    "    patient_ids = clinical_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "labels = clinical_df[target_col].values\n",
    "hist_patient_ids = clinical_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "# ==================== DATASET CLASSES (REUSE FROM ORIGINAL) ====================\n",
    "# [Insert your CombinedDataset class here - keeping it the same]\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, patient_ids, labels, mri_base, hist_base, transform=None, hist_ids=None):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.labels = labels\n",
    "        self.mri_base = Path(mri_base)\n",
    "        self.hist_base = Path(hist_base)\n",
    "        self.transform = transform\n",
    "        self.hist_ids = hist_ids if hist_ids else patient_ids\n",
    "        self.valid_data = self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        valid = []\n",
    "        all_mri_dirs = {d.name: d for d in self.mri_base.iterdir() if d.is_dir()}\n",
    "        \n",
    "        for idx, pid in enumerate(self.patient_ids):\n",
    "            hist_pid = self.hist_ids[idx]\n",
    "            \n",
    "            # Check MRI\n",
    "            patient_dir = None\n",
    "            if pid in all_mri_dirs:\n",
    "                patient_dir = all_mri_dirs[pid]\n",
    "            elif pid.startswith('YG_') and pid[3:] in all_mri_dirs:\n",
    "                patient_dir = all_mri_dirs[pid[3:]]\n",
    "            elif f\"YG_{pid}\" in all_mri_dirs:\n",
    "                patient_dir = all_mri_dirs[f\"YG_{pid}\"]\n",
    "            \n",
    "            has_mri = False\n",
    "            t1ce_file = None\n",
    "            flair_file = None\n",
    "            if patient_dir and patient_dir.exists():\n",
    "                t1ce_files = list(patient_dir.glob(\"*t1ce*\"))\n",
    "                flair_files = list(patient_dir.glob(\"*flair*\"))\n",
    "                if len(t1ce_files) > 0 and len(flair_files) > 0:\n",
    "                    has_mri = True\n",
    "                    t1ce_file = t1ce_files[0]\n",
    "                    flair_file = flair_files[0]\n",
    "            \n",
    "            # Check Histology\n",
    "            svs_files = list(self.hist_base.glob(f\"*{hist_pid}*.svs\"))\n",
    "            if len(svs_files) == 0 and hist_pid.startswith('YG_'):\n",
    "                svs_files = list(self.hist_base.glob(f\"*{hist_pid[3:]}*.svs\"))\n",
    "            has_hist = len(svs_files) > 0\n",
    "            \n",
    "            if has_mri and has_hist:\n",
    "                valid.append((idx, patient_dir, t1ce_file, flair_file, svs_files[0]))\n",
    "        \n",
    "        print(f\"Valid Combined samples: {len(valid)}/{len(self.patient_ids)}\")\n",
    "        return valid\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx, mri_dir, t1ce_path, flair_path, svs_path = self.valid_data[idx]\n",
    "        label = self.labels[real_idx]\n",
    "        pid = self.patient_ids[real_idx]\n",
    "        \n",
    "        # Load MRI\n",
    "        t1ce_img = self._load_nifti(t1ce_path)\n",
    "        flair_img = self._load_nifti(flair_path)\n",
    "        avg = (t1ce_img + flair_img) / 2\n",
    "        mri = np.stack([t1ce_img, flair_img, avg], axis=-1)\n",
    "        \n",
    "        # Load Histology\n",
    "        hist = self._load_svs(svs_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            mri = self.transform(Image.fromarray(mri.astype(np.uint8)))\n",
    "            hist = self.transform(Image.fromarray(hist))\n",
    "        \n",
    "        return {'mri': mri, 'hist': hist}, torch.tensor(label, dtype=torch.long), pid\n",
    "    \n",
    "    def _load_nifti(self, path):\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            from pathlib import Path\n",
    "            path = Path(path)\n",
    "            \n",
    "            if path.suffix == '' and not str(path).endswith('.nii'):\n",
    "                import zipfile\n",
    "                if zipfile.is_zipfile(path):\n",
    "                    with zipfile.ZipFile(path) as zf:\n",
    "                        nii_files = [f for f in zf.namelist() if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "                        if nii_files:\n",
    "                            with zf.open(nii_files[0]) as f:\n",
    "                                img_data = nib.load(f).get_fdata()\n",
    "                else:\n",
    "                    img_data = nib.load(str(path)).get_fdata()\n",
    "            else:\n",
    "                img_data = nib.load(str(path)).get_fdata()\n",
    "            \n",
    "            if len(img_data.shape) == 3:\n",
    "                img_slice = img_data[:, :, img_data.shape[2] // 2]\n",
    "            else:\n",
    "                img_slice = img_data[:, :, 0, 0] if len(img_data.shape) == 4 else img_data\n",
    "            \n",
    "            img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min() + 1e-8) * 255\n",
    "            return cv2.resize(img_slice.astype(np.uint8), (cfg.img_size, cfg.img_size))\n",
    "        except:\n",
    "            try:\n",
    "                img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:\n",
    "                    img = np.array(Image.open(path).convert('L'))\n",
    "                return cv2.resize(img, (cfg.img_size, cfg.img_size))\n",
    "            except:\n",
    "                return np.zeros((cfg.img_size, cfg.img_size), dtype=np.uint8)\n",
    "    \n",
    "    def _load_svs(self, path):\n",
    "        try:\n",
    "            from openslide import OpenSlide\n",
    "            slide = OpenSlide(str(path))\n",
    "            level = slide.level_count - 1\n",
    "            thumb = slide.read_region((0, 0), level, slide.level_dimensions[level])\n",
    "            thumb.thumbnail((cfg.img_size, cfg.img_size))\n",
    "            return np.array(thumb.convert('RGB'))\n",
    "        except:\n",
    "            img = Image.open(path)\n",
    "            img.thumbnail((cfg.img_size, cfg.img_size))\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "# ==================== VALIDATION FUNCTION ====================\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            imgs, labels, _ = batch\n",
    "            imgs = {k: v.to(device) for k, v in imgs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        if len(set(all_labels)) > 1:\n",
    "            auc_score = roc_auc_score(all_labels, all_probs)\n",
    "        else:\n",
    "            auc_score = 0.0\n",
    "    except:\n",
    "        auc_score = 0.0\n",
    "    \n",
    "    return total_loss / len(loader), acc, f1, auc_score, all_preds, all_labels, all_probs\n",
    "\n",
    "# ==================== MAIN TRAINING FUNCTION ====================\n",
    "def train_improved_model(fusion_type='cross_attention'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training IMPROVED COMBINED Model with {fusion_type.upper()} fusion\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Enhanced transforms\n",
    "    train_transform = get_enhanced_transforms(cfg.img_size, is_train=True)\n",
    "    val_transform = get_enhanced_transforms(cfg.img_size, is_train=False)\n",
    "    \n",
    "    # Split data\n",
    "    train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "        patient_ids, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Get indices for histology IDs\n",
    "    train_idx = [i for i, pid in enumerate(patient_ids) if pid in train_ids]\n",
    "    test_idx = [i for i, pid in enumerate(patient_ids) if pid in test_ids]\n",
    "    \n",
    "    train_dataset = CombinedDataset(\n",
    "        train_ids, train_labels, cfg.mri_base, cfg.histology_base, \n",
    "        train_transform, hist_ids=[hist_patient_ids[i] for i in train_idx]\n",
    "    )\n",
    "    test_dataset = CombinedDataset(\n",
    "        test_ids, test_labels, cfg.mri_base, cfg.histology_base, \n",
    "        val_transform, hist_ids=[hist_patient_ids[i] for i in test_idx]\n",
    "    )\n",
    "    \n",
    "    # Weighted sampling\n",
    "    train_dataset_labels = [train_dataset.labels[train_dataset.valid_data[i][0]] for i in range(len(train_dataset))]\n",
    "    class_counts = Counter(train_dataset_labels)\n",
    "    weights = [1.0 / class_counts[l] for l in train_dataset_labels]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, sampler=sampler, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Model\n",
    "    model = ImprovedCombinedModel(fusion_type=fusion_type).to(cfg.device)\n",
    "    \n",
    "    # Focal loss for class imbalance\n",
    "    criterion = FocalLoss(alpha=cfg.focal_alpha, gamma=cfg.focal_gamma)\n",
    "    \n",
    "    # Optimizer with layer-wise learning rates\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.mri_backbone.parameters(), 'lr': cfg.lr * 0.1},\n",
    "        {'params': model.hist_backbone.parameters(), 'lr': cfg.lr * 0.1},\n",
    "        {'params': model.mri_proj.parameters(), 'lr': cfg.lr},\n",
    "        {'params': model.hist_proj.parameters(), 'lr': cfg.lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': cfg.lr}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=cfg.lr * 0.01\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "        \n",
    "        # Progressive training\n",
    "        train_loss, train_acc = train_epoch_progressive(\n",
    "            model, train_loader, criterion, optimizer, cfg.device,\n",
    "            epoch, cfg.epochs, use_mixup=cfg.use_mixup\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_f1, val_auc, _, _, _ = validate(\n",
    "            model, test_loader, criterion, cfg.device\n",
    "        )\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"{cfg.output_dir}/improved_combined_{fusion_type}_best.pth\")\n",
    "            print(f\"Model saved! Best F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= cfg.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.load_state_dict(torch.load(f\"{cfg.output_dir}/improved_combined_{fusion_type}_best.pth\"))\n",
    "    _, final_acc, final_f1, final_auc, preds, true_labels, probs = validate(\n",
    "        model, test_loader, criterion, cfg.device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Final Results - {fusion_type.upper()}\")\n",
    "    print(f\"Accuracy: {final_acc:.4f} | F1: {final_f1:.4f} | AUC: {final_auc:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = {\n",
    "        'model_type': f'improved_combined_{fusion_type}',\n",
    "        'fusion_type': fusion_type,\n",
    "        'accuracy': float(final_acc),\n",
    "        'f1_score': float(final_f1),\n",
    "        'auc': float(final_auc),\n",
    "        'predictions': [int(p) for p in preds],\n",
    "        'true_labels': [int(t) for t in true_labels],\n",
    "        'probabilities': [float(p) for p in probs],\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{cfg.output_dir}/improved_combined_{fusion_type}_results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPROVED HYBRID MODEL TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Train models with different fusion strategies\n",
    "    all_results = {}\n",
    "    \n",
    "    for fusion_type in cfg.fusion_types:\n",
    "        try:\n",
    "            results = train_improved_model(fusion_type)\n",
    "            all_results[fusion_type] = results\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {fusion_type}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Compare results\n",
    "    if len(all_results) > 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FUSION STRATEGY COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for fusion_type, res in all_results.items():\n",
    "            comparison_data.append({\n",
    "                'Fusion Type': fusion_type.upper(),\n",
    "                'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "                'F1-Score': f\"{res['f1_score']:.4f}\",\n",
    "                'AUC': f\"{res['auc']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        comparison_df.to_csv(f\"{cfg.output_dir}/fusion_comparison.csv\", index=False)\n",
    "        \n",
    "        # Find best model\n",
    "        best_fusion = max(all_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "        print(f\"\\nBest Fusion Strategy: {best_fusion[0].upper()}\")\n",
    "        print(f\"F1-Score: {best_fusion[1]['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All results saved to: {cfg.output_dir}\")\n",
    "    print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
