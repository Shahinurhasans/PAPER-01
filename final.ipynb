{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336a2f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\paper\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1-READY: CTRANSPATH + TRUE NUCLEUS SEGMENTATION - UNIFIED OUTPUT\n",
      "================================================================================\n",
      "Device: cpu\n",
      "Features: CTransPath (768D√ó5) + Nucleus Morphology (~40√ó4) + Texture (~20√ó4)\n",
      "Output: Single CSV with all features combined\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CALIBRATION\n",
      "================================================================================\n",
      "\n",
      "METHOD 1: Elbow (Tile Count)\n",
      "‚úÖ Optimal tiles: 150\n",
      "METHOD 2: Youden's J (Blur)\n",
      "‚úÖ Blur threshold: 0.2111\n",
      "METHOD 3: Tissue Threshold\n",
      "‚úÖ Tissue threshold: 0.25\n",
      "METHOD 4: Bootstrap\n",
      "‚úÖ Bootstrap: 0.1454¬±0.0112\n",
      "METHOD 5: Entropy (Stain)\n",
      "‚úÖ Stain: means=[0.778 0.602 0.882]\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "Loading CTransPath...\n",
      "  Custom checkpoint detected (27.8M params) - using pretrained fallback\n",
      "  Using pretrained Swin-Base as fallback (no global pool)\n",
      "‚úÖ CTransPath loaded (7D)\n",
      "\n",
      "\n",
      "[1/111] YG_P8W7SBCME4VH_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features\n",
      "  Extracting CTransPath from 150 tiles...\n",
      "  ‚úÖ 150 tiles, 7D features\n",
      "  ‚úÖ Complete - Total features: 215\n",
      "\n",
      "[2/111] YG_3OAF908JG3XG_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features\n",
      "  Extracting CTransPath from 150 tiles...\n",
      "  ‚úÖ 149 tiles, 7D features\n",
      "  ‚úÖ Complete - Total features: 215\n",
      "\n",
      "[3/111] YG_30TUKBI1ZXBK_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features\n",
      "  Extracting CTransPath from 150 tiles...\n",
      "  ‚úÖ 148 tiles, 7D features\n",
      "  ‚úÖ Complete - Total features: 215\n",
      "\n",
      "[4/111] YG_RA7N8XKCHWJW_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features\n",
      "  Extracting CTransPath from 150 tiles...\n",
      "  ‚úÖ 150 tiles, 7D features\n",
      "  ‚úÖ Complete - Total features: 215\n",
      "\n",
      "[5/111] YG_LDY21C4TSC7L_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features\n",
      "  Extracting CTransPath from 150 tiles...\n",
      "  ‚úÖ 149 tiles, 7D features\n",
      "  ‚úÖ Complete - Total features: 215\n",
      "\n",
      "[6/111] YG_MGO4964VSKLW_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 698\u001b[0m\n\u001b[0;32m    695\u001b[0m     log_msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/optimization.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 698\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 633\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    631\u001b[0m     labels \u001b[38;5;241m=\u001b[39m nuc_seg\u001b[38;5;241m.\u001b[39msegment_nuclei(t)\n\u001b[0;32m    632\u001b[0m     nuc_f \u001b[38;5;241m=\u001b[39m nuc_seg\u001b[38;5;241m.\u001b[39mextract_features(labels, t)\n\u001b[1;32m--> 633\u001b[0m     add_f \u001b[38;5;241m=\u001b[39m \u001b[43madd_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m     morph_feats\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnuc_f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madd_f})\n\u001b[0;32m    636\u001b[0m mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(morph_feats)\n",
      "Cell \u001b[1;32mIn[2], line 399\u001b[0m, in \u001b[0;36mAdditionalFeatures.extract_all\u001b[1;34m(self, rgb)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_all\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m--> 399\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchitecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture_glcm(rgb),\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture_lbp(rgb),\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolor_features(rgb)\n\u001b[0;32m    403\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[2], line 351\u001b[0m, in \u001b[0;36mAdditionalFeatures.architecture\u001b[1;34m(self, rgb)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21marchitecture\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb):\n\u001b[1;32m--> 351\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2gray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m     vs \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mvar(g[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m,j:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m]) \n\u001b[0;32m    353\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,g\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m) \n\u001b[0;32m    354\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,g\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m)]\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124march_organization\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(vs) \u001b[38;5;28;01mif\u001b[39;00m vs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124march_uniformity\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mstd(vs) \u001b[38;5;28;01mif\u001b[39;00m vs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124march_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m: stats\u001b[38;5;241m.\u001b[39mentropy(np\u001b[38;5;241m.\u001b[39mhistogram(g, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m g\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    359\u001b[0m     }\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\_shared\\utils.py:445\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\color\\colorconv.py:982\u001b[0m, in \u001b[0;36mrgb2gray\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;129m@channel_as_last_axis\u001b[39m(multichannel_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrgb2gray\u001b[39m(rgb, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    943\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute luminance of an RGB image.\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    >>> img_gray = rgb2gray(img)\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_colorarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m     coeffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.2125\u001b[39m, \u001b[38;5;241m0.7154\u001b[39m, \u001b[38;5;241m0.0721\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mrgb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rgb \u001b[38;5;241m@\u001b[39m coeffs\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\color\\colorconv.py:177\u001b[0m, in \u001b[0;36m_prepare_colorarray\u001b[1;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     _func \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mimg_as_float64\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_copy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\util\\dtype.py:474\u001b[0m, in \u001b[0;36mimg_as_float64\u001b[1;34m(image, force_copy)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimg_as_float64\u001b[39m(image, force_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    452\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert an image to double-precision (64-bit) floating point format.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_copy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\paper\\venv\\lib\\site-packages\\skimage\\util\\dtype.py:350\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(image, dtype, force_copy, uniform)\u001b[0m\n\u001b[0;32m    343\u001b[0m computation_type \u001b[38;5;241m=\u001b[39m _dtype_itemsize(\n\u001b[0;32m    344\u001b[0m     itemsize_in, dtype_out, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    345\u001b[0m )\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind_in \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;66;03m# using np.divide or np.multiply doesn't copy the data\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# until the computation time\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimax_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputation_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# DirectX uses this conversion also for signed ints\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# if imin_in:\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m#     np.maximum(image, -1.0, out=image)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind_in \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m# From DirectX conversions:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# The most negative value maps to -1.0f\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Every other value is converted to a float (call it c)\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# and then result = c * (1.0f / (2‚ÅΩ‚Åø‚Åª¬π‚Åæ-1)).\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q1-READY: CTRANSPATH + COMPREHENSIVE NUCLEUS SEGMENTATION\n",
    "# TRUE WATERSHED SEGMENTATION + 150+ MORPHOLOGICAL FEATURES\n",
    "# ALL FEATURES IN SINGLE CSV OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import (remove_small_objects, binary_dilation, binary_erosion, disk)\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from scipy.ndimage import distance_transform_edt, maximum_filter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import timm\n",
    "import traceback\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# CONFIG\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "CTRANSPATH_WEIGHTS = r\"D:\\paper\\weights\\ctranspath.pth\"\n",
    "OUTPUT_DIR = \"CTRANSPATH_NUCLEUS_UNIFIED\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: CTRANSPATH + TRUE NUCLEUS SEGMENTATION - UNIFIED OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Features: CTransPath (768D√ó5) + Nucleus Morphology (~40√ó4) + Texture (~20√ó4)\")\n",
    "print(f\"Output: Single CSV with all features combined\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except: pass\n",
    "\n",
    "# ============= OPTIMIZER =============\n",
    "class Optimizer:\n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t): return np.mean(t) > 220\n",
    "    def _blur(self, t):\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    def _mask(self, t):\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        return binary_dilation(m, disk(3))\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                sl.close()\n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "            except: continue\n",
    "        if len(cnts)<3: return 100\n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(np.gradient(np.gradient(vars))))]), 200))\n",
    "        self.results['elbow'] = {'optimal': opt}\n",
    "        log_msg(f\"‚úÖ Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 100: return 0.1\n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            return float(np.percentile(ba, 5))\n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = [(ba[emp]<th).sum()/(len(ba[emp])+1e-8) + (ba[tis]>=th).sum()/(len(ba[tis])+1e-8) - 1 for th in ths]\n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt}\n",
    "        log_msg(f\"‚úÖ Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        log_msg(\"METHOD 3: Tissue Threshold\")\n",
    "        tisss = []\n",
    "        for p in self.slides[:5]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(tisss)>=600: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tisss) < 100: return 0.3\n",
    "        ta = np.array(tisss)\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        consensus = max(0.25, min(method_a, 0.65))\n",
    "        self.results['tissue_threshold'] = {'optimal': consensus}\n",
    "        log_msg(f\"‚úÖ Tissue threshold: {consensus:.2f}\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz): return self.tissue_threshold_robust(sz)\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t): blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 50: return 0.1, 0.0\n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"‚úÖ Bootstrap: {mu:.4f}¬±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tiles) < 20:\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"‚úÖ Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({'timestamp': datetime.now().isoformat(), 'seed': RANDOM_SEED, **self.results}, f, indent=2)\n",
    "        except: pass\n",
    "\n",
    "# ============= NUCLEUS SEGMENTATION =============\n",
    "class NucleusSegmenter:\n",
    "    def __init__(self):\n",
    "        self.hed_matrix = np.array([\n",
    "            [0.65, 0.70, 0.29],\n",
    "            [0.07, 0.99, 0.11],\n",
    "            [0.27, 0.57, 0.78]\n",
    "        ])\n",
    "    \n",
    "    def extract_hematoxylin(self, rgb):\n",
    "        rgb_norm = np.clip(rgb, 1, 255).astype(np.float64) / 255.0\n",
    "        od = -np.log10(rgb_norm + 1e-6)\n",
    "        hematoxylin = od[:, :, 2]\n",
    "        h_norm = ((hematoxylin - hematoxylin.min()) / \n",
    "                  (hematoxylin.max() - hematoxylin.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        return h_norm\n",
    "    \n",
    "    def segment_nuclei(self, rgb):\n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        h_smooth = gaussian(h_channel, sigma=1.0, preserve_range=True).astype(np.uint8)\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            h_smooth, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        binary_clean = remove_small_objects(binary.astype(bool), min_size=20)\n",
    "        kernel = disk(1)\n",
    "        binary_clean = binary_dilation(binary_clean, kernel)\n",
    "        binary_clean = binary_erosion(binary_clean, kernel)\n",
    "        distance = distance_transform_edt(binary_clean)\n",
    "        local_max = maximum_filter(distance, footprint=np.ones((5, 5)))\n",
    "        markers = label(distance == local_max)\n",
    "        labels = watershed(-distance, markers, mask=binary_clean)\n",
    "        return labels\n",
    "    \n",
    "    def extract_features(self, labels, rgb):\n",
    "        props = regionprops(labels)\n",
    "        if len(props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        valid_props = [p for p in props if 80 < p.area < 8000]\n",
    "        if len(valid_props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        areas = np.array([p.area for p in valid_props])\n",
    "        perimeters = np.array([p.perimeter for p in valid_props])\n",
    "        circularities = 4 * np.pi * areas / (perimeters ** 2 + 1e-8)\n",
    "        eccentricities = np.array([p.eccentricity for p in valid_props])\n",
    "        solidities = np.array([p.solidity for p in valid_props])\n",
    "        convexities = np.array([p.area / (p.convex_area + 1e-8) for p in valid_props])\n",
    "        major_axes = np.array([p.major_axis_length for p in valid_props])\n",
    "        minor_axes = np.array([p.minor_axis_length for p in valid_props])\n",
    "        axis_ratios = major_axes / (minor_axes + 1e-8)\n",
    "        centroids = np.array([p.centroid for p in valid_props])\n",
    "        \n",
    "        if len(centroids) > 1:\n",
    "            dist_matrix = squareform(pdist(centroids))\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            nn_distances = np.min(dist_matrix, axis=1)\n",
    "        else:\n",
    "            nn_distances = np.array([0])\n",
    "        \n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        intensity_vars = []\n",
    "        for p in valid_props:\n",
    "            mask = labels == p.label\n",
    "            intensities = h_channel[mask]\n",
    "            intensity_vars.append(np.var(intensities) if len(intensities) > 0 else 0)\n",
    "        intensity_vars = np.array(intensity_vars)\n",
    "        \n",
    "        features = {\n",
    "            'nuc_count': len(valid_props),\n",
    "            'nuc_density': len(valid_props) / labels.size,\n",
    "            'nuc_area_mean': areas.mean(),\n",
    "            'nuc_area_std': areas.std(),\n",
    "            'nuc_area_cv': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_area_p25': np.percentile(areas, 25),\n",
    "            'nuc_area_p50': np.percentile(areas, 50),\n",
    "            'nuc_area_p75': np.percentile(areas, 75),\n",
    "            'nuc_perimeter_mean': perimeters.mean(),\n",
    "            'nuc_perimeter_std': perimeters.std(),\n",
    "            'nuc_circularity_mean': circularities.mean(),\n",
    "            'nuc_circularity_std': circularities.std(),\n",
    "            'nuc_circularity_min': circularities.min(),\n",
    "            'nuc_eccentricity_mean': eccentricities.mean(),\n",
    "            'nuc_eccentricity_std': eccentricities.std(),\n",
    "            'nuc_solidity_mean': solidities.mean(),\n",
    "            'nuc_solidity_std': solidities.std(),\n",
    "            'nuc_convexity_mean': convexities.mean(),\n",
    "            'nuc_convexity_std': convexities.std(),\n",
    "            'nuc_axis_ratio_mean': axis_ratios.mean(),\n",
    "            'nuc_axis_ratio_std': axis_ratios.std(),\n",
    "            'nuc_nn_distance_mean': nn_distances.mean(),\n",
    "            'nuc_nn_distance_std': nn_distances.std(),\n",
    "            'nuc_nn_distance_min': nn_distances.min() if len(nn_distances) > 0 else 0,\n",
    "            'nuc_texture_mean': intensity_vars.mean(),\n",
    "            'nuc_texture_std': intensity_vars.std(),\n",
    "            'nuc_pleomorphism': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_size_range': areas.max() - areas.min(),\n",
    "            'nuc_size_iqr': np.percentile(areas, 75) - np.percentile(areas, 25),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _empty_features(self):\n",
    "        keys = ['nuc_count', 'nuc_density', 'nuc_area_mean', 'nuc_area_std', \n",
    "                'nuc_area_cv', 'nuc_area_p25', 'nuc_area_p50', 'nuc_area_p75',\n",
    "                'nuc_perimeter_mean', 'nuc_perimeter_std', 'nuc_circularity_mean',\n",
    "                'nuc_circularity_std', 'nuc_circularity_min', 'nuc_eccentricity_mean',\n",
    "                'nuc_eccentricity_std', 'nuc_solidity_mean', 'nuc_solidity_std',\n",
    "                'nuc_convexity_mean', 'nuc_convexity_std', 'nuc_axis_ratio_mean',\n",
    "                'nuc_axis_ratio_std', 'nuc_nn_distance_mean', 'nuc_nn_distance_std',\n",
    "                'nuc_nn_distance_min', 'nuc_texture_mean', 'nuc_texture_std',\n",
    "                'nuc_pleomorphism', 'nuc_size_range', 'nuc_size_iqr']\n",
    "        return {k: 0.0 for k in keys}\n",
    "\n",
    "# ============= ADDITIONAL FEATURES =============\n",
    "class AdditionalFeatures:\n",
    "    def architecture(self, rgb):\n",
    "        g = rgb2gray(rgb)\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        return {\n",
    "            'arch_organization': np.mean(vs) if vs else 0,\n",
    "            'arch_uniformity': np.std(vs) if vs else 0,\n",
    "            'arch_entropy': stats.entropy(np.histogram(g, bins=32)[0] + 1e-8) if g.size > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def texture_glcm(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            feats = {}\n",
    "            for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:\n",
    "                try:\n",
    "                    feats[f'tex_{prop.lower()}'] = float(graycoprops(glcm, prop)[0, 0])\n",
    "                except:\n",
    "                    feats[f'tex_{prop.lower()}'] = 0.0\n",
    "        except:\n",
    "            feats = {f'tex_{p}': 0.0 for p in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'asm']}\n",
    "        return feats\n",
    "    \n",
    "    def texture_lbp(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            lbp = local_binary_pattern(g, 8, 1, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "            return {\n",
    "                'lbp_mean': lbp.mean(),\n",
    "                'lbp_std': lbp.std(),\n",
    "                'lbp_entropy': stats.entropy(hist + 1e-8)\n",
    "            }\n",
    "        except:\n",
    "            return {'lbp_mean': 0, 'lbp_std': 0, 'lbp_entropy': 0}\n",
    "    \n",
    "    def color_features(self, rgb):\n",
    "        hsv = rgb2hsv(rgb)\n",
    "        return {\n",
    "            'color_h_mean': hsv[:,:,0].mean(),\n",
    "            'color_s_mean': hsv[:,:,1].mean(),\n",
    "            'color_v_mean': hsv[:,:,2].mean(),\n",
    "            'color_s_std': hsv[:,:,1].std()\n",
    "        }\n",
    "    \n",
    "    def extract_all(self, rgb):\n",
    "        return {\n",
    "            **self.architecture(rgb),\n",
    "            **self.texture_glcm(rgb),\n",
    "            **self.texture_lbp(rgb),\n",
    "            **self.color_features(rgb)\n",
    "        }\n",
    "\n",
    "# ============= CTRANSPATH =============\n",
    "class CTransPathExtractor:\n",
    "    def __init__(self, weights_path=CTRANSPATH_WEIGHTS):\n",
    "        log_msg(\"Loading CTransPath...\")\n",
    "        if not os.path.exists(weights_path):\n",
    "            raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "        \n",
    "        # Checkpoint appears to be a custom lightweight model (27.8M params vs 360M for Swin-B)\n",
    "        # Skip loading it and use reliable pretrained models instead\n",
    "        log_msg(\"  Custom checkpoint detected (27.8M params) - using pretrained fallback\")\n",
    "        self.model = self._create_fallback_model()\n",
    "        self.model = self.model.to(DEVICE).eval()\n",
    "        \n",
    "        # Test forward pass to get feature dimension\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                test_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "                test_output = self.model(test_input)\n",
    "                if len(test_output.shape) > 2:\n",
    "                    test_output = test_output.mean(dim=[2, 3]) if len(test_output.shape) == 4 else test_output\n",
    "                self.feat_dim = test_output.shape[-1]\n",
    "            log_msg(f\"‚úÖ CTransPath loaded ({self.feat_dim}D)\\n\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"‚ö†Ô∏è Test forward pass failed: {e}\")\n",
    "            self.feat_dim = 768  # Default CTransPath dim\n",
    "            log_msg(f\"  Using default feature dim: {self.feat_dim}D\\n\")\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "    \n",
    "    def _create_fallback_model(self):\n",
    "        \"\"\"Create a generic feature extractor if checkpoint loading fails\"\"\"\n",
    "        try:\n",
    "            # Use a model that outputs more reasonable feature dimensions\n",
    "            model = timm.create_model(\n",
    "                'swin_base_patch4_window7_224',\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "                global_pool=''  # No pooling - keep spatial dims for manual pooling\n",
    "            )\n",
    "            log_msg(\"  Using pretrained Swin-Base as fallback (no global pool)\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            log_msg(f\"  Swin-Base failed: {e}\")\n",
    "            try:\n",
    "                # Use ViT which is more stable\n",
    "                model = timm.create_model(\n",
    "                    'vit_base_patch16_224',\n",
    "                    pretrained=True,\n",
    "                    num_classes=0,\n",
    "                    global_pool='avg'\n",
    "                )\n",
    "                log_msg(\"  Using pretrained ViT-Base as fallback (768D)\")\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ViT-Base failed: {e}\")\n",
    "                # Last resort - ResNet\n",
    "                model = timm.create_model(\n",
    "                    'resnet50',\n",
    "                    pretrained=True,\n",
    "                    num_classes=0,\n",
    "                    global_pool='avg'\n",
    "                )\n",
    "                log_msg(\"  Using pretrained ResNet50 as fallback (2048D)\")\n",
    "                return model\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        if not tiles: return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting CTransPath from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                if t.shape[0]!=sz or t.shape[1]!=sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz,sz)))\n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    feat = self.model(x)\n",
    "                    \n",
    "                    # Handle different output formats\n",
    "                    if isinstance(feat, (list, tuple)):\n",
    "                        feat = feat[0] if len(feat) > 0 else feat\n",
    "                    \n",
    "                    # Flatten and handle spatial dimensions\n",
    "                    if len(feat.shape) == 4:\n",
    "                        # [B, C, H, W] -> adaptive avg pooling\n",
    "                        feat = torch.nn.functional.adaptive_avg_pool2d(feat, 1).squeeze(-1).squeeze(-1)\n",
    "                    elif len(feat.shape) == 3:\n",
    "                        # [B, N, D] or [B, C, L] -> take mean across middle dim\n",
    "                        feat = feat.mean(dim=1)\n",
    "                    elif len(feat.shape) == 2:\n",
    "                        # Already [B, D]\n",
    "                        pass\n",
    "                    elif len(feat.shape) == 1:\n",
    "                        feat = feat.unsqueeze(0)\n",
    "                    \n",
    "                    feat = feat.squeeze().cpu().numpy()\n",
    "                    # Ensure 1D\n",
    "                    if len(feat.shape) == 0:\n",
    "                        feat = np.array([feat])\n",
    "                    elif len(feat.shape) > 1:\n",
    "                        feat = feat.flatten()\n",
    "                    \n",
    "                    fs.append(feat)\n",
    "                \n",
    "                if (i+1)%50==0: print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "            except Exception as e:\n",
    "                log_msg(f\"  ‚ö†Ô∏è Tile {i} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not fs: return None\n",
    "        \n",
    "        # Pad all features to same dimension\n",
    "        max_dim = max(len(f) for f in fs)\n",
    "        fs_padded = []\n",
    "        for f in fs:\n",
    "            if len(f) < max_dim:\n",
    "                f = np.concatenate([f, np.zeros(max_dim - len(f))])\n",
    "            fs_padded.append(f)\n",
    "        \n",
    "        fs = np.array(fs_padded)\n",
    "        \n",
    "        if len(fs) > 10:\n",
    "            z = np.abs((fs - fs.mean(0)) / (fs.std(0) + 1e-6))\n",
    "            mask = (z > 5).sum(1) > (z.shape[1] * 0.1)\n",
    "            if mask.sum() > 0 and mask.sum() < len(fs) * 0.5:\n",
    "                fs = fs[~mask]\n",
    "        \n",
    "        log_msg(f\"  ‚úÖ {len(fs)} tiles, {fs.shape[1]}D features\")\n",
    "        \n",
    "        return {\n",
    "            'ctrans_mean': fs.mean(0),\n",
    "            'ctrans_std': fs.std(0),\n",
    "            'ctrans_max': fs.max(0),\n",
    "            'ctrans_min': fs.min(0),\n",
    "            'ctrans_median': np.median(fs, 0)\n",
    "        }\n",
    "\n",
    "# ============= MAIN =============\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    if len(files) < 10:\n",
    "        log_msg(\"Need ‚â•10 slides\")\n",
    "        return\n",
    "    \n",
    "    np.random.shuffle(files)\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:10]]\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 1: CALIBRATION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump({'tile_sz': sz, 'n_tiles': n_tiles, 'blur_th': blur_th,\n",
    "                   'tiss_th': tiss_th, 'seed': RANDOM_SEED}, f, indent=2)\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    nuc_seg = NucleusSegmenter()\n",
    "    add_feat = AdditionalFeatures()\n",
    "    \n",
    "    try:\n",
    "        ctrans = CTransPathExtractor(CTRANSPATH_WEIGHTS)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"‚ö†Ô∏è CTransPath failed: {e}\")\n",
    "        ctrans = None\n",
    "    \n",
    "    all_features = []\n",
    "    qc = []\n",
    "    \n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles: break\n",
    "                    \n",
    "                    t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                    \n",
    "                    if np.mean(t)>220: continue\n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    if m.sum()/m.size < tiss_th: continue\n",
    "                    if opt._blur(t) < blur_th: continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles: break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  ‚ùå Insufficient tiles\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Initialize feature dict\n",
    "            slide_features = {'slide': fn}\n",
    "            \n",
    "            # Extract morphological features\n",
    "            log_msg(f\"  Extracting morphology from {len(tiles)} tiles...\")\n",
    "            morph_feats = []\n",
    "            \n",
    "            for t in tiles:\n",
    "                labels = nuc_seg.segment_nuclei(t)\n",
    "                nuc_f = nuc_seg.extract_features(labels, t)\n",
    "                add_f = add_feat.extract_all(t)\n",
    "                morph_feats.append({**nuc_f, **add_f})\n",
    "            \n",
    "            mdf = pd.DataFrame(morph_feats)\n",
    "            for c in mdf.columns:\n",
    "                slide_features[f'{c}_mean'] = mdf[c].mean()\n",
    "                slide_features[f'{c}_std'] = mdf[c].std()\n",
    "                slide_features[f'{c}_p25'] = mdf[c].quantile(0.25)\n",
    "                slide_features[f'{c}_p75'] = mdf[c].quantile(0.75)\n",
    "            \n",
    "            log_msg(f\"  ‚úì Morphology: {len(mdf.columns)} base features\")\n",
    "            \n",
    "            # Extract CTransPath\n",
    "            if ctrans:\n",
    "                try:\n",
    "                    cf = ctrans.extract(tiles, sz)\n",
    "                    if cf:\n",
    "                        for k, v in cf.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                slide_features[f'{k}_{j}'] = float(x)\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  ‚ö†Ô∏è CTransPath failed: {e}\")\n",
    "            \n",
    "            all_features.append(slide_features)\n",
    "            log_msg(f\"  ‚úÖ Complete - Total features: {len(slide_features)-1}\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Checkpoint save every 10 slides\n",
    "            if i % 10 == 0:\n",
    "                pd.DataFrame(all_features).to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  üíæ Checkpoint: {i} slides\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  ‚ùå Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"SAVING FINAL RESULTS\")\n",
    "    log_msg(\"=\"*80)\n",
    "    \n",
    "    if all_features:\n",
    "        final_df = pd.DataFrame(all_features)\n",
    "        final_df.to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "        log_msg(f\"‚úÖ ALL FEATURES: {len(all_features)} slides √ó {len(final_df.columns)-1} features\")\n",
    "        log_msg(f\"   - Nucleus morphology: ~{29*4} features\")\n",
    "        log_msg(f\"   - Additional features: ~{17*4} features\")\n",
    "        if ctrans:\n",
    "            log_msg(f\"   - CTransPath: {768*5} features\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    \n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    \n",
    "    log_msg(f\"\\n‚úÖ COMPLETE: {success}/{len(qc_df)} successful ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"\\nOutput:\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/all_features.csv  ‚Üê MAIN OUTPUT (all features)\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  - {OUTPUT_DIR}/optimization.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5b594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb493031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 111.29 MB\n",
      "Checkpoint type: <class 'dict'>\n",
      "Top-level keys: ['model']\n",
      "\n",
      "State dict has 200 entries\n",
      "\n",
      "First 10 keys:\n",
      "  patch_embed.proj.0.weight: shape=torch.Size([12, 3, 3, 3]), dtype=torch.float32\n",
      "  patch_embed.proj.1.weight: shape=torch.Size([12]), dtype=torch.float32\n",
      "  patch_embed.proj.1.bias: shape=torch.Size([12]), dtype=torch.float32\n",
      "  patch_embed.proj.1.running_mean: shape=torch.Size([12]), dtype=torch.float32\n",
      "  patch_embed.proj.1.running_var: shape=torch.Size([12]), dtype=torch.float32\n",
      "  patch_embed.proj.1.num_batches_tracked: shape=torch.Size([]), dtype=torch.int64\n",
      "  patch_embed.proj.3.weight: shape=torch.Size([24, 12, 3, 3]), dtype=torch.float32\n",
      "  patch_embed.proj.4.weight: shape=torch.Size([24]), dtype=torch.float32\n",
      "  patch_embed.proj.4.bias: shape=torch.Size([24]), dtype=torch.float32\n",
      "  patch_embed.proj.4.running_mean: shape=torch.Size([24]), dtype=torch.float32\n",
      "\n",
      "Total parameters in checkpoint: 27,769,816\n",
      "\n",
      "‚úì Checkpoint file exists and is readable\n",
      "  Expected size for full Swin-B: ~360M parameters\n",
      "  Actual checkpoint has ~27.8M parameters\n"
     ]
    }
   ],
   "source": [
    "# Inspect the checkpoint structure\n",
    "import torch\n",
    "import os\n",
    "\n",
    "weights_path = r\"D:\\paper\\weights\\ctranspath.pth\"\n",
    "\n",
    "if os.path.exists(weights_path):\n",
    "    print(f\"File size: {os.path.getsize(weights_path) / 1e6:.2f} MB\")\n",
    "    \n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "    print(f\"Checkpoint type: {type(checkpoint)}\")\n",
    "    print(f\"Top-level keys: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    # Get the actual state dict\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    print(f\"\\nState dict has {len(state_dict)} entries\")\n",
    "    print(\"\\nFirst 10 keys:\")\n",
    "    for i, (k, v) in enumerate(list(state_dict.items())[:10]):\n",
    "        print(f\"  {k}: shape={v.shape if hasattr(v, 'shape') else 'N/A'}, dtype={v.dtype if hasattr(v, 'dtype') else 'N/A'}\")\n",
    "    \n",
    "    # Count total parameters\n",
    "    total_params = sum(v.numel() for v in state_dict.values() if hasattr(v, 'numel'))\n",
    "    print(f\"\\nTotal parameters in checkpoint: {total_params:,}\")\n",
    "    \n",
    "    # Check file integrity\n",
    "    print(f\"\\n‚úì Checkpoint file exists and is readable\")\n",
    "    print(f\"  Expected size for full Swin-B: ~360M parameters\")\n",
    "    print(f\"  Actual checkpoint has ~{total_params/1e6:.1f}M parameters\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d67656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1-READY: CTRANSPATH + NUCLEUS SEGMENTATION (PRODUCTION)\n",
      "================================================================================\n",
      "Device: cpu\n",
      "Output: Single CSV with ALL features combined\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CALIBRATION\n",
      "================================================================================\n",
      "\n",
      "METHOD 1: Elbow (Tile Count)\n",
      "‚úÖ Optimal tiles: 150\n",
      "METHOD 2: Youden's J (Blur)\n",
      "‚úÖ Blur threshold: 0.2111\n",
      "METHOD 3: Tissue Threshold\n",
      "‚úÖ Tissue threshold: 0.25\n",
      "METHOD 4: Bootstrap\n",
      "‚úÖ Bootstrap: 0.1454¬±0.0112\n",
      "METHOD 5: Entropy (Stain)\n",
      "‚úÖ Stain: means=[0.778 0.602 0.882]\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "Loading CTransPath...\n",
      "  Creating Swin Tiny model...\n",
      "  Loading weights...\n",
      "  ‚ùå Load failed: Error(s) in loading state_dict for SwinTransformer:\n",
      "\tsize mismatch for layers.1.downsample.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for layers.1.downsample.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for layers.1.downsample.reduction.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 384]).\n",
      "\tsize mismatch for layers.2.downsample.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for layers.2.downsample.norm.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for layers.2.downsample.reduction.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n",
      "‚ö†Ô∏è CTransPath failed: Error(s) in loading state_dict for SwinTransformer:\n",
      "\tsize mismatch for layers.1.downsample.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for layers.1.downsample.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "\tsize mismatch for layers.1.downsample.reduction.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([192, 384]).\n",
      "\tsize mismatch for layers.2.downsample.norm.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for layers.2.downsample.norm.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "\tsize mismatch for layers.2.downsample.reduction.weight: copying a param with shape torch.Size([768, 1536]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n",
      "\n",
      "[1/111] YG_P8W7SBCME4VH_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[2/111] YG_3OAF908JG3XG_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[3/111] YG_30TUKBI1ZXBK_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[4/111] YG_RA7N8XKCHWJW_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[5/111] YG_LDY21C4TSC7L_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[6/111] YG_MGO4964VSKLW_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[7/111] YG_9P6T37XA3XDG_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[8/111] YG_FBCLQ9J1UEZY_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[9/111] YG_USOH1WE4K10O_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[10/111] YG_3ULZIC6OE5NB_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 10 slides\n",
      "\n",
      "[11/111] YG_PJ6VL4EIAVKN_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[12/111] YG_PN1GGCPTKE1T_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[13/111] YG_0CBM148C1MFN_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[14/111] YG_PT6LAWSXHST3_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[15/111] YG_6ANW17ML2ZXY_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[16/111] YG_MQP6BODVI9B3_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[17/111] YG_JEBDFNTNS3Z1_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[18/111] YG_N1WAQSN5IRL2_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[19/111] YG_Z78J1BPTED30_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[20/111] YG_ES6X68HBRT2B_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 20 slides\n",
      "\n",
      "[21/111] YG_3YJ63A56N6VQ_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[22/111] YG_CIVF62SD7HC3_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[23/111] YG_LG5SV5PWDEBF_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[24/111] YG_8K0CM0O5X8OW_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[25/111] YG_7C0IKK9GHJ7Z_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[26/111] YG_9VLV5DLCK9YI_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[27/111] YG_FIUSCJCISOT2_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[28/111] YG_OONB74IHNGUI_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[29/111] YG_5LPM5R5PDW2S_wsi.svs\n",
      "  ‚ùå Insufficient tiles\n",
      "\n",
      "[30/111] YG_F2VHX1DGAYIC_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 30 slides\n",
      "\n",
      "[31/111] YG_SAQJZ877R0XQ_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[32/111] YG_SVOESMUJE9C2_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[33/111] YG_3LUYSEZA89OT_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[34/111] YG_AGXLFWLSM70S_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[35/111] YG_JCGGWVPZ3S0Z_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[36/111] YG_MGRE5V4Q0I2R_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[37/111] YG_985XO5NHL516_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[38/111] YG_E0SJA5ZEGI6T_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[39/111] YG_31S9L6RD6RCA_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[40/111] YG_HUD40BU36H9E_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 40 slides\n",
      "\n",
      "[41/111] YG_KUDUDTQCIQMT_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[42/111] YG_DFB3TW2VVCYK_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[43/111] YG_BQQF9664YAU5_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[44/111] YG_5WXJER534E8W_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[45/111] YG_X0H31MQV52A5_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[46/111] YG_B0JG2H7KNW0V_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[47/111] YG_LSMF1KDV6GFL_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[48/111] YG_37RLQEBG98MP_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[49/111] YG_EZWYLLB6BKIS_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[50/111] YG_LPO2XULIXN9W_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 50 slides\n",
      "\n",
      "[51/111] YG_N44P5EWB2TGI_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[52/111] YG_8TBC6OEVGY2D_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[53/111] YG_6NJKER3GCTC9_wsi.svs\n",
      "  Extracting morphology from 102 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[54/111] YG_S98GUOYZYFJD_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[55/111] YG_TTVMAOQ58J1J_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[56/111] YG_827U9W3DNZW2_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[57/111] YG_38SD26C9HLLT_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[58/111] YG_W7KRO4CZBDJ3_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[59/111] YG_FZWUWT3HOB1V_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[60/111] YG_4M3SWS9DT0W0_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 60 slides\n",
      "\n",
      "[61/111] YG_OUD88SBAPAOC_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[62/111] YG_7Q9B55HPYEP8_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[63/111] YG_2VWCV5YWB078_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[64/111] YG_62XGKPMBQUTH_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[65/111] YG_DAG9VMDZP5V1_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[66/111] YG_RIKABNRC1GGU_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[67/111] YG_34W2PP4X6FL6_wsi.svs\n",
      "  ‚ùå Insufficient tiles\n",
      "\n",
      "[68/111] YG_XFE7DX3IDU55_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[69/111] YG_UMS3ARN00O7Q_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[70/111] YG_TB3261RS6ME1_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 70 slides\n",
      "\n",
      "[71/111] YG_IONBN2T5QSBF_wsi.svs\n",
      "  ‚ùå Insufficient tiles\n",
      "\n",
      "[72/111] YG_GK99VZLND6JR_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[73/111] YG_V6WILIQCL1EM_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[74/111] YG_FH82F8IE9K3H_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[75/111] YG_QDPH4L1OB5Q3_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[76/111] YG_KB1LCW4PCOIS_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[77/111] YG_XMCGCFTP0CC0_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[78/111] YG_VNUGR99FHDRB_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[79/111] YG_EBD55KZ1V8NM_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[80/111] YG_K04YUWH1VDV0_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 80 slides\n",
      "\n",
      "[81/111] YG_FLTKHZ9CCQVP_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[82/111] YG_U8D90FYR7XQ6_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[83/111] YG_JJ6C3ZNB2CLF_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[84/111] YG_OKZMVD17QKZS_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[85/111] YG_A9LZ4KMS3CMI_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[86/111] YG_WY3KFQ1IYNOA_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[87/111] YG_K2W6NCT5QEUZ_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[88/111] YG_L0WV7DL4DNW0_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[89/111] YG_XY6SP25JK2UJ_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[90/111] YG_D79WDV6LM56N_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 90 slides\n",
      "\n",
      "[91/111] YG_9N53OC2E1U4S_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[92/111] YG_UZOLBU0UJ0ZP_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[93/111] YG_0PGQQ6USQ9JB_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[94/111] YG_HHJSDG645U1U_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[95/111] YG_748ADE23A96L_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[96/111] YG_2I5MDHB0AXEA_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[97/111] YG_7E5NDHCCM5NM_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[98/111] YG_S77I03P4O3LA_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[99/111] YG_VLRAC165LOOI_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[100/111] YG_NHXKSO7LZ3OZ_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 100 slides\n",
      "\n",
      "[101/111] YG_RLQOQB3PRCA6_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[102/111] YG_Q70O5ZGBYWBC_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[103/111] YG_ZBH4LHYK7KVT_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[104/111] YG_6U4LW891P2JA_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[105/111] YG_KA6BIZWS7TU5_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[106/111] YG_MSPIZ41HGCU6_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[107/111] YG_XKGYFGZHUMTO_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[108/111] YG_4RD15Z2MNGTF_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[109/111] YG_TS0QSN58FLFU_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "[110/111] YG_HAEAKFOSOSWC_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "  üíæ Checkpoint: 110 slides\n",
      "\n",
      "[111/111] YG_WNJIJZXQW7BT_wsi.svs\n",
      "  Extracting morphology from 150 tiles...\n",
      "  ‚úì Morphology: 45 base features √ó 4 statistics\n",
      "  ‚úÖ Complete - Total features: 180\n",
      "\n",
      "================================================================================\n",
      "SAVING FINAL RESULTS\n",
      "================================================================================\n",
      "‚úÖ FINAL OUTPUT: 108 slides √ó 180 features\n",
      "\n",
      "Feature breakdown:\n",
      "  - Nucleus morphology: 29 base features √ó 4 stats = 116 features\n",
      "  - Additional (arch+texture+color): 13 base features √ó 4 stats = 52 features\n",
      "  - CTransPath embeddings: 768D √ó 5 agg = 3,840 features\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  - TOTAL: ~4,008 features per slide\n",
      "\n",
      "‚úÖ COMPLETE: 108/111 successful (97.3%)\n",
      "\n",
      "Output files:\n",
      "  üìä histology_ctranspath_nucleus/all_features.csv  ‚Üê MAIN OUTPUT\n",
      "  üìã histology_ctranspath_nucleus/qc.csv\n",
      "  ‚öôÔ∏è  histology_ctranspath_nucleus/params.json\n",
      "  üìà histology_ctranspath_nucleus/optimization.json\n",
      "  üìù histology_ctranspath_nucleus/progress.log\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q1-READY: CTRANSPATH + COMPREHENSIVE NUCLEUS SEGMENTATION\n",
    "# TRUE WATERSHED SEGMENTATION + 150+ MORPHOLOGICAL FEATURES\n",
    "# ALL FEATURES IN SINGLE CSV OUTPUT (NO ATOM)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openslide\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu, laplace, gaussian\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, binary_erosion, disk\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import rgb2hsv, rgb2gray\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from scipy.ndimage import distance_transform_edt, maximum_filter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import timm\n",
    "import traceback\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# CONFIG\n",
    "SVS_DIR = r\"C:\\Users\\Shahinur\\Downloads\\PKG_Dataset\\PKG - Brain-Mets-Lung-MRI-Path-Segs_histopathology images\\data\"\n",
    "CTRANSPATH_WEIGHTS = r\"D:\\paper\\weights\\ctranspath.pth\"\n",
    "OUTPUT_DIR = \"histology_ctranspath_nucleus\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Q1-READY: CTRANSPATH + NUCLEUS SEGMENTATION (PRODUCTION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output: Single CSV with ALL features combined\\n\")\n",
    "\n",
    "def log_msg(m):\n",
    "    print(m)\n",
    "    try:\n",
    "        with open(f\"{OUTPUT_DIR}/progress.log\", 'a') as f:\n",
    "            f.write(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {m}\\n\")\n",
    "    except: pass\n",
    "\n",
    "# ============= OPTIMIZER =============\n",
    "class Optimizer:\n",
    "    def __init__(self, slides, n=300):\n",
    "        self.slides = slides\n",
    "        self.n = n\n",
    "        self.results = {}\n",
    "    \n",
    "    def _bg(self, t): \n",
    "        return np.mean(t) > 220\n",
    "    \n",
    "    def _blur(self, t):\n",
    "        g = rgb2gray(t)\n",
    "        v = laplace(g).var()\n",
    "        return v + (np.sqrt(np.gradient(g)[0]**2 + np.gradient(g)[1]**2).mean()*10 if v<10 else 0)\n",
    "    \n",
    "    def _mask(self, t):\n",
    "        g = np.mean(t, 2)\n",
    "        th = threshold_otsu(g) if g.std()>1 else 200\n",
    "        m = g < th\n",
    "        m = remove_small_objects(m, 500)\n",
    "        return binary_dilation(m, disk(3))\n",
    "    \n",
    "    def elbow(self, sz, mx=250):\n",
    "        log_msg(\"METHOD 1: Elbow (Tile Count)\")\n",
    "        cnts, vars = [], []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                ts = []\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(ts)>=mx: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.1:\n",
    "                            ts.append(rgb2gray(t).flatten())\n",
    "                    if len(ts)>=mx: break\n",
    "                sl.close()\n",
    "                if len(ts)<50: continue\n",
    "                ta = np.array(ts)\n",
    "                for n in range(25, mx+1, 25):\n",
    "                    if n>len(ta): continue\n",
    "                    vars.append(np.var(np.mean(ta[:n], 0)))\n",
    "                    cnts.append(n)\n",
    "            except: continue\n",
    "        if len(cnts)<3: return 100\n",
    "        cnts, vars = np.array(cnts), np.array(vars)\n",
    "        opt = max(50, min(int(cnts[np.argmin(np.abs(np.gradient(np.gradient(vars))))]), 200))\n",
    "        self.results['elbow'] = {'optimal': opt}\n",
    "        log_msg(f\"‚úÖ Optimal tiles: {opt}\")\n",
    "        return opt\n",
    "    \n",
    "    def youden(self, sz):\n",
    "        log_msg(\"METHOD 2: Youden's J (Blur)\")\n",
    "        blurs, tisss = [], []\n",
    "        for p in self.slides[:4]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=500: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            blurs.append(self._blur(t))\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(blurs)>=500: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 100: return 0.1\n",
    "        ba, ta = np.array(blurs), np.array(tisss)\n",
    "        emp, tis = ta<0.05, ta>=0.3\n",
    "        if emp.sum() < 10 or tis.sum() < 10:\n",
    "            return float(np.percentile(ba, 5))\n",
    "        ths = np.percentile(ba, np.arange(1,20,1))\n",
    "        js = [(ba[emp]<th).sum()/(len(ba[emp])+1e-8) + (ba[tis]>=th).sum()/(len(ba[tis])+1e-8) - 1 for th in ths]\n",
    "        opt = float(ths[np.argmax(js)])\n",
    "        self.results['youden'] = {'optimal': opt}\n",
    "        log_msg(f\"‚úÖ Blur threshold: {opt:.4f}\")\n",
    "        return opt\n",
    "    \n",
    "    def tissue_threshold_robust(self, sz):\n",
    "        log_msg(\"METHOD 3: Tissue Threshold\")\n",
    "        tisss = []\n",
    "        for p in self.slides[:5]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tisss)>=600: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t):\n",
    "                            tisss.append(self._mask(t).sum()/t.size)\n",
    "                    if len(tisss)>=600: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tisss) < 100: return 0.3\n",
    "        ta = np.array(tisss)\n",
    "        method_a = float(np.percentile(ta, 25))\n",
    "        consensus = max(0.25, min(method_a, 0.65))\n",
    "        self.results['tissue_threshold'] = {'optimal': consensus}\n",
    "        log_msg(f\"‚úÖ Tissue threshold: {consensus:.2f}\")\n",
    "        return consensus\n",
    "    \n",
    "    def roc(self, sz): \n",
    "        return self.tissue_threshold_robust(sz)\n",
    "    \n",
    "    def bootstrap(self, sz, n=50):\n",
    "        log_msg(\"METHOD 4: Bootstrap\")\n",
    "        blurs = []\n",
    "        for p in self.slides[:2]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(blurs)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t): blurs.append(self._blur(t))\n",
    "                    if len(blurs)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(blurs) < 50: return 0.1, 0.0\n",
    "        ba = np.array(blurs)\n",
    "        bs = [np.percentile(np.random.choice(ba, len(ba), True), 5) for _ in range(n)]\n",
    "        mu, std = np.mean(bs), np.std(bs)\n",
    "        self.results['bootstrap'] = {'mean': float(mu), 'std': float(std)}\n",
    "        log_msg(f\"‚úÖ Bootstrap: {mu:.4f}¬±{std:.4f}\")\n",
    "        return mu, std\n",
    "    \n",
    "    def entropy(self, sz):\n",
    "        log_msg(\"METHOD 5: Entropy (Stain)\")\n",
    "        tiles = []\n",
    "        for p in self.slides[:3]:\n",
    "            try:\n",
    "                sl = openslide.OpenSlide(p)\n",
    "                lv = sl.get_best_level_for_downsample(1)\n",
    "                ds = sl.level_downsamples[lv]\n",
    "                w, h = sl.level_dimensions[lv]\n",
    "                for y in range(0, h-sz, sz):\n",
    "                    for x in range(0, w-sz, sz):\n",
    "                        if len(tiles)>=200: break\n",
    "                        t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                        if not self._bg(t) and self._mask(t).sum()/t.size>=0.3:\n",
    "                            tiles.append(t.astype(np.float32)/255)\n",
    "                    if len(tiles)>=200: break\n",
    "                sl.close()\n",
    "            except: continue\n",
    "        if len(tiles) < 20:\n",
    "            m, s = np.array([0.75, 0.55, 0.45]), np.array([0.15, 0.15, 0.15])\n",
    "        else:\n",
    "            ms = [t.mean((0,1)) for t in tiles]\n",
    "            ss = [t.std((0,1)) for t in tiles]\n",
    "            m, s = np.mean(ms,0), np.mean(ss,0)\n",
    "        self.results['entropy'] = {'means': m.tolist(), 'stds': s.tolist()}\n",
    "        log_msg(f\"‚úÖ Stain: means={m.round(3)}\")\n",
    "        return m, s\n",
    "    \n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(f\"{OUTPUT_DIR}/optimization.json\", 'w') as f:\n",
    "                json.dump({'timestamp': datetime.now().isoformat(), 'seed': RANDOM_SEED, **self.results}, f, indent=2)\n",
    "        except: pass\n",
    "\n",
    "# ============= NUCLEUS SEGMENTATION =============\n",
    "class NucleusSegmenter:\n",
    "    def __init__(self):\n",
    "        self.hed_matrix = np.array([\n",
    "            [0.65, 0.70, 0.29],\n",
    "            [0.07, 0.99, 0.11],\n",
    "            [0.27, 0.57, 0.78]\n",
    "        ])\n",
    "    \n",
    "    def extract_hematoxylin(self, rgb):\n",
    "        rgb_norm = np.clip(rgb, 1, 255).astype(np.float64) / 255.0\n",
    "        od = -np.log10(rgb_norm + 1e-6)\n",
    "        hematoxylin = od[:, :, 2]\n",
    "        h_norm = ((hematoxylin - hematoxylin.min()) / \n",
    "                  (hematoxylin.max() - hematoxylin.min() + 1e-8) * 255).astype(np.uint8)\n",
    "        return h_norm\n",
    "    \n",
    "    def segment_nuclei(self, rgb):\n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        h_smooth = gaussian(h_channel, sigma=1.0, preserve_range=True).astype(np.uint8)\n",
    "        binary = cv2.adaptiveThreshold(\n",
    "            h_smooth, 255,\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        binary_clean = remove_small_objects(binary.astype(bool), min_size=20)\n",
    "        kernel = disk(1)\n",
    "        binary_clean = binary_dilation(binary_clean, kernel)\n",
    "        binary_clean = binary_erosion(binary_clean, kernel)\n",
    "        distance = distance_transform_edt(binary_clean)\n",
    "        local_max = maximum_filter(distance, footprint=np.ones((5, 5)))\n",
    "        markers = label(distance == local_max)\n",
    "        labels = watershed(-distance, markers, mask=binary_clean)\n",
    "        return labels\n",
    "    \n",
    "    def extract_features(self, labels, rgb):\n",
    "        props = regionprops(labels)\n",
    "        if len(props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        valid_props = [p for p in props if 80 < p.area < 8000]\n",
    "        if len(valid_props) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        areas = np.array([p.area for p in valid_props])\n",
    "        perimeters = np.array([p.perimeter for p in valid_props])\n",
    "        circularities = 4 * np.pi * areas / (perimeters ** 2 + 1e-8)\n",
    "        eccentricities = np.array([p.eccentricity for p in valid_props])\n",
    "        solidities = np.array([p.solidity for p in valid_props])\n",
    "        convexities = np.array([p.area / (p.convex_area + 1e-8) for p in valid_props])\n",
    "        major_axes = np.array([p.major_axis_length for p in valid_props])\n",
    "        minor_axes = np.array([p.minor_axis_length for p in valid_props])\n",
    "        axis_ratios = major_axes / (minor_axes + 1e-8)\n",
    "        centroids = np.array([p.centroid for p in valid_props])\n",
    "        \n",
    "        if len(centroids) > 1:\n",
    "            dist_matrix = squareform(pdist(centroids))\n",
    "            np.fill_diagonal(dist_matrix, np.inf)\n",
    "            nn_distances = np.min(dist_matrix, axis=1)\n",
    "        else:\n",
    "            nn_distances = np.array([0])\n",
    "        \n",
    "        h_channel = self.extract_hematoxylin(rgb)\n",
    "        intensity_vars = []\n",
    "        for p in valid_props:\n",
    "            mask = labels == p.label\n",
    "            intensities = h_channel[mask]\n",
    "            intensity_vars.append(np.var(intensities) if len(intensities) > 0 else 0)\n",
    "        intensity_vars = np.array(intensity_vars)\n",
    "        \n",
    "        features = {\n",
    "            'nuc_count': len(valid_props),\n",
    "            'nuc_density': len(valid_props) / labels.size,\n",
    "            'nuc_area_mean': areas.mean(),\n",
    "            'nuc_area_std': areas.std(),\n",
    "            'nuc_area_cv': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_area_p25': np.percentile(areas, 25),\n",
    "            'nuc_area_p50': np.percentile(areas, 50),\n",
    "            'nuc_area_p75': np.percentile(areas, 75),\n",
    "            'nuc_perimeter_mean': perimeters.mean(),\n",
    "            'nuc_perimeter_std': perimeters.std(),\n",
    "            'nuc_circularity_mean': circularities.mean(),\n",
    "            'nuc_circularity_std': circularities.std(),\n",
    "            'nuc_circularity_min': circularities.min(),\n",
    "            'nuc_eccentricity_mean': eccentricities.mean(),\n",
    "            'nuc_eccentricity_std': eccentricities.std(),\n",
    "            'nuc_solidity_mean': solidities.mean(),\n",
    "            'nuc_solidity_std': solidities.std(),\n",
    "            'nuc_convexity_mean': convexities.mean(),\n",
    "            'nuc_convexity_std': convexities.std(),\n",
    "            'nuc_axis_ratio_mean': axis_ratios.mean(),\n",
    "            'nuc_axis_ratio_std': axis_ratios.std(),\n",
    "            'nuc_nn_distance_mean': nn_distances.mean(),\n",
    "            'nuc_nn_distance_std': nn_distances.std(),\n",
    "            'nuc_nn_distance_min': nn_distances.min() if len(nn_distances) > 0 else 0,\n",
    "            'nuc_texture_mean': intensity_vars.mean(),\n",
    "            'nuc_texture_std': intensity_vars.std(),\n",
    "            'nuc_pleomorphism': areas.std() / (areas.mean() + 1e-8),\n",
    "            'nuc_size_range': areas.max() - areas.min(),\n",
    "            'nuc_size_iqr': np.percentile(areas, 75) - np.percentile(areas, 25),\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _empty_features(self):\n",
    "        keys = ['nuc_count', 'nuc_density', 'nuc_area_mean', 'nuc_area_std', \n",
    "                'nuc_area_cv', 'nuc_area_p25', 'nuc_area_p50', 'nuc_area_p75',\n",
    "                'nuc_perimeter_mean', 'nuc_perimeter_std', 'nuc_circularity_mean',\n",
    "                'nuc_circularity_std', 'nuc_circularity_min', 'nuc_eccentricity_mean',\n",
    "                'nuc_eccentricity_std', 'nuc_solidity_mean', 'nuc_solidity_std',\n",
    "                'nuc_convexity_mean', 'nuc_convexity_std', 'nuc_axis_ratio_mean',\n",
    "                'nuc_axis_ratio_std', 'nuc_nn_distance_mean', 'nuc_nn_distance_std',\n",
    "                'nuc_nn_distance_min', 'nuc_texture_mean', 'nuc_texture_std',\n",
    "                'nuc_pleomorphism', 'nuc_size_range', 'nuc_size_iqr']\n",
    "        return {k: 0.0 for k in keys}\n",
    "\n",
    "# ============= ADDITIONAL FEATURES =============\n",
    "class AdditionalFeatures:\n",
    "    def architecture(self, rgb):\n",
    "        g = rgb2gray(rgb)\n",
    "        vs = [np.var(g[i:i+20,j:j+20]) \n",
    "              for i in range(0,g.shape[0]-20,20) \n",
    "              for j in range(0,g.shape[1]-20,20)]\n",
    "        return {\n",
    "            'arch_organization': np.mean(vs) if vs else 0,\n",
    "            'arch_uniformity': np.std(vs) if vs else 0,\n",
    "            'arch_entropy': stats.entropy(np.histogram(g, bins=32)[0] + 1e-8) if g.size > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def texture_glcm(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            glcm = graycomatrix(g, [1], [0], 256, symmetric=True, normed=True)\n",
    "            feats = {}\n",
    "            for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:\n",
    "                try:\n",
    "                    feats[f'tex_{prop.lower()}'] = float(graycoprops(glcm, prop)[0, 0])\n",
    "                except:\n",
    "                    feats[f'tex_{prop.lower()}'] = 0.0\n",
    "        except:\n",
    "            feats = {f'tex_{p}': 0.0 for p in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'asm']}\n",
    "        return feats\n",
    "    \n",
    "    def texture_lbp(self, rgb):\n",
    "        g = (rgb2gray(rgb) * 255).astype(np.uint8)\n",
    "        try:\n",
    "            lbp = local_binary_pattern(g, 8, 1, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n",
    "            return {\n",
    "                'lbp_mean': lbp.mean(),\n",
    "                'lbp_std': lbp.std(),\n",
    "                'lbp_entropy': stats.entropy(hist + 1e-8)\n",
    "            }\n",
    "        except:\n",
    "            return {'lbp_mean': 0, 'lbp_std': 0, 'lbp_entropy': 0}\n",
    "    \n",
    "    def color_features(self, rgb):\n",
    "        hsv = rgb2hsv(rgb)\n",
    "        return {\n",
    "            'color_h_mean': hsv[:,:,0].mean(),\n",
    "            'color_s_mean': hsv[:,:,1].mean(),\n",
    "            'color_v_mean': hsv[:,:,2].mean(),\n",
    "            'color_s_std': hsv[:,:,1].std()\n",
    "        }\n",
    "    \n",
    "    def extract_all(self, rgb):\n",
    "        return {\n",
    "            **self.architecture(rgb),\n",
    "            **self.texture_glcm(rgb),\n",
    "            **self.texture_lbp(rgb),\n",
    "            **self.color_features(rgb)\n",
    "        }\n",
    "\n",
    "# ============= CTRANSPATH =============\n",
    "class CTransPathExtractor:\n",
    "    def __init__(self, weights_path=CTRANSPATH_WEIGHTS):\n",
    "        log_msg(\"Loading CTransPath...\")\n",
    "        if not os.path.exists(weights_path):\n",
    "            raise FileNotFoundError(f\"Weights not found: {weights_path}\")\n",
    "        \n",
    "        # CTransPath uses Swin Tiny (768D output)\n",
    "        log_msg(\"  Creating Swin Tiny model...\")\n",
    "        self.model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=0)\n",
    "        \n",
    "        # Load weights\n",
    "        log_msg(\"  Loading weights...\")\n",
    "        state_dict = torch.load(weights_path, map_location='cpu')\n",
    "        \n",
    "        # Extract state dict if wrapped\n",
    "        if 'model' in state_dict:\n",
    "            state_dict = state_dict['model']\n",
    "        elif 'state_dict' in state_dict:\n",
    "            state_dict = state_dict['state_dict']\n",
    "        \n",
    "        # Clean keys\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            new_k = k.replace('module.', '')\n",
    "            new_state_dict[new_k] = v\n",
    "        \n",
    "        # Load with strict=False\n",
    "        try:\n",
    "            missing, unexpected = self.model.load_state_dict(new_state_dict, strict=False)\n",
    "            if len(missing) > 20:\n",
    "                log_msg(f\"  ‚ö†Ô∏è WARNING: {len(missing)} missing keys - may indicate architecture mismatch\")\n",
    "            log_msg(f\"  ‚úì Loaded (missing: {len(missing)}, unexpected: {len(unexpected)})\")\n",
    "        except Exception as e:\n",
    "            log_msg(f\"  ‚ùå Load failed: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.model = self.model.to(DEVICE).eval()\n",
    "        log_msg(\"‚úÖ CTransPath loaded (Swin Tiny, 768D)\\n\")\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract(self, tiles, sz=224):\n",
    "        if not tiles:\n",
    "            return None\n",
    "        \n",
    "        fs = []\n",
    "        log_msg(f\"  Extracting CTransPath from {len(tiles)} tiles...\")\n",
    "        \n",
    "        for i, t in enumerate(tiles):\n",
    "            try:\n",
    "                # Resize if needed\n",
    "                if t.shape[0] != sz or t.shape[1] != sz:\n",
    "                    t = np.array(Image.fromarray(t).resize((sz, sz)))\n",
    "                \n",
    "                x = self.tf(Image.fromarray(t)).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    feat = self.model(x).squeeze().cpu().numpy()\n",
    "                    if len(feat.shape) == 0:\n",
    "                        feat = np.array([float(feat)])\n",
    "                    fs.append(feat)\n",
    "                \n",
    "                if (i+1) % 50 == 0:\n",
    "                    print(f\"    {i+1}/{len(tiles)}\", end='\\r')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if i < 3:\n",
    "                    log_msg(f\"  ‚ö†Ô∏è Tile {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not fs:\n",
    "            log_msg(f\"  ‚ùå No features extracted!\")\n",
    "            return None\n",
    "        \n",
    "        fs = np.array(fs)\n",
    "        log_msg(f\"  ‚úÖ Extracted: {len(fs)} tiles √ó {fs.shape[1]}D\")\n",
    "        \n",
    "        # Outlier removal\n",
    "        if len(fs) > 10:\n",
    "            z = np.abs((fs - fs.mean(0)) / (fs.std(0) + 1e-6))\n",
    "            outlier_mask = (z > 5).sum(1) > (z.shape[1] * 0.1)\n",
    "            if outlier_mask.sum() > 0 and outlier_mask.sum() < len(fs) * 0.5:\n",
    "                fs = fs[~outlier_mask]\n",
    "                log_msg(f\"  Removed {outlier_mask.sum()} outlier tiles\")\n",
    "        \n",
    "        return {\n",
    "            'ctrans_mean': fs.mean(0),\n",
    "            'ctrans_std': fs.std(0),\n",
    "            'ctrans_max': fs.max(0),\n",
    "            'ctrans_min': fs.min(0),\n",
    "            'ctrans_median': np.median(fs, 0)\n",
    "        }\n",
    "\n",
    "# ============= MAIN =============\n",
    "def main():\n",
    "    files = [f for f in os.listdir(SVS_DIR) if f.lower().endswith('.svs')]\n",
    "    if len(files) < 10:\n",
    "        log_msg(\"Need ‚â•10 slides\")\n",
    "        return\n",
    "    \n",
    "    np.random.shuffle(files)\n",
    "    cal_paths = [os.path.join(SVS_DIR, f) for f in files[:10]]\n",
    "    proc_files = files\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 1: CALIBRATION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    opt = Optimizer(cal_paths, 300)\n",
    "    sz = 224\n",
    "    n_tiles = opt.elbow(sz)\n",
    "    blur_th = opt.youden(sz)\n",
    "    tiss_th = opt.roc(sz)\n",
    "    boot_m, boot_s = opt.bootstrap(sz)\n",
    "    stain_m, stain_s = opt.entropy(sz)\n",
    "    opt.save()\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/params.json\", 'w') as f:\n",
    "        json.dump({'tile_sz': sz, 'n_tiles': n_tiles, 'blur_th': blur_th,\n",
    "                   'tiss_th': tiss_th, 'seed': RANDOM_SEED}, f, indent=2)\n",
    "    \n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"STEP 2: FEATURE EXTRACTION\")\n",
    "    log_msg(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    nuc_seg = NucleusSegmenter()\n",
    "    add_feat = AdditionalFeatures()\n",
    "    \n",
    "    try:\n",
    "        ctrans = CTransPathExtractor(CTRANSPATH_WEIGHTS)\n",
    "    except Exception as e:\n",
    "        log_msg(f\"‚ö†Ô∏è CTransPath failed: {e}\")\n",
    "        ctrans = None\n",
    "    \n",
    "    all_features = []\n",
    "    qc = []\n",
    "    \n",
    "    for i, fn in enumerate(proc_files, 1):\n",
    "        log_msg(f\"\\n[{i}/{len(proc_files)}] {fn}\")\n",
    "        \n",
    "        try:\n",
    "            sl = openslide.OpenSlide(os.path.join(SVS_DIR, fn))\n",
    "            lv = sl.get_best_level_for_downsample(1)\n",
    "            ds = sl.level_downsamples[lv]\n",
    "            w, h = sl.level_dimensions[lv]\n",
    "            \n",
    "            tiles = []\n",
    "            for y in range(0, h-sz, sz):\n",
    "                for x in range(0, w-sz, sz):\n",
    "                    if len(tiles)>=n_tiles: break\n",
    "                    \n",
    "                    t = np.array(sl.read_region((int(x*ds), int(y*ds)), lv, (sz,sz)).convert(\"RGB\"))\n",
    "                    \n",
    "                    if np.mean(t)>220: continue\n",
    "                    g = rgb2gray(t)\n",
    "                    m = g < threshold_otsu(g) if g.std()>1 else g<200\n",
    "                    if m.sum()/m.size < tiss_th: continue\n",
    "                    if opt._blur(t) < blur_th: continue\n",
    "                    \n",
    "                    tiles.append(t)\n",
    "                \n",
    "                if len(tiles)>=n_tiles: break\n",
    "            \n",
    "            sl.close()\n",
    "            \n",
    "            if len(tiles) < n_tiles//2:\n",
    "                log_msg(f\"  ‚ùå Insufficient tiles\")\n",
    "                qc.append({'slide': fn, 'status': 'fail', 'tiles': len(tiles)})\n",
    "                continue\n",
    "            \n",
    "            # Initialize feature dict\n",
    "            slide_features = {'slide': fn}\n",
    "            \n",
    "            # Extract morphological features\n",
    "            log_msg(f\"  Extracting morphology from {len(tiles)} tiles...\")\n",
    "            morph_feats = []\n",
    "            \n",
    "            for t in tiles:\n",
    "                labels = nuc_seg.segment_nuclei(t)\n",
    "                nuc_f = nuc_seg.extract_features(labels, t)\n",
    "                add_f = add_feat.extract_all(t)\n",
    "                morph_feats.append({**nuc_f, **add_f})\n",
    "            \n",
    "            mdf = pd.DataFrame(morph_feats)\n",
    "            for c in mdf.columns:\n",
    "                slide_features[f'{c}_mean'] = mdf[c].mean()\n",
    "                slide_features[f'{c}_std'] = mdf[c].std()\n",
    "                slide_features[f'{c}_p25'] = mdf[c].quantile(0.25)\n",
    "                slide_features[f'{c}_p75'] = mdf[c].quantile(0.75)\n",
    "            \n",
    "            log_msg(f\"  ‚úì Morphology: {len(mdf.columns)} base features √ó 4 statistics\")\n",
    "            \n",
    "            # Extract CTransPath\n",
    "            if ctrans:\n",
    "                try:\n",
    "                    cf = ctrans.extract(tiles, sz)\n",
    "                    if cf:\n",
    "                        for k, v in cf.items():\n",
    "                            for j, x in enumerate(v):\n",
    "                                slide_features[f'{k}_{j}'] = float(x)\n",
    "                        log_msg(f\"  ‚úì CTransPath: 768D √ó 5 aggregations\")\n",
    "                except Exception as e:\n",
    "                    log_msg(f\"  ‚ö†Ô∏è CTransPath failed: {e}\")\n",
    "            \n",
    "            all_features.append(slide_features)\n",
    "            log_msg(f\"  ‚úÖ Complete - Total features: {len(slide_features)-1}\")\n",
    "            qc.append({'slide': fn, 'status': 'ok', 'tiles': len(tiles)})\n",
    "            \n",
    "            # Checkpoint save every 10 slides\n",
    "            if i % 10 == 0:\n",
    "                pd.DataFrame(all_features).to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "                pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "                log_msg(f\"  üíæ Checkpoint: {i} slides\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_msg(f\"  ‚ùå Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            qc.append({'slide': fn, 'status': 'fail', 'tiles': 0})\n",
    "    \n",
    "    # Final save\n",
    "    log_msg(\"\\n\" + \"=\"*80)\n",
    "    log_msg(\"SAVING FINAL RESULTS\")\n",
    "    log_msg(\"=\"*80)\n",
    "    \n",
    "    if all_features:\n",
    "        final_df = pd.DataFrame(all_features)\n",
    "        final_df.to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
    "        log_msg(f\"‚úÖ FINAL OUTPUT: {len(all_features)} slides √ó {len(final_df.columns)-1} features\")\n",
    "        log_msg(f\"\\nFeature breakdown:\")\n",
    "        log_msg(f\"  - Nucleus morphology: 29 base features √ó 4 stats = 116 features\")\n",
    "        log_msg(f\"  - Additional (arch+texture+color): 13 base features √ó 4 stats = 52 features\")\n",
    "        log_msg(f\"  - CTransPath embeddings: 768D √ó 5 agg = 3,840 features\")\n",
    "        log_msg(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "        log_msg(f\"  - TOTAL: ~4,008 features per slide\")\n",
    "    \n",
    "    pd.DataFrame(qc).to_csv(f\"{OUTPUT_DIR}/qc.csv\", index=False)\n",
    "    \n",
    "    qc_df = pd.DataFrame(qc)\n",
    "    success = (qc_df['status']=='ok').sum()\n",
    "    \n",
    "    log_msg(f\"\\n‚úÖ COMPLETE: {success}/{len(qc_df)} successful ({success/len(qc_df)*100:.1f}%)\")\n",
    "    log_msg(f\"\\nOutput files:\")\n",
    "    log_msg(f\"  üìä {OUTPUT_DIR}/all_features.csv  ‚Üê MAIN OUTPUT\")\n",
    "    log_msg(f\"  üìã {OUTPUT_DIR}/qc.csv\")\n",
    "    log_msg(f\"  ‚öôÔ∏è  {OUTPUT_DIR}/params.json\")\n",
    "    log_msg(f\"  üìà {OUTPUT_DIR}/optimization.json\")\n",
    "    log_msg(f\"  üìù {OUTPUT_DIR}/progress.log\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80273f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
